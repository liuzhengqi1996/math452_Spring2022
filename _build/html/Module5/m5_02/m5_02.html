
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Batch normalization &#8212; Math 452 Site</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Building and Training ResNet with Pytorch" href="../m5_03/m5_03.html" />
    <link rel="prev" title="Data normalization and weights initialization" href="../m5_01/m5_01.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/PSU_SCI_RGB_2C.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Math 452 Site</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Welcome to Math 452
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  contents
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module0/ch0_.html">
   Module 0 Get started: course information and preparations:
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module0/ch0_1.html">
     Course information, requirements and reference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module0/ch0_2.html">
     Course background and introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module0/ch0_3.html">
     Introduction to Python and Pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module0/quiz0.html">
     Preliminary Quiz
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module1/module1_.html">
   Module 1: Linear machine learning models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_01.html">
     Machine learning basics, popular data sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_02.html">
     Linearly separable sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_03.html">
     Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_04.html">
     KL-divergence and cross-entropy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_05.html">
     Support vector machine and relation with LR
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_06.html">
     Optimization and gradient descent method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_hw.html">
     Homework 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/Programming_Assignment_1.html">
     Module 1 Programming Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/quiz1.html">
     Quiz 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module2/module2_.html">
   Module 2: Probability and training algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_01.html">
     Introduction to probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_02.html">
     Probabilistic derivation of logistic regression models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_03/m2_03.html">
     Convex functions and convergence of gradient descen
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_04.html">
     Stochastic gradient descent method and convergence theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_05.html">
     MNIST: training and generalization accuracy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_hw.html">
     Homework 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/Programming_Assignment_2.html">
     Week 2 Programming Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/quiz2.html">
     Quiz 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module3/module3_.html">
   Module 3: Deep neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_01/m3_01.html">
     Nonlinear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_02/m3_02.html">
     Polynomials and Weierstrass theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_03/m3_03.html">
     Finite element method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_04/m3_04.html">
     Deep neural network functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_05/m3_05.html">
     Universal approximation properties
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_06.html">
     Application to data classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_07.html">
     DNN for image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_08/m3_08.html">
     Monte Carlo Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/C08_DNN.html">
     Building and Training Deep Neural Networks (DNNs) with Pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_hw.html">
     Homework 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/Programming_Assignment_3.html">
     Week 3 Programming Assignment
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module4/module4_.html">
   Module 4: Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_01/m4_01.html">
     Convolutional neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_02/m4_02.html">
     Convolutional operations on images
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_03/m4_03.html">
     Some classic CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_04/m4_04.html">
     Training CNN with GPU on Colab
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_05.html">
     Building and Training Convolutional Neural Networks (CNNs) with Pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_hw.html">
     Homework 4
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/Programming_Assignment_4.html">
     Week 4 Programming Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/quiz4.html">
     Quiz 5
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../module5_.html">
   Module 5: Normalization, ResNet and Multigrid
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../m5_01/m5_01.html">
     Data normalization and weights initialization
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Batch normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../m5_03/m5_03.html">
     Building and Training ResNet with Pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../m5_04/m5_04.html">
     Multigrid Method for Finite Element
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../m5_hw.html">
     Homework 5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Programming_Assignment_5.html">
     Week 5 Programming Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../quiz5.html">
     Quiz 5
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module6/module6_.html">
   Module 6: MgNet
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module6/m6_01.html">
     MgNet: a special CNN based on multigrid method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module6/m6_02.html">
     Multigrid and MgNet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module6/MG_MgNet.html">
     Multigrid and MgNet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module6/Final_Project.html">
     MATH 452: Final Project
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/Module5/m5_02/m5_02.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/liuzhengqi1996/math452_Spring2022/main?urlpath=lab/tree/Module5/m5_02/m5_02.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#download-the-lecture-notes-here-notes">
   Download the lecture notes here: Notes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recall-the-original-dnn-model">
   Recall the original DNN model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-batch-normalization-and-new-model">
   “’Real’” Batch Normalization and “’new’ model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bn-some-modified-sgd-on-new-batch-normalized-model">
     BN: some ’modified” SGD on new batch normalized model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-phase-in-batch-normalized-dnn">
     Testing phase in Batch-Normalized DNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-normalization-for-cnn">
     Batch Normalization for CNN
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="batch-normalization">
<h1>Batch normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">IFrame</span>

<span class="n">IFrame</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="s2">&quot;https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_c20ifjjk&amp;flashvars[streamerType]=auto&amp;amp;flashvars[localizationCode]=en&amp;amp;flashvars[leadWithHTML5]=true&amp;amp;flashvars[sideBarContainer.plugin]=true&amp;amp;flashvars[sideBarContainer.position]=left&amp;amp;flashvars[sideBarContainer.clickToClose]=true&amp;amp;flashvars[chapters.plugin]=true&amp;amp;flashvars[chapters.layout]=vertical&amp;amp;flashvars[chapters.thumbnailRotator]=false&amp;amp;flashvars[streamSelector.plugin]=true&amp;amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;amp;flashvars[dualScreen.plugin]=true&amp;amp;flashvars[hotspots.plugin]=1&amp;amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;amp;&amp;wid=1_t1lki6t4&quot;</span>  <span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="s1">&#39;800&#39;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="s1">&#39;500&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="800"
    height="500"
    src="https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_c20ifjjk&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_t1lki6t4"
    frameborder="0"
    allowfullscreen
></iframe>
</div></div>
</div>
<div class="section" id="download-the-lecture-notes-here-notes">
<h2>Download the lecture notes here: <a class="reference external" href="https://sites.psu.edu/math452/files/2022/03/DetailedE01-02.pdf">Notes</a><a class="headerlink" href="#download-the-lecture-notes-here-notes" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="recall-the-original-dnn-model">
<h2>Recall the original DNN model<a class="headerlink" href="#recall-the-original-dnn-model" title="Permalink to this headline">¶</a></h2>
<p>Consider the classical (fully connected) artificial deep neural network
(DNN) <span class="math notranslate nohighlight">\(f^{L}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}f^{1} &amp; =\theta^{1}(x):=W^{1} x+b^{1}, \\ f^{\ell} &amp; =\theta^{\ell} \circ \sigma\left(f^{\ell-1}\right):=W^{\ell} \sigma\left(f^{\ell-1}\right)+b^{\ell}, \ell=2, \ldots, L .\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n}\)</span> is the input vector, <span class="math notranslate nohighlight">\(\sigma\)</span> is a
non-linear function (activation).</p>
</div>
<div class="section" id="real-batch-normalization-and-new-model">
<h2>“’Real’” Batch Normalization and “’new’ model<a class="headerlink" href="#real-batch-normalization-and-new-model" title="Permalink to this headline">¶</a></h2>
<p>Definition of <span class="math notranslate nohighlight">\(B N\)</span> operation based on the batch</p>
<p>Following the idea in normalization, we consider that we have the all training data as</p>
<div class="math notranslate nohighlight">
\[
    (X, Y):=\left\{x_{i}, y_{i}\right\}_{i=1}^{N} 
\]</div>
<p>Since the normalization is applied to each activation independently, let
us focus on a particular activation <span class="math notranslate nohighlight">\(\left[f^{\ell}\right]_{k}\)</span> and omit
<span class="math notranslate nohighlight">\(k\)</span> as <span class="math notranslate nohighlight">\(f^{\ell}\)</span> for clarity. We have <span class="math notranslate nohighlight">\(N\)</span> values of this activation in
the batch,</p>
<div class="math notranslate nohighlight">
\[
    X=\left\{x_{1}, \cdots, x_{N}\right\}
\]</div>
<p>Let the normalized values be <span class="math notranslate nohighlight">\(\hat{f}^{\ell}\)</span>, and their linear transformations be
<span class="math notranslate nohighlight">\(\tilde{f}^{\ell} .\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq1-51">
<span class="eqno">(44)<a class="headerlink" href="#equation-eq1-51" title="Permalink to this equation">¶</a></span>\[\begin{split}
    \begin{gathered}
\mu_{X}^{\ell} \leftarrow \mathbb{E}_{x \sim X}\left[f^{\ell}(x)\right]=\frac{1}{N} \sum_{i=1}^{N} f^{\ell}\left(x_{i}\right) \\
\sigma_{X}^{\ell} \leftarrow \mathbb{E}_{x \sim X}\left[\left(f^{\ell}(x)-\mathbb{E}_{x \sim X}\left[f^{\ell}(x)\right]\right)^{2}\right]=\frac{1}{N} \sum_{i=1}^{N}\left(f^{\ell}\left(x_{i}\right)-\mu_{X}\right)^{2} \quad \text { batch mean } \\
\hat{f}^{\ell}(x) \leftarrow \frac{f^{\ell}(x)-\mu_{X}^{\ell}}{\sqrt{\sigma_{X}^{\ell}+\epsilon}} \\
\tilde{f}^{\ell}(x) \leftarrow \gamma^{\ell} \hat{f}^{\ell}(x)+\beta^{\ell}
\end{gathered}
\end{split}\]</div>
<p>Here we note that all these operations in the previous equation are defined by element-wise. Then at last, we define the BN operation based on the batch set as</p>
<div class="math notranslate nohighlight">
\[
    \mathrm{BN}_{X}\left(f^{\ell}(x)\right)=\tilde{f}^{\ell}(x):=\gamma^{\ell} \frac{f^{\ell}(x)-\mu_{X}^{\ell}}{\sqrt{\sigma_{X}^{\ell}+\epsilon}}+\beta^{\ell}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{f}^{\ell}(x), \mu_{X}^{\ell}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{X}^{\ell}\)</span> are
given above.</p>
<p>“New” model for BN
In summary, we have the new DNN model with BN as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}\tilde{f}^{1}\left(x_{i}\right) &amp; =\left(\theta^{1}\left(x_{i}\right)\right) \\ \tilde{f}^{\ell} &amp; =\theta^{\ell} \circ \sigma \circ \mathrm{BN}_{X}\left(\tilde{f}^{\ell-1}\right), \quad \ell=2, \ldots, L .\end{cases}
\end{split}\]</div>
<p>For a more comprehensive notation, we can use the next notation</p>
<div class="math notranslate nohighlight">
\[
    \sigma_{\mathrm{BN}}:=\sigma \circ \mathrm{BN}_{X}
\]</div>
<p>Here one thing is important that we need to mention is that because of the new scale
<span class="math notranslate nohighlight">\(\gamma^{\ell}\)</span> and shift <span class="math notranslate nohighlight">\(\beta^{\ell}\)</span> added after the BN operation.
We can remove the basis <span class="math notranslate nohighlight">\(b^{\ell}\)</span> in <span class="math notranslate nohighlight">\(\theta^{\ell}\)</span>, thus to say the
real model we will compute should be</p>
<div class="math notranslate nohighlight" id="equation-eq1-55">
<span class="eqno">(45)<a class="headerlink" href="#equation-eq1-55" title="Permalink to this equation">¶</a></span>\[\begin{split}
    \begin{cases}\tilde{f}^{1}\left(x_{i}\right) &amp; =W^{1} x_{i} \\ \tilde{f}^{\ell} &amp; =W^{\ell} \sigma_{\mathrm{BN}}\left(\tilde{f}^{\ell-1}\right), \quad \ell=2, \ldots, L .\end{cases}
\end{split}\]</div>
<p>Combine the two definition, we note</p>
<div class="math notranslate nohighlight">
\[
    \tilde{\Theta}:=\{W, \gamma, \beta\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(W=\left\{W^{1}, \cdots, W^{l}\right\}, \gamma:=\left\{\gamma^{2}, \cdots, \gamma^{L}\right\}\)</span>
and <span class="math notranslate nohighlight">\(\beta:=\left\{\beta^{2}, \cdots, \beta^{L}\right\}\)</span></p>
<p>Finally, we have the loss function as:</p>
<div class="math notranslate nohighlight" id="equation-eq1-57">
<span class="eqno">(46)<a class="headerlink" href="#equation-eq1-57" title="Permalink to this equation">¶</a></span>\[
    \mathcal{L}(\tilde{\Theta})=\mathbb{E}_{(x, y) \sim(X, Y)} \approx \frac{1}{N} \sum_{i=1}^{N} \ell\left(\tilde{f}^{L}\left(x_{i} ; \tilde{\Theta}\right), y_{i}\right)
\]</div>
<p>A key observation in <a class="reference internal" href="#equation-eq1-57">(46)</a> and the new BN model <a class="reference internal" href="#equation-eq1-55">(45)</a> is that</p>
<div class="math notranslate nohighlight" id="equation-eq1-58">
<span class="eqno">(47)<a class="headerlink" href="#equation-eq1-58" title="Permalink to this equation">¶</a></span>\[\begin{split}
    \begin{aligned}
\mu_{X}^{\ell} &amp;=\mathbb{E}_{x \sim X}\left[f^{\ell}(x)\right] \\
\sigma_{X}^{\ell} &amp;=\mathbb{E}_{x \sim X}\left[\left(f^{\ell}(x)-\mathbb{E}_{x \sim X}\left[f^{\ell}(x)\right]\right)^{2}\right] \\
\mathcal{L}(\tilde{\Theta}) &amp;=\mathbb{E}_{(x, y) \sim(X, Y)}\left[\ell\left(\tilde{f}^{L}\left(x_{i} ; \tilde{\Theta}\right), y_{i}\right)\right]
\end{aligned}
\end{split}\]</div>
<p>Here we need to mention that <span class="math notranslate nohighlight">\(x \sim X\)</span> means <span class="math notranslate nohighlight">\(x\)</span> subject to the discrete distribution of all data <span class="math notranslate nohighlight">\(X\)</span>.</p>
<div class="section" id="bn-some-modified-sgd-on-new-batch-normalized-model">
<h3>BN: some ’modified” SGD on new batch normalized model<a class="headerlink" href="#bn-some-modified-sgd-on-new-batch-normalized-model" title="Permalink to this headline">¶</a></h3>
<p>Following the key observation in <a class="reference internal" href="#equation-eq1-58">(47)</a>, and recall the similar case in
SGD, we do the the sampling trick in <a class="reference internal" href="#equation-eq1-57">(46)</a> and obtain the mini-batch
SGD:</p>
<div class="math notranslate nohighlight" id="equation-eq1-59">
<span class="eqno">(48)<a class="headerlink" href="#equation-eq1-59" title="Permalink to this equation">¶</a></span>\[
    x \sim X \approx x \sim \mathcal{B}
\]</div>
<p>here <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is a mini-batch of batch <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\mathcal{B} \subset X .\)</span></p>
<p>However, for problem in <a class="reference internal" href="#equation-eq1-57">(46)</a>, it is very difficult to find some subtle
sampling method because of the composition of <span class="math notranslate nohighlight">\(\mu_{X}^{\ell}\)</span> and
<span class="math notranslate nohighlight">\(\left[\sigma_{X}^{\ell}\right]^{2}\)</span>. However, one simple way for
sampling <a class="reference internal" href="#equation-eq1-57">(46)</a> can be chosen as taking <a class="reference internal" href="#equation-eq1-59">(48)</a> for all the expectation
case in <a class="reference internal" href="#equation-eq1-57">(46)</a> and <a class="reference internal" href="#equation-eq1-58">(47)</a>.</p>
<p>This is to say, in training process ( <span class="math notranslate nohighlight">\(t\)</span>-th step for example), once we
choose <span class="math notranslate nohighlight">\(B_{t} \subset X\)</span> as the mini-batch, then the model becomes
$<span class="math notranslate nohighlight">\(\begin{cases}\tilde{f}^{1}\left(x_{i}\right) &amp; =W^{1} x_{i}, \\ \tilde{f}^{\ell} &amp; =W^{\ell} \sigma_{\mathrm{BN}}\left(\tilde{f}^{\ell-1}\right), \quad \ell=2, \ldots, L .\end{cases}\)</span>$
where</p>
<div class="math notranslate nohighlight">
\[
    \sigma_{\mathrm{BN}}:=\sigma \circ \mathrm{BN}_{\mathcal{B}_{t}}
\]</div>
<p>or we can say that <span class="math notranslate nohighlight">\(X\)</span> is replaced by <span class="math notranslate nohighlight">\(\mathcal{B}_{t}\)</span> in this case.</p>
<p>Here <span class="math notranslate nohighlight">\(\mathrm{BN}_{\mathcal{B}_{t}}\)</span> is defined by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{array}{cr}
\mu_{\mathcal{B}_{t}}^{\ell} &amp; \leftarrow \frac{1}{m} \sum_{i=1}^{m} f^{\ell}\left(x_{i}\right) \\
\sigma_{\mathcal{B}_{t}}^{\ell} &amp; \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(f^{\ell}\left(x_{i}\right)-\mu_{\mathcal{B}_{t}}\right)^{2} \quad \text { mini-batch mean } \\
\hat{f}^{\ell}(x) &amp; \leftarrow \frac{f^{\ell}(x)-\mu_{\mathcal{B}_{t}}^{\ell}}{\sqrt{\sigma_{\mathcal{B}_{t}}^{\ell}+\epsilon}} \\
\mathrm{BN}_{\mathcal{B}_{t}}\left(\tilde{f}^{\ell}\right):=\tilde{f}^{\ell}(x) &amp; \leftarrow \gamma^{\ell} \hat{f}^{\ell}(x)+\beta^{\ell} \\
&amp; \text { normalize }
\end{array}
\end{split}\]</div>
<p>Here BN operation introduce some new parameters as <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. Thus to say, for training phase, if we choose
mini-batch as <span class="math notranslate nohighlight">\(\mathcal{B}_{t}\)</span> in <span class="math notranslate nohighlight">\(t\)</span>-th training step, we need to take
gradient as</p>
<div class="math notranslate nohighlight">
\[
    \frac{1}{m} \nabla_{\tilde{\Theta}} \sum_{i \in \mathcal{B}_{t}} \ell\left(\tilde{f}^{L}\left(x_{i} ; \tilde{\Theta}\right), y_{i}\right)
\]</div>
<p>which needs us the to take gradient for <span class="math notranslate nohighlight">\(\mu_{B}^{\ell}\)</span> or
<span class="math notranslate nohighlight">\(\left[\sigma_{B}^{\ell}\right]^{2}\)</span> w.r.t <span class="math notranslate nohighlight">\(w^{i}\)</span> for <span class="math notranslate nohighlight">\(i \leq \ell\)</span>.</p>
<p>Questions: To derive the new gradient formula for BN step because of the
fact that</p>
<div class="math notranslate nohighlight">
\[
    \mu_{\mathcal{B}_{t}}^{\ell}, \quad \text { and } \quad \sigma_{\mathcal{B}_{t}}^{\ell}
\]</div>
<p>contain the output of <span class="math notranslate nohighlight">\(\tilde{f}^{\ell-1}\)</span>.</p>
</div>
<div class="section" id="testing-phase-in-batch-normalized-dnn">
<h3>Testing phase in Batch-Normalized DNN<a class="headerlink" href="#testing-phase-in-batch-normalized-dnn" title="Permalink to this headline">¶</a></h3>
<p>One key problem is that, in the BN operator, we need to compute the mean
and variance in a data set (batch or mini-batch). However, in the
inference step, we just input one data into this DNN, how to compute the
BN operator in this situation.</p>
<p>Actually, the <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> parameter is fixed after training,
the only problem is to compute the mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>.
All the mean <span class="math notranslate nohighlight">\(\mu_{\mathcal{B}_{t}}\)</span> and variance
<span class="math notranslate nohighlight">\(\sigma_{\mathcal{B}}^{2}\)</span> during the training phase are just the
approximation of the mean and variance of whole batch i.e. <span class="math notranslate nohighlight">\(\mu_{X}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{X}^{2}\)</span> as shown in <a class="reference internal" href="#equation-eq1-58">(47)</a>.</p>
<p>One natural idea might be just use the BN operator w.r.t to the whole
training data set, thus to say just compute <span class="math notranslate nohighlight">\(\mu_{X}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{X}^{2}\)</span> by definition in <a class="reference internal" href="#equation-eq1-51">(44)</a>.</p>
<p>However, there are at least the next few problems:</p>
<ul class="simple">
<li><p>computation cost,</p></li>
<li><p>ignoring the statistical approximation (don’t make use of the
<span class="math notranslate nohighlight">\(\mu_{\mathcal{B}_{t}}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{\mathcal{B}_{t}}^{2}\)</span> in
training phase).</p></li>
</ul>
<p>Considering that we have the statistical approximation for <span class="math notranslate nohighlight">\(\mu_{X}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{X}^{2}\)</span> during each SGD step, moving average might be a more
straightforward way. Thus two say, we define the <span class="math notranslate nohighlight">\(\mu^{\ell}\)</span> and
<span class="math notranslate nohighlight">\(\left[\sigma^{\ell}\right]^{2}\)</span> for the inference (test) phase as</p>
<div class="math notranslate nohighlight">
\[
    \mu^{\ell}=\frac{1}{T} \sum_{t=1}^{T} \mu_{\mathcal{B}_{t}}^{\ell}, \quad \sigma^{\ell}=\frac{1}{T} \frac{m}{m-1} \sum_{t=1}^{T} \sigma_{\mathcal{B}_{t}}^{\ell}
\]</div>
<p>Here we take Bessel’s correction for unbiased variance.
Another way to do this is to call the similar idea in momentum. At each
time step we update the running averages for mean and variance using an
exponential decay based on the momentum parameter:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
&amp;\mu_{\mathcal{B}_{t}}^{\ell}=\alpha \mu_{\mathcal{B}_{t-1}}^{\ell}+(1-\alpha) \mu_{\mathcal{B}_{t}}^{\ell} \\
&amp;\sigma_{\mathcal{B}_{t}}^{\ell}=\alpha \sigma_{\mathcal{B}_{t-1}}^{\ell}+(1-\alpha) \sigma_{\mathcal{B}_{t}}^{\ell}
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\alpha\)</span> is close to 1 , we can take it as <span class="math notranslate nohighlight">\(0.9\)</span>
generally. Then we all take bath mean and variance as
<span class="math notranslate nohighlight">\(\mu_{X}^{\ell} \approx \mu_{\mathcal{B}_{T}}^{\ell}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{X}^{\ell} \approx \sigma_{\mathcal{B}_{T}}^{\ell} .\)</span></p>
<p>Many people argue that the variance here should also use Bessel’s
correction.</p>
</div>
<div class="section" id="batch-normalization-for-cnn">
<h3>Batch Normalization for CNN<a class="headerlink" href="#batch-normalization-for-cnn" title="Permalink to this headline">¶</a></h3>
<p>One key idea in <span class="math notranslate nohighlight">\(\mathrm{BN}\)</span> is to do normalization with each scalar
features (neurons) separately along a mini-batch. Thus to say, we need
one to identify what is neuron in CNN. This is a historical problem,
some people think neuron in CNN should be the pixel in each channel some
thing that each channel is just one neuron. BN choose the later one. One
(most ?) important reason for this choice is the fact of computation
cost. For convolutional layers, BN additionally wants the normalization
to obey the convolutional property - so that different elements of the
same feature map, at different locations, are normalized in the same
way. To compute <span class="math notranslate nohighlight">\(\mu_{\mathcal{B}_{t}}^{\ell}\)</span>, we take mean of the set
of all values in a feature map across both the elements of a mini-batch
and spatial locations - so for a mini-batch of size <span class="math notranslate nohighlight">\(m\)</span> and feature maps
of size <span class="math notranslate nohighlight">\(m_{\ell} \times n_{\ell}\)</span> (image geometrical size), we use the
effective mini-batch of size <span class="math notranslate nohighlight">\(m m_{\ell} n_{\ell}\)</span>. We learn a pair of
parameters <span class="math notranslate nohighlight">\(\gamma_{k}\)</span> and <span class="math notranslate nohighlight">\(\beta_{k}\)</span> per feature map (k-th channel),
rather than per activation</p>
<p>For simplicity, then have the following BN scheme for CNN</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{array}{cc}
{\left[\mu_{\mathcal{B}_{t}}^{\ell}\right]_{j} \leftarrow \frac{1}{m \times m_{\ell} \times n_{\ell}} \sum_{i=1}^{m} \sum_{1 \leq s \leq m_{\ell}, 1 \leq t \leq n_{\ell}}\left[f^{\ell}\left(x_{i}\right)\right]_{j ; s t}} &amp; \text { mean on channel } j \\
{\left[\sigma_{\mathcal{B}_{t}}^{\ell}\right]_{j} \leftarrow \frac{1}{m \times m_{\ell} \times n_{\ell}} \sum_{i=1}^{m} \sum_{1 \leq s \leq m_{\ell}, 1 \leq t \leq n_{\ell}}\left(\left[f^{\ell}\left(x_{i}\right)\right]_{j ; s t}-\left[\mu_{\mathcal{B}_{t}}^{\ell}\right]_{j}\right)^{2}} &amp; \text { variance on channel } j \\
{\left[\hat{f}^{\ell}(x)\right]_{j ; s t} \leftarrow \frac{\left[f^{\ell}(x)\right]_{j, s t}-\left[\mu_{\mathcal{B}_{t}}^{\ell}\right]_{j}}{\sqrt{\left[\sigma_{\mathcal{B}_{t}}^{\ell}\right]_{j}+\epsilon}}} &amp; \text { normalize }
\end{array}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
    \left[\mathrm{BN}_{\mathcal{B}_{t}}\left(\tilde{f}^{\ell}\right)\right]_{j ; s t}:=\left[\tilde{f}^{\ell}(x)\right]_{j ; s t} \leftarrow\left[\gamma^{\ell}\right]_{j}\left[\hat{f}^{\ell}(x)\right]_{j ; s t}+\left[\beta^{\ell}\right]_{j}
\]</div>
<p>scale and shift on channel</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "liuzhengqi1996/math452_Spring2022",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Module5/m5_02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="../m5_01/m5_01.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Data normalization and weights initialization</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="../m5_03/m5_03.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Building and Training ResNet with Pytorch</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Department of Mathematics, Penn State University Park<br/>
        
            &copy; Copyright The Pennsylvania State University, 2021. This material is not licensed for resale.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>