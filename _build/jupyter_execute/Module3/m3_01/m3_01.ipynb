{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f53c8bd",
   "metadata": {},
   "source": [
    "# Nonlinear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf25e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "    width=\"800\"\n",
       "    height=\"500\"\n",
       "    src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_mpdchgne&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_fam1ya2z\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x110c54b50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_mpdchgne&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_fam1ya2z\" ,width='800', height='500')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b95b604",
   "metadata": {},
   "source": [
    "## Download the lecture notes here: [Notes](https://sites.psu.edu/math452/files/2022/01/C01_-Nonlinear-Models-1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604f38a",
   "metadata": {},
   "source": [
    "## Nonlinear classifiable sets\n",
    "\n",
    "In the section, we will extend the linearly separable sets to nonlinear\n",
    "case. A natural extension is like what kernel method does in SVM for\n",
    "binary case, we will introduce the so-called feature mapping.\n",
    "\n",
    "Thus, we have the following natural extension for linearly separable by\n",
    "using feature mapping and original definition of linearly separable.\n",
    "\n",
    "Definition 1 (nonlinearly separable sets). These data sets\n",
    "\n",
    "$$\n",
    "    A_{1}, A_{2}, \\cdots, A_{k} \\subset \\mathbb{R}^{d}\n",
    "$$\n",
    "\n",
    "are called nonlinearly separable, if there exist a feature space $\\mathbb{R}^{\\tilde{d}}$\n",
    "\n",
    "and a smooth (if it has derivatives of all orders) feature mapping\n",
    "\n",
    "$$\n",
    "    \\varphi: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{d}\n",
    "$$\n",
    "\n",
    "such that\n",
    "\n",
    "$$\n",
    "    \\tilde{A}_{i}:=\\varphi\\left(A_{i}\\right)=\\left\\{\\tilde{x} \\mid \\tilde{x}=\\varphi(x), x \\in A_{i}\\right\\},\\quad i=1,2, \\ldots, k\n",
    "$$\n",
    "\n",
    "are linearly separable.\n",
    "\n",
    "Remark 1. \n",
    "\n",
    "1. This definition is also consistent with the definition of\n",
    "linearly separable as we can just take  $\\tilde{d}=d$ and $\\varphi= id$ if $A_{1}, A_{2}, \\cdots, A_{k}$\n",
    "\n",
    "are already linearly separable.\n",
    "\n",
    "2.  The kernel method in SVM is mainly based on this idea for binary\n",
    "    case $(\\mathrm{k}=2)$ \n",
    "    \n",
    "    where they use kernel functions to approximate\n",
    "    this $\\varphi(x)$\n",
    "\n",
    "3.  For most commonly used deep learning models, they are all associated\n",
    "    with a softmax mapping which means that we can interpret these deep\n",
    "    learning models as the approximation for feature mapping $\\varphi$\n",
    "\n",
    "However, softmax is not so crucial for this definition actually as we\n",
    "have the next equivalent result. \n",
    "\n",
    "Theorem 1.\n",
    "\n",
    "$A_{1}, A_{2}, \\cdots, A_{k} \\subset \\mathbb{R}^{d}$ are nonlinearly separable is equivalent that there there exist a smooth classification\n",
    "function \n",
    "\n",
    "$$\n",
    "    \\psi: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{k}\n",
    "$$\n",
    "\n",
    "such that for all $i=1: k$\n",
    "\n",
    "and $j \\neq i$\n",
    "\n",
    "$$\n",
    "    \\psi_{i}(x)>\\psi_{j}(x), \\quad \\forall x \\in A_{i}\n",
    "$$\n",
    "\n",
    "Proof. On the one\n",
    "hand, it is easy to see that if $A_{1}, A_{2}, \\cdots, A_{k} \\subset \\mathbb{R}^{d}$\n",
    "\n",
    "are nonlinearly separable then they we can just take \n",
    "\n",
    "$$\n",
    "    \\psi(x)=p(\\varphi(x) ; \\theta)\n",
    "$$\n",
    "\n",
    "where the  $\\boldsymbol{p}(y ; \\theta)$\n",
    "\n",
    "is the softmax function for linearly separable sets $\\varphi\\left(A_{i}\\right)$ for \n",
    "$i = 1,2, \\cdots, k$\n",
    "\n",
    "On the other hand, let assume that $\\psi$\n",
    "\n",
    "is the smooth classification functions for  $A_{1}, A_{2}, \\cdots, A_{k} \\subset \\mathbb{R}^{d} $\n",
    "\n",
    "We claim that, we can take $\\varphi(x)=\\psi(x)$\n",
    "\n",
    "and then\n",
    "\n",
    "$$\n",
    "    \\varphi\\left(A_{1}\\right), \\varphi\\left(A_{2}\\right), \\cdots, \\varphi\\left(A_{k}\\right) \\subset \\mathbb{R}^{k} \\quad(\\tilde{d}=k)\n",
    "$$\n",
    "\n",
    "will be linearly separable. Actually, if you take $\\theta=(I, 0)$\n",
    "\n",
    "in softmax mapping $\\boldsymbol{p}(x ; \\theta)$ , then the monotonicity of $e^{x}$\n",
    "\n",
    "show that for all $i=1: k $ and  $j \\neq i $\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{p}_{i}(\\varphi(x) ; \\theta)=\\frac{e^{\\psi_{i}(x)}}{\\sum_{i=1}^{k} e^{\\psi_{i}(x)}}>\\frac{e^{\\psi_{j}(x)}}{\\sum_{i=1}^{k} e^{\\psi_{i}(x)}}=\\boldsymbol{p}_{j}(\\varphi(x) ; \\theta), \\quad \\forall x \\in A_{i} .\n",
    "$$\n",
    "\n",
    "\n",
    "Similarly to linearly separable sets, we have the next lemme for $k=2$\n",
    "\n",
    "Lemma 1. \n",
    "$A _{1}$ and $A_{2}$ are nonlinearly separable is equivalent that there exists a function $ \\varphi: \\mathbb{R}^{d} \\mapsto \\mathbb{R} $ such that\n",
    "\n",
    "$$\n",
    "    \\varphi(x)>0 \\quad \\forall x \\in A_{1} \\quad \\text { and } \\quad \\varphi(x)<0 \\quad \\forall x \\in A_{2}\n",
    "$$\n",
    "\n",
    "Proof. Based the equivalence of nonlinearly separable sets, there exists $\\psi_{1}(x)$ and $\\psi_{2}(2)$\n",
    "\n",
    "such that for all $i=1: 2$ and $j \\neq i $\n",
    "    \n",
    "$$\n",
    "    \\psi_{i}(x)>\\psi_{j}(x), \\quad \\forall x \\in A_{i}\n",
    "$$\n",
    "\n",
    "Then, we can just take \n",
    "\n",
    "$$\n",
    "    \\varphi(x)=\\psi_{1}(x)-\\psi_{2}(x)\n",
    "$$\n",
    "\n",
    "On the other hand, if there exist $\\varphi(x)$, then we can construct $\\psi_{1}(x)$ and $\\psi_{2}(2)$ as\n",
    "\n",
    "$$\n",
    "    \\psi_{1}(x)=\\frac{1}{2} \\varphi(x) \\quad \\text { and } \\quad \\psi_{2}(x)=-\\frac{2}{2} \\varphi(x)\n",
    "$$\n",
    "\n",
    "Remark 2. Here we mention that, we only assume that for all $i=1: k$ and $j \\neq i$ we have $\\psi_{i}(x)>\\psi_{j}(x), \\forall x \\in A_{i}$ for nonlinearly separable. We do not assume that $\\psi_{i}(x) \\geq 0$ or $\\sum_{i=1}^{k} \\psi_{i}(x)=1$ , which means that\n",
    "\n",
    "$$\n",
    "    \\psi(x)=\\left(\\begin{array}{c}\\psi_{1}(x) \\\\ \\psi_{2}(x) \\\\ \\vdots \\\\ \\psi_{k}(x)\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "is not a discrete probability distribution over all k classes.\n",
    "\n",
    "The previous theorem shows that softmax function is not so crucial in\n",
    "nonlinearly separable case. Combined with deep learning models, we have\n",
    "the following understanding about what deep learning models are\n",
    "approximating.\n",
    "\n",
    "1.  If a classification model is followed with a softmax, then the it is\n",
    "    approximating the feature mapping $\\varphi: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{\\bar{d}}$\n",
    "\n",
    "2.  If the classification model dose not followed by softmax, then it is\n",
    "    approximating $\\psi: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{k}$ directly.\n",
    "\n",
    "Example 1. Consider $k=2$ and $A_{1} \\subset\\left\\{(x, y) \\mid x^{2}+y^{2}<1\\right\\}, \\quad A_{2} \\subset\\left\\{(x, y) \\mid x^{2}+y^{2}>1\\right\\}$, then we can have the following nonlinear feature mapping:\n",
    "\n",
    "![image](images/img1.png)\n",
    "\n",
    "Here we have the following comparison for linear and nonlinear models\n",
    "from the viewpoint of loss functions:\n",
    "\n",
    "Linear case (Logistic regression):\n",
    "\n",
    "$$\n",
    "    L_{\\lambda}(\\theta)=\\sum_{j=1}^{N} \\ell\\left(y_{j}, p\\left(x_{j} ; \\theta\\right)\\right)+\\lambda R(\\|\\theta\\|)\n",
    "$$\n",
    "\n",
    "Nonlinear case:\n",
    "\n",
    "$$\n",
    "    L_{\\lambda}(\\theta)=\\sum_{j=1}^{N} \\ell\\left(y_{j}, p\\left(\\varphi\\left(x_{j} ; \\theta_{1}\\right) ; \\theta_{2}\\right)\\right)+\\lambda R(\\|\\theta\\|)\n",
    "$$\n",
    "\n",
    "Remark 3. We have the following remarks.\n",
    "\n",
    "1. $\\ell(q, p)=\\sum_{i=1}^{k}-q_{i} \\log p_{i} \\leftrightarrow \\text { cross-entropy }$\n",
    "\n",
    "2.  $p(x ; \\theta)=\\operatorname{softmax}(W x+b)$ where $\\theta=(W, b)$\n",
    "\n",
    "3.  $\\theta=\\left(\\theta_{1}, \\theta_{2}\\right)$ for nonlinear case\n",
    "\n",
    "4.  $\\lambda R(\\|\\theta\\|) \\leftrightarrow$ regularization term\n",
    "\n",
    "In general, we have the following popular nonlinear models for $\\varphi(x ; \\theta)$\n",
    "\n",
    "1.  Polynomials.\n",
    "\n",
    "2.  Piecewise polynomials (finite element method).\n",
    "\n",
    "3.  Kernel functions in SVM.\n",
    "\n",
    "4.  Deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a1ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}