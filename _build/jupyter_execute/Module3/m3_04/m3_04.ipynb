{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "    width=\"800\"\n",
       "    height=\"500\"\n",
       "    src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_1x52r7v2&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_3eff9dla\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe4bd25bfd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src= \"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_1x52r7v2&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_3eff9dla\" ,width='800', height='500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "    width=\"800\"\n",
       "    height=\"500\"\n",
       "    src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_wwt0aak3&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_u1s5u1jt\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe4bd27d400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_wwt0aak3&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_u1s5u1jt\",width='800', height='500') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the lecture notes here: [Notes](https://sites.psu.edu/math452/files/2022/01/C04_-Deep-neural-network-functions.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: from finite element to neural network\n",
    "\n",
    "\n",
    "In this chapter, we will introduce the so-called shallow neural network\n",
    "(deep neural network with one hidden layer) from the viewpoint of finite\n",
    "element method.\n",
    "\n",
    "Let us first consider the linear finite element functions on the unit\n",
    "interval $\\bar{\\Omega}=$ $[0,1]$ in $1 \\mathrm{D}$. We then consider a\n",
    "set of equidistant girds $\\Omega_{\\ell}$ of level $\\ell$ on the unit\n",
    "interval $\\bar{\\Omega}=[0,1]$ and mesh length $h_{\\ell}=2^{-\\ell}$. The\n",
    "grid points $x_{\\ell, i}$ are given by\n",
    "\n",
    "$$\n",
    "    x_{\\ell, i}:=i h_{\\ell}, \\quad 0 \\leq i \\leq 2^{\\ell} \n",
    "$$\n",
    "\n",
    "For $\\ell=1$, we denote the special hat function:\n",
    "\n",
    "$$\n",
    "    \\varphi(x)= \\begin{cases}2 x & x \\in\\left[0, \\frac{1}{2}\\right] \\\\ 2(1-x) & x \\in\\left[\\frac{1}{2}, 1\\right] \\\\ 0, & \\text { others }\\end{cases}\n",
    "$$\n",
    "\n",
    "The next diagram shows this basis function:\n",
    "\n",
    "```{figure} ./images/2022_01_05_e65f0d6bf0db4974ee45g-05.jpg\n",
    ":height: 150px\n",
    ":name: img1\n",
    "Diagram of $\\varphi(x)$\n",
    "```\n",
    "\n",
    "Then, for any nodal basis function\n",
    "$\\varphi_{\\ell, i}$ as below:\n",
    "\n",
    "```{figure} ./images/2022_01_05_e65f0d6bf0db4974ee45g-06.jpg\n",
    ":height: 150px\n",
    ":name: img2\n",
    "Diagram of $\\varphi_{\\ell, i}(x)$\n",
    "```\n",
    "\n",
    "in a fine grid $\\mathcal{T}_{\\ell}$ can be written as\n",
    "\n",
    "$$\n",
    "    \\varphi_{\\ell, i}=\\varphi\\left(\\frac{x-x_{\\ell, i-1}}{2 h_{\\ell}}\\right)=\\varphi\\left(w_{\\ell} x+b_{\\ell, i}\\right)\n",
    "$$\n",
    "\n",
    "That is to say, any $\\varphi_{\\ell, i}(x)$ can be obtained from\n",
    "$\\varphi(x)$ ba scaling (dilation) and translation with\n",
    "\n",
    "$$\n",
    "    w_{\\ell}=2^{\\ell-1}, \\quad b_{\\ell, i}=\\frac{-(i-1)}{2}\n",
    "$$\n",
    "\n",
    "in $\\varphi_{\\ell, i}=\\varphi\\left(w_{\\ell} x+b_{\\ell, i}\\right)$.\n",
    "\n",
    "Let recall the finite element interpolation as\n",
    "\n",
    "$$\n",
    "    u(x) \\approx u_{\\ell}(x):=\\sum_{0 \\leq i \\leq 2^{\\ell}} u\\left(x_{\\ell, i}\\right) \\varphi_{\\ell, i}(x)\n",
    "$$\n",
    "\n",
    "for any smooth function $u(x)$ on $(0,1)$. The above interpolation will\n",
    "converge as $\\ell \\rightarrow \\infty$, which show that\n",
    "\n",
    "$$\n",
    "    \\operatorname{span}\\left\\{\\varphi\\left(w_{\\ell} x+b_{\\ell, i}\\right)\\right\\} \\quad \\text { is dense in } \\quad H^{1}(0,1)\n",
    "$$\n",
    "\n",
    "Thus, we may have the next concise relation:\n",
    "\n",
    "FE space $=\\operatorname{span}\\left\\{\\varphi\\left(w_{\\ell} x+b_{\\ell, i}\\right) \\mid 0 \\leq i \\leq 2^{\\ell}, \\ell=1,2, \\cdots\\right\\} \\subset \\operatorname{span}\\{\\varphi(w x+b) \\mid w, b \\in \\mathbb{R}\\} .$\n",
    "\n",
    "In other words, the finite element space can be understood as the linear\n",
    "combination of $\\varphi(w x+b)$ with certain special choice of $w$ and\n",
    "$b$.\n",
    "\n",
    "Here, we need to point that this\n",
    "$\\operatorname{span}\\{\\varphi(w x+b) \\mid w, b \\in \\mathbb{R}\\}$ is\n",
    "exact the deep neural networks with one hidden layer (shallow neural\n",
    "networks) with activation function $\\varphi(x)$. More precisely,\n",
    "\n",
    "$$\n",
    "    f \\in \\operatorname{span}\\{\\varphi(w x+b) \\mid w, b \\in \\mathbb{R}\\}\n",
    "$$\n",
    "\n",
    "means there exist positive integer $N$ and $w_{j}, b_{j} \\in \\mathbb{R}$\n",
    "such that \n",
    "\n",
    "$$\n",
    "    f=\\sum_{j=1}^{N} a_{j} \\varphi\\left(w_{j} x+b_{j}\\right)\n",
    "$$\n",
    "\n",
    "The above function is also called one hidden neural network function\n",
    "with $N$ neurons.\n",
    "\n",
    "```{admonition} Remark\n",
    "1- By making $w_{\\ell}$ and $b_{\\ell, i}$ arbitrary,\n",
    "we get a much larger class of function which is exact a special neural\n",
    "network with activation function $\\varphi(x)$.\n",
    "\n",
    "2- Generalizations:\n",
    "\n",
    "a\\) $\\varphi$ can be different, such as\n",
    "$\\operatorname{ReLU}(x)=\\max \\{0, x\\}$.\n",
    "\n",
    "b\\) There is a natural extension for hight dimension $d$ as\n",
    "\n",
    "$$\n",
    "    \\{\\varphi(w \\cdot x+b)\\}\n",
    "$$\n",
    "\n",
    "where\n",
    "$w \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}$ and\n",
    "$w \\cdot x=\\sum_{i=1}^{d} w_{i} x_{i}$. This is called “deep” neural\n",
    "network with one hidden layer.\n",
    "```\n",
    "\n",
    "## Why we need deep neural networks via composition\n",
    "\n",
    "### FEM ans DNN $_{1}$ in 1D\n",
    "\n",
    "Thanks to the connection between $\\varphi(x)$ and\n",
    "$\\operatorname{ReLU}(x)=\\{0, x\\}$\n",
    "\n",
    "$$\n",
    "    \\varphi(x)=2 \\operatorname{ReLU}(x)-4 \\operatorname{ReLu}\\left(x-\\frac{1}{2}\\right)+2 \\operatorname{ReLU}(x-1)\n",
    "$$\n",
    "\n",
    "It suffices to show that each basis function $\\varphi_{\\ell, i}$ can be\n",
    "represented by a ReLU DNN. We first note that the basis function\n",
    "$\\varphi_{i}$ has the support in $\\left[x_{i-1}, x_{i+1}\\right]$ can be\n",
    "easily written as\n",
    "\n",
    "$$\n",
    "    \\varphi_{\\ell, i}(x)=\\frac{1}{h_{\\ell}} \\operatorname{ReLU}\\left(x-x_{\\ell, i-1}\\right)-\\left(\\frac{2}{h_{\\ell}}\\right) \\operatorname{ReLU}\\left(x-x_{\\ell, i}\\right)+\\frac{1}{h_{\\ell}} \\operatorname{ReLU}\\left(x-x_{\\ell, i+1}\\right)\n",
    "$$\n",
    "\n",
    "More generally, if function $\\varphi_{i}$ is not on the uniform grid but\n",
    "has support in $\\left[x_{i-1}, x_{i+1}\\right]$ can be easily written as\n",
    "\n",
    "$$\n",
    "    \\varphi_{i}(x)=\\frac{1}{h_{i-1}} \\operatorname{ReLU}\\left(x-x_{i-1}\\right)-\\left(\\frac{1}{h_{i-1}}+\\frac{1}{h_{i}}\\right) \\operatorname{ReLU}\\left(x-x_{i}\\right)+\\frac{1}{h_{i}} \\operatorname{ReLU}\\left(x-x_{i+1}\\right)\n",
    "$$\n",
    "\n",
    "where $h_{i}=x_{i+1}-x_{i}$.\n",
    "\n",
    "Thus is to say, we have the next theorem.\n",
    "\n",
    "```{prf:theorem}\n",
    ":label: thm34_1\n",
    "For $d=1$, and $\\Omega \\subset \\mathbb{R}^{d}$ is a bounded\n",
    "interval, then DNN $_{1}$ can be used to cover all linear finite element\n",
    "function in on $\\Omega$.\n",
    "```\n",
    "\n",
    "### Linear finite element cannot be recovered by DNN $_{1}$ for $d \\geq 2$\n",
    "\n",
    "In view of {prf:ref}`thm34_1` and the fact that\n",
    "$\\mathrm{DNN}_{\\mathrm{J}} \\subseteq \\mathrm{DNN}_{\\mathrm{J}+1}$, it is\n",
    "natural to ask that how many layers are needed at least to recover all\n",
    "linear finite element functions in $\\mathbb{R}^{d}$ for $d \\geq 2$. In\n",
    "this section, we will show that\n",
    "\n",
    "$$\n",
    "    J_{d} \\geq 2, \\quad \\text { if } \\quad d \\geq 2\n",
    "$$\n",
    "\n",
    "where $J_{d}$ is the minimal $J$ such that all linear finite element functions in\n",
    "$\\mathbb{R}^{d}$ can be recovered by DNN $_{J}$\n",
    "\n",
    "In particular, we will show the following theorem.\n",
    "\n",
    "```{prf:theorem}\n",
    ":label: thm34_2\n",
    "If $\\Omega \\subset \\mathbb{R}^{d}$ is either a bounded domain\n",
    "or $\\Omega=\\mathbb{R}^{d}, \\mathrm{DNN}_{1}$ can not be used to recover\n",
    "all linear finite element functions on $\\Omega$.\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "We prove it by contradiction. Let us assume that for any\n",
    "continuous piecewise linear function $f: \\Omega \\rightarrow \\mathbb{R}$,\n",
    "we can find finite $N \\in \\mathbb{N}, w_{i} \\in \\mathbb{R}^{1, d}$ as\n",
    "row vector and $\\alpha_{i}, b_{i}, \\beta \\in \\mathbb{R}$ such that\n",
    "\n",
    "$$\n",
    "    f=\\sum_{i=1}^{N} \\alpha_{i} \\operatorname{ReLU}\\left(w_{i} x+b_{i}\\right)+\\beta\n",
    "$$\n",
    "\n",
    "with $f_{i}=\\alpha_{i} \\operatorname{ReLU}\\left(w_{i} x+b_{i}\\right), \\alpha_{i} \\neq 0$\n",
    "and $w_{i} \\neq 0 .$ Consider the finite element functions, if this one\n",
    "hidden layer ReLU DNN can recover any basis function of FEM, then it can\n",
    "recover the finite element space. Thus let us assume $f$ is a locally\n",
    "supported basis function for FEM. Furthermore, if $\\Omega$ is a bounded\n",
    "domain, we assume that \n",
    "\n",
    "$$\n",
    "    d(\\operatorname{supp}(f), \\partial \\Omega)>0\n",
    "$$ (eq1_15)\n",
    "\n",
    "with \n",
    "\n",
    "$$\n",
    "    d(A, B)=\\inf _{x \\in A, y \\in B}\\|x-y\\|\n",
    "$$\n",
    "\n",
    "as the distance of two closed sets.\n",
    "\n",
    "A more important observation is that\n",
    "$\\nabla f: \\Omega \\rightarrow \\mathbb{R}^{d}$ is a piecewise constant\n",
    "vector function. The key point is to consider the discontinuous points\n",
    "for $g:=\\nabla f=$ $\\sum_{i=1}^{N} \\nabla f_{i} .$\n",
    "\n",
    "For more general case, we can define the set of discontinuous points of\n",
    "a function by\n",
    "\n",
    "$$\n",
    "    D_{g}:=\\{x \\in \\Omega \\mid x \\text { is a discontinuous point of } g\\}\n",
    "$$\n",
    "\n",
    "Because of the property that\n",
    "\n",
    "$$\n",
    "    D_{f+g} \\supseteq D_{f} \\cup D_{g} \\backslash\\left(D_{f} \\cap D_{g}\\right)\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "    D_{\\sum_{i=1}^{N} g_{i}} \\supseteq \\bigcup_{i=1}^{N} D_{g_{i}} \\backslash \\bigcup_{i \\neq j}\\left(D_{g_{i}} \\cap D_{g_{j}}\\right)\n",
    "$$\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\n",
    "    g_{i}=\\nabla f_{i}(x)=\\nabla\\left(\\alpha_{i} \\operatorname{ReLU}\\left(w_{i} x+b_{i}\\right)\\right)=\\left(\\alpha_{i} H\\left(w_{i} x+b_{i}\\right)\\right) w_{i} \\in \\mathbb{R}^{d}\n",
    "$$ (eq1_18)\n",
    "\n",
    "for $i=1: N$ with $H$ be the Heaviside function defined as:\n",
    "\n",
    "$$\n",
    "    H(x)= \\begin{cases}0 & \\text { if } x \\leq 0 \\\\ 1 & \\text { if } x>0\\end{cases}\n",
    "$$\n",
    "\n",
    "This means that \n",
    "\n",
    "$$\n",
    "    D_{g_{i}}=\\left\\{x \\mid w_{i} x+b_{i}=0\\right\\}\n",
    "$$ (eq1_19)\n",
    "\n",
    "is a $d-1$ dimensional affine space in $\\mathbb{R}^{d}$\n",
    "\n",
    "Without loss of generality, we can assume that\n",
    "\n",
    "$$\n",
    "    D_{g_{i}} \\neq D_{g_{j}}\n",
    "$$\n",
    "\n",
    "When the other case occurs, i.e. $D_{g_{6}}=D_{g_{2}}=\\cdots=D_{g_{\\ell_{2}}}$, by the definition of\n",
    "$g_{i}$ in {eq}`eq1_18` and $D_{g_{i}}$ in {eq}`eq1_19`, this happens if and only if\n",
    "there is a row vector $(w, b)$ such that\n",
    "\n",
    "$$\n",
    "    c_{\\ell_{i}}(w b)=\\left(w_{\\ell_{i}} b_{\\ell_{i}}\\right)\n",
    "$$ (eq1_21)\n",
    "\n",
    "with some $c_{\\ell_{i}} \\neq 0$ for $i=1: k$. We combine those $g_{\\ell_{i}}$ as\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "\\tilde{g}_{\\ell} &=\\sum_{i=1}^{k} g_{\\ell_{i}}=\\sum_{i=1}^{k} \\alpha_{\\ell_{i}} H\\left(w_{\\ell_{i}} x+b_{\\ell_{i}}\\right) w_{\\ell_{i}}, \\\\\n",
    "&=\\sum_{i=1}^{k}\\left(c_{\\ell_{i}} \\alpha_{\\ell_{i}} H\\left(c_{\\ell_{i}}(w x+b)\\right)\\right) w, \\\\\n",
    "&=\\left\\{\\begin{array}{lll}\n",
    "\\left(\\sum_{i=1}^{k} c_{\\ell_{i}} \\alpha_{\\ell_{i}} H\\left(c_{\\ell_{i}}\\right)\\right) w & \\text { if } & w x+b>0, \\\\\n",
    "\\left(\\sum_{i=1}^{k} c_{\\ell_{i}} \\alpha_{\\ell_{i}} H\\left(-c_{\\ell_{i}}\\right)\\right) w & \\text { if } & w x+b \\leq 0 .\n",
    "\\end{array}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, if\n",
    "\n",
    "$$\n",
    "    \\left(\\sum_{i=1}^{k} c_{\\ell_{i}} \\alpha_{\\ell_{i}} H\\left(c_{\\ell_{i}}\\right)\\right)=\\left(\\sum_{i=1}^{k} c_{\\ell_{i}} \\alpha_{\\ell_{i}} H\\left(-c_{\\ell_{i}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "$\\tilde{g}_{\\ell}$ is a constant vector function, that is to say\n",
    "$D_{\\sum_{i=1}^{k} g_{\\ell_{i}}}=D_{\\tilde{g}_{\\ell}}=\\emptyset .$\n",
    "Otherwise, $\\tilde{g}_{\\ell}$ is a piecewise constant vector function\n",
    "with the property that\n",
    "\n",
    "$$\n",
    "    D_{\\sum_{i=1}^{k} g_{\\ell_{i}}}=D_{\\tilde{g}_{\\ell}}=D_{g_{\\ell_{i}}}=\\{x \\mid w x+b=0\\} \n",
    "$$\n",
    "\n",
    "This means that we can use condition {eq}`eq1_21` as an equivalence relation\n",
    "and split $\\left\\{g_{i}\\right\\}_{i=1}^{N}$ into some groups, and we can\n",
    "combine those $g_{\\ell_{i}}$ in each group as what we do above. After\n",
    "that, we have\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^{N} g_{i}=\\sum_{\\ell=1}^{\\bar{N}} \\tilde{g}_{\\ell}\n",
    "$$\n",
    "\n",
    "with $D_{\\tilde{g}_{s}} \\neq D_{\\bar{g}_{t}}$. Finally, we can have that\n",
    "$D_{\\tilde{g}_{s}} \\cap D_{\\bar{g}_{t}}$ is an empty set or a $d-2$\n",
    "dimensional affine space in $\\mathbb{R}^{d} .$ Since $N \\leq N$ is a\n",
    "finite number,\n",
    "\n",
    "$$\n",
    "    D:=\\bigcup_{i=1}^{N} D_{\\tilde{g}_{\\ell}} \\backslash \\bigcup_{s \\neq t}\\left(D_{\\tilde{g}_{s}} \\cap D_{\\tilde{g}_{t}}\\right)\n",
    "$$\n",
    "\n",
    "is an unbounded set.\n",
    "\n",
    "-   If $\\Omega=\\mathbb{R}^{d}$,\n",
    "\n",
    "$$\n",
    "    \\operatorname{supp}(\\mathrm{f}) \\supseteq D_{g}=D_{\\sum_{i=1}^{N} g_{i}}=D_{\\sum_{t=1}^{N} \\tilde{g}_{\\ell}} \\supseteq D\n",
    "$$\n",
    "\n",
    "is contradictory to the assumption that $f$ is locally supported.\n",
    "\n",
    "-   If $\\Omega$ is a bounded domain,\n",
    "\n",
    "$$\n",
    "    d(D, \\partial \\Omega)= \\begin{cases}s>0 & \\text { if } D_{\\tilde{g}_{i}} \\cap \\Omega=\\emptyset, \\forall i \\\\ 0 & \\text { otherwise. }\\end{cases}\n",
    "$$\n",
    "\n",
    "Note again that all $D_{\\tilde{g}_{i}}$ ’s are $d-1$ dimensional affine\n",
    "spaces, while $D_{\\tilde{g}_{i}} \\cap D_{\\tilde{g}_{j}}$ is either an\n",
    "empty set or a d-2 dimensional affine space. If\n",
    "$d(D, \\partial \\Omega)>0$, this implies that $\\nabla f$ is continuous in\n",
    "$\\Omega$, which contradicts the assumption that $f$ is a basis function\n",
    "in FEM. If $d(D, \\partial \\Omega)=0$, this contradicts the previous\n",
    "assumption in {eq}`eq1_15`.\n",
    "\n",
    "Hence DNN $_{1}$ cannot recover any piecewise linear function in\n",
    "$\\Omega$ for $d \\geq 2 .$\n",
    "```\n",
    "\n",
    "Following the proof above, we have the following theorem\n",
    "\n",
    "```{prf:theorem}\n",
    "$\\left\\{\\operatorname{ReLU}\\left(w_{i} x+b_{i}\\right)\\right\\}_{i=1}^{m}$\n",
    "are linearly independent if $\\left(w_{i}, b_{i}\\right)$ and\n",
    "$\\left(w_{j}, b_{j}\\right)$ are linearly independent in\n",
    "$\\mathbb{R}^{1 \\times(d+1)}$ for any $i \\neq j .$\n",
    "```\n",
    "\n",
    "## Definition of neural network space\n",
    "\n",
    "\n",
    "- Primary variables $n_{0}=d$\n",
    "\n",
    "$$\n",
    "    x^{0}=x=\\left(\\begin{array}{c}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{n}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "- $n_{1}$ hyperplanes\n",
    "\n",
    "$$\n",
    "    W^{1} x+b^{1}=\\left(\\begin{array}{c}\n",
    "w_{1}^{1} x+b_{1}^{1} \\\\\n",
    "w_{2}^{1} x+b_{2}^{1} \\\\\n",
    "\\vdots \\\\\n",
    "w_{n}^{1} x+b_{n}^{1}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "- $n_{1}$-neurons:\n",
    "\n",
    "$$\n",
    "    x^{1}=\\sigma\\left(W^{1} x+b^{1}\\right)=\\left(\\begin{array}{c}\n",
    "\\sigma\\left(w_{1}^{1} x+b_{1}^{1}\\right) \\\\\n",
    "\\sigma\\left(w_{2}^{1} x+b_{2}^{1}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma\\left(w_{n}^{1} x+b_{n}^{1}\\right)\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "- $n_{2}$-hyperplanes\n",
    "\n",
    "$$\n",
    "    W^{2} x^{1}+b^{2}=\\left(\\begin{array}{c}\n",
    "w_{1}^{2} x^{1}+b_{2}^{2} \\\\\n",
    "w_{2}^{2} x^{1}+b_{2}^{2} \\\\\n",
    "\\vdots \\\\\n",
    "w_{n}^{2} x^{1}+b_{n}^{2}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Shallow neural network functions:\n",
    "\n",
    "${ }_{n} \\mathrm{~N}\\left(n_{1}, n_{2}\\right)={ }_{n} \\mathrm{~N}\\left(\\sigma ; n_{1}, n_{2}\\right)=\\left\\{W^{2} x^{1}+b^{2}, x^{1}=\\sigma\\left(W^{1} x+b^{1}\\right)\\right.$\n",
    "with\n",
    "$\\left.W^{\\ell} \\in \\mathbb{R}^{n_{\\ell} \\times n_{\\ell-1}}, b^{\\ell} \\in \\mathbb{R}^{n_{\\ell}}, \\ell=1,2, n_{0}=d\\right\\}$\n",
    "\n",
    "${ }_{n} \\mathrm{~N}\\left(\\sigma ; n_{1}, n_{2}, \\ldots, n_{L}\\right)=\\left\\{W^{L} x^{L-1}+b^{L}, x^{\\ell}=\\sigma\\left(W^{\\ell} x^{\\ell-1}+b^{\\ell}\\right)\\right.$\n",
    "with\n",
    "$\\left.W^{\\ell} \\in \\mathbb{R}^{n_{\\ell} \\times n_{\\ell-1}}, b^{\\ell} \\in \\mathbb{R}^{n_{\\ell}}, \\ell=1: L, n_{0}=d, x^{0}=x\\right\\}$\n",
    "\n",
    "First, let us first define the so-called 1-hidden layer (shallow) neural\n",
    "network.\n",
    "\n",
    "The 1 -hidden layer (shallow) neural network is defined as:\n",
    "\n",
    "$$\n",
    "    { }_{n} \\mathrm{~N}={ }_{n} \\mathrm{~N}(\\sigma)={ }_{n} \\mathrm{~N}^{1}(\\sigma)=\\bigcup_{n_{1} \\geq 1} \\mathrm{~N}\\left(\\sigma ; n_{1}, 1\\right)\n",
    "$$\n",
    "\n",
    "The 2-hidden layer (shallow) neural network is defined as:\n",
    "\n",
    "$$\n",
    "    { }_{n} \\mathrm{~N}^{2}(\\sigma)=\\bigcup_{n_{1}, n_{2} \\geq 1} \\mathrm{~N}\\left(\\sigma ; n_{1}, n_{2}, 1\\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}