{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0e91e0",
   "metadata": {},
   "source": [
    "# Optimization and gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8bf96a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "    width=\"800\"\n",
       "    height=\"500\"\n",
       "    src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_wota11ay&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_o38cisoq\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f9a9125cfd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_wota11ay&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_o38cisoq\",width='800', height='500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7938d",
   "metadata": {},
   "source": [
    "## Download the lecture notes here: [Notes](https://sites.psu.edu/math452/files/2021/12/A06GradientDescent_Video_Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546dbedf",
   "metadata": {},
   "source": [
    "## Gradient descent method\n",
    "\n",
    "For simplicity, let us just consider a general optimization problem\n",
    "\n",
    "$$\n",
    "    \\label{optmodel}\n",
    "    \\min_{x\\in \\mathbb{R}^n } f(x).\n",
    "$$ (problem)\n",
    "\n",
    "![image](../figures/diag_GD.png)\n",
    "\n",
    "### A general approach: line search method\n",
    "\n",
    "Given any initial guess $x_1$, the line search method uses the following\n",
    "algorithm\n",
    "\n",
    "$$\n",
    "    \\eta_t= argmin_{\\eta\\in \\mathbb{R}^1} f(x_t - \\eta p_t)\\qquad \\mbox{(1D minimization problem)}\n",
    "$$\n",
    "\n",
    "to produce $\\{ x_{t}\\}_{t=1}^{\\infty}$ \n",
    "\n",
    "$$\n",
    "    x_{t+1} = x_{t} - \\eta_t p_t.\n",
    "$$ (line-search)\n",
    "\n",
    "Here $\\eta_t$ is called the step size in\n",
    "optimization and also learning rate in machine learn\n",
    "ing, $p_t$ is called\n",
    "the descent direction, which is the critical component of this\n",
    "algorithm. And $x_t$ tends to\n",
    "\n",
    "$$\n",
    "    x^*= argmin_{x\\in \\mathbb{R}^n} f(x) \\iff f(x^*)=\\min_{x\\in \\mathbb{R}^n} f(x)\n",
    "$$\n",
    "\n",
    "as $t$ tends to infinity. There is a series of optimization algorithms\n",
    "which follow the above form just using different choices of $p_t$.\n",
    "\n",
    "Then, the next natural question is what a good choice of $p_t$ is? We\n",
    "have the following theorem to show why gradient direction is a good\n",
    "choice for $p_t$.\n",
    "\n",
    "```{admonition} lemma\n",
    "Given $x \\in \\mathbb{R}^n$, if $\\nabla f(x)\\neq 0$, the fast descent\n",
    "direction of $f$ at $x$ is the negative gradient direction, namely\n",
    "\n",
    "$$\n",
    "    -\\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|} = \\mathop{\\arg\\min}_{ p \\in \\mathbb{R}^n, \\|p\\|=1} \\left. \\frac{\\partial f(x + \\eta p)}{\\partial \\eta} \\right|_{\\eta=0}.\n",
    "$$\n",
    "\n",
    "It means that $f(x)$ decreases most rapidly along the negative gradient\n",
    "direction.\n",
    "```\n",
    "\n",
    "```{admonition} proof\n",
    "*Proof.* Let $p$ be a direction in $\\mathbb{R}^{n},\\|p\\|=1$. Consider\n",
    "the local decrease of the function $f(\\cdot)$ along direction $p$\n",
    "\n",
    "$$\n",
    "    \\Delta(p)=\\lim _{\\eta \\downarrow 0} \\frac{1}{\\eta}\\left(f(x+\\eta p)-f(x)\\right)=\\left. \\frac{\\partial f(x + \\eta p)}{\\partial \\eta} \\right|_{\\eta=0}.\n",
    "$$\n",
    "\n",
    "Note that \n",
    "\n",
    "$$\n",
    "    \\begin{split}\n",
    "\\left. \\frac{\\partial f(x + \\eta p)}{\\partial \\eta} \\right|_{\\eta=0}=\\sum_{i=1}^n\\left. \\frac{\\partial f}{\\partial x_i}(x + \\eta p)p_i \\right|_{\\eta=0} =(\\nabla f, p),\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "which means that\n",
    "\n",
    "$$\n",
    "    f(x+\\eta p)-f(x)=\\eta(\\nabla f(x), p)+o(\\eta) .\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "    \\Delta(p)=(\\nabla f(x), p).\n",
    "$$\n",
    "\n",
    "Using the Cauchy-Schwarz inequality\n",
    "$-\\|x\\| \\cdot\\|y\\| \\leq( x, y) \\leq\\|x\\| \\cdot\\|y\\|,$ we obtain\n",
    "\n",
    "$$\n",
    "    -\\|\\nabla f(x)\\| \\le (\\nabla f(x), p)\\le \\|\\nabla f(x)\\| .\n",
    "$$\n",
    "\n",
    "Let us take\n",
    "\n",
    "$$\n",
    "    \\bar{p}=-\\nabla f(x) /\\|\\nabla f(x)\\|.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "    \\Delta(\\bar{p})=-(\\nabla f(x), \\nabla f(x)) /\\|\\nabla f(x)\\|=-\\|\\nabla f(x)\\|.\n",
    "$$\n",
    "\n",
    "The direction $-\\nabla f(x)$ (the antigradient) is the direction of the\n",
    "fastest local decrease of the function $f(\\cdot)$ at point $x.$ ◻\n",
    "```\n",
    "\n",
    "Here is a simple diagram for this property.\n",
    "\n",
    "Since at each point, $f(x)$ decreases most rapidly along the negative\n",
    "gradient direction, it is then natural to choose the search direction in\n",
    "{eq}`line-search`  in the negative gradient direction and the\n",
    "resulting algorithm is the so-called gradient descent method.\n",
    "\n",
    "```{prf:algorithm} Algrihthm\n",
    ":label: my_algorithm1\n",
    "Given the initial guess $x_0$, learning rate $\\eta_t>0$\n",
    "\n",
    "**For** t=1,2,$\\cdots$,\n",
    "\n",
    "$$\n",
    "    x_{t+1} =  x_{t} - \\eta_{t} \\nabla f({x}_{t}),\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In practice, we need a \"stopping criterion\" that determines when the\n",
    "above gradient descent method to stop. One possibility is\n",
    "\n",
    "> **While** $S(x_t; f) = \\|\\nabla f(x_t)\\|\\le \\epsilon$ or $t \\ge T$\n",
    "\n",
    "for some small tolerance $\\epsilon>0$ or maximal number of iterations\n",
    "$T$. In general, a good stopping criterion is hard to come by and it is\n",
    "a subject that has called a lot of research in optimization for machine\n",
    "learning.\n",
    "\n",
    "In the gradient method, the scalar factors for the gradients,\n",
    "$\\eta_{t},$ are called the step sizes. Of course, they must be positive.\n",
    "There are many variants of the gradient method, which differ one from\n",
    "another by the step-size strategy. Let us consider the most important\n",
    "examples.\n",
    "\n",
    "1.  The sequence $\\left\\{\\eta_t\\right\\}_{t=0}^{\\infty}$ is chosen in\n",
    "    advance. For example, (constant step)\n",
    "    \n",
    "    $$\n",
    "        \\eta_t=\\frac{\\eta}{\\sqrt{t+1}};\n",
    "    $$\n",
    "\n",
    "2.  Full relaxation:\n",
    "\n",
    "    $$\n",
    "        \\eta_t=\\arg \\min _{\\eta \\geq 0} f\\left(x_t-\\eta \\nabla f\\left(x_t\\right)\\right);\n",
    "    $$\n",
    "\n",
    "3.  The Armijo rule: Find $x_{t+1}=x_t-\\eta \\nabla f\\left(x_t\\right)$\n",
    "    with $\\eta>0$ such that\n",
    "    \n",
    "    $$\n",
    "        \\alpha\\left(\\nabla f\\left(x_t\\right), x_t-x_{t+1}\\right) \\leq f\\left(x_t\\right)-f\\left(x_{t+1}\\right),\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "        \\beta\\left(\\nabla f\\left(x_t\\right), x_t-x_{t+1}\\right) \\geq f\\left(x_t\\right)-f\\left(x_{t+1}\\right),\n",
    "    $$\n",
    "    \n",
    "    where $0<\\alpha<\\beta<1$ are some fixed parameters.\n",
    "\n",
    "Comparing these strategies, we see that\n",
    "\n",
    "1.  The first strategy is the simplest one. It is often used in the\n",
    "    context of convex optimization. In this framework, the behavior of\n",
    "    functions is much more predictable than in the general nonlinear\n",
    "    case.\n",
    "\n",
    "2.  The second strategy is completely theoretical. It is never used in\n",
    "    practice since even in one-dimensional case we cannot find the exact\n",
    "    minimum in finite time.\n",
    "\n",
    "3.  The third strategy is used in the majority of practical algorithms.\n",
    "    It has the following geometric interpretation. Let us fix\n",
    "    $x \\in \\mathbb{R}^{n}$ assuming that $\\nabla f(x) \\neq 0$. Consider\n",
    "    the following function of one variable:\n",
    "    \n",
    "    $$\n",
    "        \\phi (\\eta)=f(x-\\eta \\nabla f(x)),\\quad \\eta\\ge0.\n",
    "    $$\n",
    "    \n",
    "    Then the\n",
    "    step-size values acceptable for this strategy belong to the part of\n",
    "    the graph of $\\phi$ which is located between two linear functions:\n",
    "    \n",
    "    $$\n",
    "        \\phi_{1}(\\eta)=f(x)-\\alpha \\eta\\|\\nabla f(x)\\|^{2}, \\quad \\phi_{2}(\\eta)=f(x)-\\beta \\eta\\|\\nabla f(x)\\|^{2}\n",
    "    $$\n",
    "    \n",
    "    Note that $\\phi(0)=\\phi_{1}(0)=\\phi_{2}(0)$ and\n",
    "    $\\phi^{\\prime}(0)<\\phi_{2}^{\\prime}(0)<\\phi_{1}^{\\prime}(0)<0 .$\n",
    "    Therefore, the acceptable values exist unless $\\phi(\\cdot)$ is not\n",
    "    bounded below. There are several very fast one-dimensional\n",
    "    procedures for finding a point satisfying the Armijo conditions.\n",
    "    However, their detailed description is not important for us now.\n",
    "    \n",
    "\n",
    "##  Convergence of Gradient Descent method\n",
    "\n",
    "Now we are ready to study the rate of convergence of unconstrained\n",
    "minimization schemes. For the optimization problem {eq}`problem`\n",
    "\n",
    "\n",
    "$$\n",
    "    \\min_{x\\in \\mathbb{R}^n} f(x).\n",
    "$$\n",
    "\n",
    "We assume that $f(x)$ is convex. Then we say that $x^*$ is a minimizer if\n",
    "\n",
    "$$\n",
    "    f(x^*) = \\min_{x \\in \\mathbb{R}^n} f(x).\n",
    "$$\n",
    "\n",
    "For minimizer $x^*$, we have\n",
    "\n",
    "$$\n",
    "    \\label{key}\n",
    "    \\nabla f(x^*) = 0.\n",
    "$$\n",
    "\n",
    "We have the next two properties of the minimizer\n",
    "for convex functions:\n",
    "\n",
    "1.  If $f(x) \\ge c_0$, for some $c_0 \\in \\mathbb{R}$, then we have\n",
    "\n",
    "    $$\n",
    "        \\mathop{\\arg\\min} f \\neq \\emptyset.\n",
    "    $$\n",
    "\n",
    "2.  If $f(x)$ is $\\lambda$-strongly convex, then $f(x)$ has a unique\n",
    "    minimizer, namely, there exists a unique $x^*\\in \\mathbb{R}^n$ such\n",
    "    that\n",
    "    \n",
    "    $$\n",
    "        f(x^*) = \\min_{x\\in \\mathbb{R}^n }f(x).\n",
    "    $$\n",
    "\n",
    "To investigate the convergence of gradient descent method, let us recall\n",
    "the gradient descent method:\n",
    "\n",
    "```{prf:algorithm} Algorithm\n",
    ":label: my_algorithm2\n",
    "\n",
    "**For**: $t = 1, 2, \\cdots$ \n",
    " \n",
    "$$\n",
    "    \\label{equ:fgd-iteration}\n",
    "    x_{t+1} =  x_{t} - \\eta_t \\nabla f(x_t),\n",
    "$$\n",
    "\n",
    "where $\\eta_t$ is the stepsize / learning rate.\n",
    "```\n",
    "\n",
    "We have the next theorem about the convergence of gradient descent\n",
    "method under the Assumption.\n",
    "\n",
    "```{admonition} Theorem\n",
    "For Gradient Descent Algorithm {prf:ref}`my_algorithm2` , if\n",
    "$f(x)$ satisfies Assumption, then\n",
    "\n",
    "$$\n",
    "    \\|x_t - x^*\\|^2 \\le  \\alpha^t \\|x_0 - x^*\\|^2\n",
    "$$\n",
    "\n",
    "if $0<\\eta_t <\\frac{2\\lambda}{L^2}$ and $\\alpha < 1$.\n",
    "\n",
    "Particularly, if $\\eta_t = \\frac{\\lambda}{L^2}$, then\n",
    "\n",
    "$$\n",
    "    \\|x_t - x^*\\|^2 \\le  \\left(1 - \\frac{\\lambda^2}{L^2}\\right)^t \\|x_0 - x^*\\|^2.\n",
    "$$\n",
    "```\n",
    "\n",
    "```{admonition} Proof\n",
    "*Proof.* Note that \n",
    "\n",
    "$$\n",
    "    x_{t+1} - x =  x_{t} - \\eta_t \\nabla f(x_t)  - x.\n",
    "$$\n",
    "\n",
    "By taking $L^2$ norm for both sides, we get\n",
    "\n",
    "$$\n",
    "    \\|x_{t+1} - x \\|^2 = \\|x_{t} - \\eta_t \\nabla f(x_t) - x \\|^2.\n",
    "$$\n",
    "\n",
    "Let\n",
    "$x = x^*$. It holds that \n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\|x_{t+1} - x^* \\|^2 &=  \\| x_{t} - \\eta_t \\nabla f(x_t) - x^* \\|^2 \\\\\n",
    "    &= \\|x_t-x^*\\|^2 - 2\\eta_t \\nabla f(x_t)^\\top (x_t - x^*) + \\eta_t^2 \\|\\nabla f(x_t) - \\nabla f(x^*)\\|^2 \\qquad \\mbox{ (by $\\nabla f(x^*)=0$)}\\\\\n",
    "    &\\le \\|x_t - x^*\\|^2 - 2\\eta_t \\lambda \\|x_t - x^*\\|^2 + \\eta_t ^2 L^2 \\|x_t - x^*\\|^2  \\quad\n",
    "    \\mbox{(by $\\lambda$- strongly convex and Lipschitz)}\\\\\n",
    "    &\\le (1 - 2\\eta_t \\lambda + \\eta_t^2 L^2) \\|x_t - x^*\\|^2\n",
    "    =\\alpha \\|x_t - x^*\\|^2,\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\alpha = \\left(L^2 (\\eta_t  -{\\lambda\\over L^2})^2 + 1-{\\lambda^2\\over L^2}\\right)<1\\  \\mbox{if } 0< \\eta_t<\\frac{2\\lambda}{L^2}.\n",
    "$$\n",
    "\n",
    "Particularly, if $\\eta_t =\\frac{\\lambda}{L^2}$,\n",
    "\n",
    "$$\n",
    "    \\alpha=1-{\\lambda^2\\over L^2},\n",
    "$$ \n",
    "\n",
    "which finishes the proof. ◻\n",
    "```\n",
    "\n",
    "This means that if the learning rate is chosen appropriatly,\n",
    "$\\{x_t\\}_{t=1}^\\infty$ from the gradient descent method will converge to\n",
    "the minimizer $x^*$ of the function.\n",
    "\n",
    "There are some issues on Gradient Descent method:\n",
    "\n",
    "-   $\\nabla f(x_{t})$ is very expensive to compute.\n",
    "\n",
    "-   Gradient Descent method does not yield generalization accuracy.\n",
    "\n",
    "The stochastic gradient descent (SGD) method in the next section will\n",
    "focus on these two issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bf418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}