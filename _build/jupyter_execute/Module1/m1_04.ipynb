{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea2fe99",
   "metadata": {},
   "source": [
    "# KL-divergence and cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d04662be",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "    width=\"800\"\n",
       "    height=\"500\"\n",
       "    src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_1x5pta90&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_l1sjg1vv\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f9a1c17ef70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_1x5pta90&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_l1sjg1vv\"  ,width='800', height='500')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd11bca",
   "metadata": {},
   "source": [
    "## Download the lecture notes here: [Notes](https://sites.psu.edu/math452/files/2021/12/A04CrossEntropy_Video_Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b9dd8",
   "metadata": {},
   "source": [
    "Cross-entropy minimization is frequently used in optimization and\n",
    "rare-event probability estimation. When comparing a distribution against\n",
    "a fixed reference distribution, cross-entropy and KL divergence are\n",
    "identical up to an additive constant.\n",
    "\n",
    "The KL(Kullback--Leibler) divergence defines a special distance between\n",
    "two discrete probability distributions \n",
    "$\n",
    "p=\\left( \\begin{array}{ccc}\n",
    "p_1\\\\\n",
    "\\vdots \\\\\n",
    "p_k\n",
    "\\end{array} \\right),\\quad  q=\\left( \\begin{array}{ccc}\n",
    "q_1\\\\\n",
    "\\vdots \\\\\n",
    "q_k\n",
    "\\end{array} \\right\n",
    ")\n",
    "$\n",
    "with $\n",
    "0\\le p_i, q_i\\le1$\n",
    "and\n",
    "$\\sum_{i=1}^{k}p_i=\\sum_{i=1}^{k}q_i=1$ by $\n",
    "D_{\\rm KL}(q,p)= \\sum_{i=1}^k q_i\\log \\frac{q_i}{p_i}.$\n",
    "\n",
    "```{admonition} Lemma\n",
    "$D_{\\rm KL}(q,p)$ works like a \"distance\\\" without the symmetry:\n",
    "\n",
    "1.  $D_{\\rm KL}(q,p)\\ge0$;\n",
    "\n",
    "2.  $D_{\\rm KL}(q,p)=0$ if and only if $p=q$;\n",
    "```\n",
    "\n",
    "```{admonition} Proof\n",
    "*Proof.* We first note that the elementary inequality\n",
    "$\\log x \\le x - 1, \\quad\\mathrm{for\\ any\\ }x\\ge0,$ and the equality\n",
    "holds if and only if $x=1$.\n",
    "$-D_{\\rm KL}(q,p) = - \\sum_{i=1}^c q_i\\log \\frac{q_i}{p_i}   = \\sum_{i=1}^k q_i\\log \\frac{p_i}{q_i} \\le \\sum_{i=1}^k q_i( \\frac{p_i}{q_i}  - 1) = 0.$\n",
    "And the equality holds if and only if\n",
    "$\\frac{p_i}{q_i} = 1 \\quad \\forall i = 1:k.$ \n",
    "```\n",
    "\n",
    "Define cross-entropy for distribution $p$ and $q$ by\n",
    "$\n",
    "H(q,p) = - \\sum_{i=1}^k q_i \\log p_i,$ and the entropy for distribution\n",
    "$q$ by $\n",
    "H(q) = - \\sum_{i=1}^k q_i \\log q_i.$ Note that\n",
    "$D_{\\rm KL}(q,p)= \\sum_{i=1}^k q_i\\log \\frac{q_i}{p_i} =  \\sum_{i=1}^k q_i \\log q_i - \\sum_{i=1}^k q_i \\log p_i$\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "    H(q,p) = H(q) + D_{\\rm KL}(q,p).\n",
    "$$ (rel1)\n",
    "\n",
    "It follows from the [relation](rel1) that \n",
    "\n",
    "$$\n",
    "    \\mathop{\\arg\\min}_p D_{\\rm KL}(q,p)=\\mathop{\\arg\\min}_p H(q,p).\n",
    "$$ (rel2)\n",
    "\n",
    "The concept of cross-entropy can be used to define a loss function in\n",
    "machine learning and optimization. Let us assume $y_i$ is the true label\n",
    "for $x_i$, for example $y_i = e_{k_i}$ if $x_i \\in A_{k_i}$. Consider\n",
    "the predicted distribution \n",
    "$p(x;\\theta) = \\frac{1}{\\sum\\limits_{i=1}^k e^{w_i x+b_i}}$.\n",
    "$\\begin{pmatrix}\n",
    "e^{w_1 x+b_1}\\\\\n",
    "e^{w_2 x+b_2}\\\\\n",
    "\\vdots\\\\\n",
    "e^{w_k x+b_k}\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "p_1(x; \\theta) \\\\\n",
    "p_2(x; \\theta) \\\\\n",
    "\\vdots \\\\\n",
    "p_k(x; \\theta)\n",
    "\\end{pmatrix}$\n",
    "for any data $x \\in A$. By[.](rel2), the minimization of KL divergence is\n",
    "equivalent to the minimization of the cross-entropy, namely\n",
    "$\\mathop{\\arg\\min}_{\\theta} \\sum_{i=1}^N D_{\\rm KL}(y_i, p(x_i;\\theta)) = \\mathop{\\arg\\min}_{\\theta} \\sum_{i=1}^N H(y_i,  p(x_i;  \\theta)).$\n",
    "Recall that we have all data\n",
    "$D = \\{(x_1,y_1),(x_2,y_2),\\cdots, (x_N, y_N)\\}$. Then, it is natural to\n",
    "consider the loss function as following:\n",
    "$\\sum_{j=1}^N H(y_i,  p(x_i;  \\theta)),$ which measures the\n",
    "distance between the real label and predicted one for all data. In the\n",
    "meantime, we can check that \n",
    "$\\begin{aligned}\n",
    "\\sum_{j=1}^N H(y_j,  p(x_j;  \\theta))&=-\\sum_{j=1}^N y_j  \\cdot \\log   p(x_j;  \\theta )\\\\\n",
    "&=-\\sum_{j=1}^N  \\log p_{i_j}(x_i; \\theta) \\quad (\\text{because}~y_j = e_{i_j}~\\text{for}~x_j \\in A_{i_j})\\\\\n",
    "&=-\\sum_{i=1}^k \\sum_{x\\in A_i}  \\log p_{i}(x;  \\theta) \\\\\n",
    "&=-\\log \\prod_{i=1}^k \\prod_{x\\in A_i}   p_{i}(x;  \\theta)\\\\\n",
    "& = L(\\theta)\n",
    "\\end{aligned}$ with $L(\\theta)$ \n",
    "defined in as\n",
    "$L( \\theta) = - \\sum_{i=1}^k \\sum_{x\\in A_i} \\log p_{i}(x; \\theta).$\n",
    "\n",
    "That is to say, the logistic regression loss function defined by\n",
    "likelihood in []() is exact the loss function defined by measuring\n",
    "the distance between real label and predicted one via cross-entropy. We\n",
    "can note $\\label{key}\n",
    "\\min_{ \\theta} L_\\lambda( \\theta) \\Leftrightarrow \\min_{ \\theta} \\sum_{j=1}^N H(y_i,  p(x_i;  \\theta)) + \\lambda R(\\| \\theta\\|) \n",
    "\\Leftrightarrow \\min_{ \\theta} \\sum_{j=1}^N D_{\\rm KL}(y_i, p(x_i;  \\theta)) + \\lambda R(\\| \\theta\\|).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0162c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}