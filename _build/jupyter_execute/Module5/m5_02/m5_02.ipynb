{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "    width=\"800\"\n",
       "    height=\"500\"\n",
       "    src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_c20ifjjk&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_t1lki6t4\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff14ce5afd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_c20ifjjk&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_t1lki6t4\"  ,width='800', height='500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the lecture notes here: [Notes](https://sites.psu.edu/math452/files/2022/03/DetailedE01-02.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall the original DNN model\n",
    "\n",
    "Consider the classical (fully connected) artificial deep neural network\n",
    "(DNN) $f^{L}$,\n",
    "\n",
    "$$\n",
    "    \\begin{cases}f^{1} & =\\theta^{1}(x):=W^{1} x+b^{1}, \\\\ f^{\\ell} & =\\theta^{\\ell} \\circ \\sigma\\left(f^{\\ell-1}\\right):=W^{\\ell} \\sigma\\left(f^{\\ell-1}\\right)+b^{\\ell}, \\ell=2, \\ldots, L .\\end{cases}\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^{n}$ is the input vector, $\\sigma$ is a\n",
    "non-linear function (activation).\n",
    "\n",
    "## “’Real’” Batch Normalization and \"’new’ model\n",
    "\n",
    "\n",
    "Definition of $B N$ operation based on the batch\n",
    "\n",
    "Following the idea in normalization, we consider that we have the all training data as \n",
    "\n",
    "$$\n",
    "    (X, Y):=\\left\\{x_{i}, y_{i}\\right\\}_{i=1}^{N} \n",
    "$$\n",
    "\n",
    "Since the normalization is applied to each activation independently, let\n",
    "us focus on a particular activation $\\left[f^{\\ell}\\right]_{k}$ and omit\n",
    "$k$ as $f^{\\ell}$ for clarity. We have $N$ values of this activation in\n",
    "the batch,\n",
    "\n",
    "$$\n",
    "    X=\\left\\{x_{1}, \\cdots, x_{N}\\right\\}\n",
    "$$\n",
    "\n",
    "Let the normalized values be $\\hat{f}^{\\ell}$, and their linear transformations be\n",
    "$\\tilde{f}^{\\ell} .$ \n",
    "\n",
    "$$\n",
    "    \\begin{gathered}\n",
    "\\mu_{X}^{\\ell} \\leftarrow \\mathbb{E}_{x \\sim X}\\left[f^{\\ell}(x)\\right]=\\frac{1}{N} \\sum_{i=1}^{N} f^{\\ell}\\left(x_{i}\\right) \\\\\n",
    "\\sigma_{X}^{\\ell} \\leftarrow \\mathbb{E}_{x \\sim X}\\left[\\left(f^{\\ell}(x)-\\mathbb{E}_{x \\sim X}\\left[f^{\\ell}(x)\\right]\\right)^{2}\\right]=\\frac{1}{N} \\sum_{i=1}^{N}\\left(f^{\\ell}\\left(x_{i}\\right)-\\mu_{X}\\right)^{2} \\quad \\text { batch mean } \\\\\n",
    "\\hat{f}^{\\ell}(x) \\leftarrow \\frac{f^{\\ell}(x)-\\mu_{X}^{\\ell}}{\\sqrt{\\sigma_{X}^{\\ell}+\\epsilon}} \\\\\n",
    "\\tilde{f}^{\\ell}(x) \\leftarrow \\gamma^{\\ell} \\hat{f}^{\\ell}(x)+\\beta^{\\ell}\n",
    "\\end{gathered}\n",
    "$$ (eq1_51)\n",
    "\n",
    "Here we note that all these operations in the previous equation are defined by element-wise. Then at last, we define the BN operation based on the batch set as\n",
    "\n",
    "$$\n",
    "    \\mathrm{BN}_{X}\\left(f^{\\ell}(x)\\right)=\\tilde{f}^{\\ell}(x):=\\gamma^{\\ell} \\frac{f^{\\ell}(x)-\\mu_{X}^{\\ell}}{\\sqrt{\\sigma_{X}^{\\ell}+\\epsilon}}+\\beta^{\\ell}\n",
    "$$\n",
    "\n",
    "where $\\tilde{f}^{\\ell}(x), \\mu_{X}^{\\ell}$ and $\\sigma_{X}^{\\ell}$ are\n",
    "given above.\n",
    "\n",
    "“New” model for BN\n",
    "In summary, we have the new DNN model with BN as:\n",
    "\n",
    "$$\n",
    "    \\begin{cases}\\tilde{f}^{1}\\left(x_{i}\\right) & =\\left(\\theta^{1}\\left(x_{i}\\right)\\right) \\\\ \\tilde{f}^{\\ell} & =\\theta^{\\ell} \\circ \\sigma \\circ \\mathrm{BN}_{X}\\left(\\tilde{f}^{\\ell-1}\\right), \\quad \\ell=2, \\ldots, L .\\end{cases}\n",
    "$$\n",
    "\n",
    "For a more comprehensive notation, we can use the next notation\n",
    "\n",
    "$$\n",
    "    \\sigma_{\\mathrm{BN}}:=\\sigma \\circ \\mathrm{BN}_{X}\n",
    "$$\n",
    "\n",
    "Here one thing is important that we need to mention is that because of the new scale\n",
    "$\\gamma^{\\ell}$ and shift $\\beta^{\\ell}$ added after the BN operation.\n",
    "We can remove the basis $b^{\\ell}$ in $\\theta^{\\ell}$, thus to say the\n",
    "real model we will compute should be\n",
    "\n",
    "$$\n",
    "    \\begin{cases}\\tilde{f}^{1}\\left(x_{i}\\right) & =W^{1} x_{i} \\\\ \\tilde{f}^{\\ell} & =W^{\\ell} \\sigma_{\\mathrm{BN}}\\left(\\tilde{f}^{\\ell-1}\\right), \\quad \\ell=2, \\ldots, L .\\end{cases}\n",
    "$$ (eq1_55)\n",
    "\n",
    "Combine the two definition, we note\n",
    "\n",
    "$$\n",
    "    \\tilde{\\Theta}:=\\{W, \\gamma, \\beta\\}\n",
    "$$\n",
    "\n",
    "where $W=\\left\\{W^{1}, \\cdots, W^{l}\\right\\}, \\gamma:=\\left\\{\\gamma^{2}, \\cdots, \\gamma^{L}\\right\\}$\n",
    "and $\\beta:=\\left\\{\\beta^{2}, \\cdots, \\beta^{L}\\right\\}$\n",
    "\n",
    "Finally, we have the loss function as:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(\\tilde{\\Theta})=\\mathbb{E}_{(x, y) \\sim(X, Y)} \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(\\tilde{f}^{L}\\left(x_{i} ; \\tilde{\\Theta}\\right), y_{i}\\right)\n",
    "$$ (eq1_57)\n",
    "\n",
    "A key observation in {eq}`eq1_57` and the new BN model {eq}`eq1_55` is that\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "\\mu_{X}^{\\ell} &=\\mathbb{E}_{x \\sim X}\\left[f^{\\ell}(x)\\right] \\\\\n",
    "\\sigma_{X}^{\\ell} &=\\mathbb{E}_{x \\sim X}\\left[\\left(f^{\\ell}(x)-\\mathbb{E}_{x \\sim X}\\left[f^{\\ell}(x)\\right]\\right)^{2}\\right] \\\\\n",
    "\\mathcal{L}(\\tilde{\\Theta}) &=\\mathbb{E}_{(x, y) \\sim(X, Y)}\\left[\\ell\\left(\\tilde{f}^{L}\\left(x_{i} ; \\tilde{\\Theta}\\right), y_{i}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$ (eq1_58)\n",
    "\n",
    "Here we need to mention that $x \\sim X$ means $x$ subject to the discrete distribution of all data $X$.\n",
    "\n",
    "### BN: some ’modified” SGD on new batch normalized model\n",
    "\n",
    "Following the key observation in {eq}`eq1_58`, and recall the similar case in\n",
    "SGD, we do the the sampling trick in {eq}`eq1_57` and obtain the mini-batch\n",
    "SGD: \n",
    "\n",
    "$$\n",
    "    x \\sim X \\approx x \\sim \\mathcal{B}\n",
    "$$ (eq1_59)\n",
    "\n",
    "here $\\mathcal{B}$ is a mini-batch of batch $X$ with $\\mathcal{B} \\subset X .$\n",
    "\n",
    "However, for problem in {eq}`eq1_57`, it is very difficult to find some subtle\n",
    "sampling method because of the composition of $\\mu_{X}^{\\ell}$ and\n",
    "$\\left[\\sigma_{X}^{\\ell}\\right]^{2}$. However, one simple way for\n",
    "sampling {eq}`eq1_57` can be chosen as taking {eq}`eq1_59` for all the expectation\n",
    "case in {eq}`eq1_57` and {eq}`eq1_58`.\n",
    "\n",
    "This is to say, in training process ( $t$-th step for example), once we\n",
    "choose $B_{t} \\subset X$ as the mini-batch, then the model becomes\n",
    "$$\\begin{cases}\\tilde{f}^{1}\\left(x_{i}\\right) & =W^{1} x_{i}, \\\\ \\tilde{f}^{\\ell} & =W^{\\ell} \\sigma_{\\mathrm{BN}}\\left(\\tilde{f}^{\\ell-1}\\right), \\quad \\ell=2, \\ldots, L .\\end{cases}$$\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\sigma_{\\mathrm{BN}}:=\\sigma \\circ \\mathrm{BN}_{\\mathcal{B}_{t}}\n",
    "$$\n",
    "\n",
    "or we can say that $X$ is replaced by $\\mathcal{B}_{t}$ in this case.\n",
    "\n",
    "Here $\\mathrm{BN}_{\\mathcal{B}_{t}}$ is defined by \n",
    "\n",
    "$$\n",
    "    \\begin{array}{cr}\n",
    "\\mu_{\\mathcal{B}_{t}}^{\\ell} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} f^{\\ell}\\left(x_{i}\\right) \\\\\n",
    "\\sigma_{\\mathcal{B}_{t}}^{\\ell} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(f^{\\ell}\\left(x_{i}\\right)-\\mu_{\\mathcal{B}_{t}}\\right)^{2} \\quad \\text { mini-batch mean } \\\\\n",
    "\\hat{f}^{\\ell}(x) & \\leftarrow \\frac{f^{\\ell}(x)-\\mu_{\\mathcal{B}_{t}}^{\\ell}}{\\sqrt{\\sigma_{\\mathcal{B}_{t}}^{\\ell}+\\epsilon}} \\\\\n",
    "\\mathrm{BN}_{\\mathcal{B}_{t}}\\left(\\tilde{f}^{\\ell}\\right):=\\tilde{f}^{\\ell}(x) & \\leftarrow \\gamma^{\\ell} \\hat{f}^{\\ell}(x)+\\beta^{\\ell} \\\\\n",
    "& \\text { normalize }\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here BN operation introduce some new parameters as $\\gamma$ and $\\beta$. Thus to say, for training phase, if we choose\n",
    "mini-batch as $\\mathcal{B}_{t}$ in $t$-th training step, we need to take\n",
    "gradient as\n",
    "\n",
    "$$\n",
    "    \\frac{1}{m} \\nabla_{\\tilde{\\Theta}} \\sum_{i \\in \\mathcal{B}_{t}} \\ell\\left(\\tilde{f}^{L}\\left(x_{i} ; \\tilde{\\Theta}\\right), y_{i}\\right)\n",
    "$$\n",
    "\n",
    "which needs us the to take gradient for $\\mu_{B}^{\\ell}$ or\n",
    "$\\left[\\sigma_{B}^{\\ell}\\right]^{2}$ w.r.t $w^{i}$ for $i \\leq \\ell$.\n",
    "\n",
    "Questions: To derive the new gradient formula for BN step because of the\n",
    "fact that\n",
    "\n",
    "$$\n",
    "    \\mu_{\\mathcal{B}_{t}}^{\\ell}, \\quad \\text { and } \\quad \\sigma_{\\mathcal{B}_{t}}^{\\ell}\n",
    "$$\n",
    "\n",
    "contain the output of $\\tilde{f}^{\\ell-1}$.\n",
    "\n",
    "\n",
    "### Testing phase in Batch-Normalized DNN\n",
    "\n",
    "One key problem is that, in the BN operator, we need to compute the mean\n",
    "and variance in a data set (batch or mini-batch). However, in the\n",
    "inference step, we just input one data into this DNN, how to compute the\n",
    "BN operator in this situation.\n",
    "\n",
    "Actually, the $\\gamma$ and $\\beta$ parameter is fixed after training,\n",
    "the only problem is to compute the mean $\\mu$ and variance $\\sigma^{2}$.\n",
    "All the mean $\\mu_{\\mathcal{B}_{t}}$ and variance\n",
    "$\\sigma_{\\mathcal{B}}^{2}$ during the training phase are just the\n",
    "approximation of the mean and variance of whole batch i.e. $\\mu_{X}$ and\n",
    "$\\sigma_{X}^{2}$ as shown in {eq}`eq1_58`.\n",
    "\n",
    "One natural idea might be just use the BN operator w.r.t to the whole\n",
    "training data set, thus to say just compute $\\mu_{X}$ and\n",
    "$\\sigma_{X}^{2}$ by definition in {eq}`eq1_51`.\n",
    "\n",
    "However, there are at least the next few problems:\n",
    "\n",
    "-   computation cost,\n",
    "\n",
    "-   ignoring the statistical approximation (don’t make use of the\n",
    "    $\\mu_{\\mathcal{B}_{t}}$ and $\\sigma_{\\mathcal{B}_{t}}^{2}$ in\n",
    "    training phase).\n",
    "\n",
    "Considering that we have the statistical approximation for $\\mu_{X}$ and\n",
    "$\\sigma_{X}^{2}$ during each SGD step, moving average might be a more\n",
    "straightforward way. Thus two say, we define the $\\mu^{\\ell}$ and\n",
    "$\\left[\\sigma^{\\ell}\\right]^{2}$ for the inference (test) phase as\n",
    "\n",
    "$$\n",
    "    \\mu^{\\ell}=\\frac{1}{T} \\sum_{t=1}^{T} \\mu_{\\mathcal{B}_{t}}^{\\ell}, \\quad \\sigma^{\\ell}=\\frac{1}{T} \\frac{m}{m-1} \\sum_{t=1}^{T} \\sigma_{\\mathcal{B}_{t}}^{\\ell}\n",
    "$$\n",
    "\n",
    "Here we take Bessel’s correction for unbiased variance. \n",
    "Another way to do this is to call the similar idea in momentum. At each\n",
    "time step we update the running averages for mean and variance using an\n",
    "exponential decay based on the momentum parameter: \n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "&\\mu_{\\mathcal{B}_{t}}^{\\ell}=\\alpha \\mu_{\\mathcal{B}_{t-1}}^{\\ell}+(1-\\alpha) \\mu_{\\mathcal{B}_{t}}^{\\ell} \\\\\n",
    "&\\sigma_{\\mathcal{B}_{t}}^{\\ell}=\\alpha \\sigma_{\\mathcal{B}_{t-1}}^{\\ell}+(1-\\alpha) \\sigma_{\\mathcal{B}_{t}}^{\\ell}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\alpha$ is close to 1 , we can take it as $0.9$\n",
    "generally. Then we all take bath mean and variance as\n",
    "$\\mu_{X}^{\\ell} \\approx \\mu_{\\mathcal{B}_{T}}^{\\ell}$ and\n",
    "$\\sigma_{X}^{\\ell} \\approx \\sigma_{\\mathcal{B}_{T}}^{\\ell} .$\n",
    "\n",
    "Many people argue that the variance here should also use Bessel’s\n",
    "correction.\n",
    "\n",
    "### Batch Normalization for CNN\n",
    "\n",
    "One key idea in $\\mathrm{BN}$ is to do normalization with each scalar\n",
    "features (neurons) separately along a mini-batch. Thus to say, we need\n",
    "one to identify what is neuron in CNN. This is a historical problem,\n",
    "some people think neuron in CNN should be the pixel in each channel some\n",
    "thing that each channel is just one neuron. BN choose the later one. One\n",
    "(most ?) important reason for this choice is the fact of computation\n",
    "cost. For convolutional layers, BN additionally wants the normalization\n",
    "to obey the convolutional property - so that different elements of the\n",
    "same feature map, at different locations, are normalized in the same\n",
    "way. To compute $\\mu_{\\mathcal{B}_{t}}^{\\ell}$, we take mean of the set\n",
    "of all values in a feature map across both the elements of a mini-batch\n",
    "and spatial locations - so for a mini-batch of size $m$ and feature maps\n",
    "of size $m_{\\ell} \\times n_{\\ell}$ (image geometrical size), we use the\n",
    "effective mini-batch of size $m m_{\\ell} n_{\\ell}$. We learn a pair of\n",
    "parameters $\\gamma_{k}$ and $\\beta_{k}$ per feature map (k-th channel),\n",
    "rather than per activation\n",
    "\n",
    "For simplicity, then have the following BN scheme for CNN\n",
    "\n",
    "$$\n",
    "    \\begin{array}{cc}\n",
    "{\\left[\\mu_{\\mathcal{B}_{t}}^{\\ell}\\right]_{j} \\leftarrow \\frac{1}{m \\times m_{\\ell} \\times n_{\\ell}} \\sum_{i=1}^{m} \\sum_{1 \\leq s \\leq m_{\\ell}, 1 \\leq t \\leq n_{\\ell}}\\left[f^{\\ell}\\left(x_{i}\\right)\\right]_{j ; s t}} & \\text { mean on channel } j \\\\\n",
    "{\\left[\\sigma_{\\mathcal{B}_{t}}^{\\ell}\\right]_{j} \\leftarrow \\frac{1}{m \\times m_{\\ell} \\times n_{\\ell}} \\sum_{i=1}^{m} \\sum_{1 \\leq s \\leq m_{\\ell}, 1 \\leq t \\leq n_{\\ell}}\\left(\\left[f^{\\ell}\\left(x_{i}\\right)\\right]_{j ; s t}-\\left[\\mu_{\\mathcal{B}_{t}}^{\\ell}\\right]_{j}\\right)^{2}} & \\text { variance on channel } j \\\\\n",
    "{\\left[\\hat{f}^{\\ell}(x)\\right]_{j ; s t} \\leftarrow \\frac{\\left[f^{\\ell}(x)\\right]_{j, s t}-\\left[\\mu_{\\mathcal{B}_{t}}^{\\ell}\\right]_{j}}{\\sqrt{\\left[\\sigma_{\\mathcal{B}_{t}}^{\\ell}\\right]_{j}+\\epsilon}}} & \\text { normalize }\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\left[\\mathrm{BN}_{\\mathcal{B}_{t}}\\left(\\tilde{f}^{\\ell}\\right)\\right]_{j ; s t}:=\\left[\\tilde{f}^{\\ell}(x)\\right]_{j ; s t} \\leftarrow\\left[\\gamma^{\\ell}\\right]_{j}\\left[\\hat{f}^{\\ell}(x)\\right]_{j ; s t}+\\left[\\beta^{\\ell}\\right]_{j}\n",
    "$$\n",
    "\n",
    "scale and shift on channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}