{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization and weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "    width=\"800\"\n",
       "    height=\"500\"\n",
       "    src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_01dyptyu&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_ri00enil\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff8ed946e80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_01dyptyu&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_ri00enil\"  ,width='800', height='500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the lecture notes here: [Notes](https://sites.psu.edu/math452/files/2022/03/DetailedE01-02.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization in DNNs and CNNs\n",
    "\n",
    "### Normalization for input data of DNNs\n",
    "\n",
    "Consider that we have the all training data as\n",
    "\n",
    "$$\n",
    "    (X, Y):=\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1}^{N}\n",
    "$$\n",
    "\n",
    "for $x_{i} \\in \\mathbb{R}^{d}$ and $y_{i} \\in \\mathbb{R}^{k}$.\n",
    "\n",
    "Before we input every data into a DNN model, we will apply the following\n",
    "normalization for all data $x_{i}$ for each component. Let denote\n",
    "\n",
    "$$\n",
    "    \\left[x_{i}\\right]_{j} \\longleftrightarrow \\text { the } \\mathrm{j} \\text {-th component of data } x_{i}\n",
    "$$\n",
    "\n",
    "Then we have following formula of for all $j=1,2, \\cdots, d$\n",
    "\n",
    "$$\n",
    "    \\left[\\tilde{x}_{i}\\right]_{j}=\\frac{\\left[x_{i}\\right]_{j}-\\left[\\mu_{X}\\right]_{j}}{\\sqrt{\\left[\\sigma_{X}\\right]_{j}}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\left[\\mu_{X}\\right]_{j}=\\mathbb{E}_{x \\sim X}\\left[[x]_{j}\\right]=\\frac{1}{N} \\sum_{i=1}^{N}\\left[x_{i}\\right]_{j}, \\quad\\left[\\sigma_{X}\\right]_{j}=\\mathbb{V}_{x \\sim X}\\left[[x]_{j}\\right]=\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\left[x_{i}\\right]_{j}-\\left[\\mu_{X}\\right]_{j}\\right)^{2} \n",
    "$$\n",
    "\n",
    "Here $x \\sim X$ means that $x$ is a discrete random variable on $X$ with\n",
    "probability \n",
    "\n",
    "$$\n",
    "    \\mathbb{P}\\left(x=x_{i}\\right)=\\frac{1}{N}\n",
    "$$\n",
    "\n",
    "for any $x_{i} \\in X$.\n",
    "\n",
    "For simplicity, we rewrite the element-wise definition above as the\n",
    "following compact form\n",
    "\n",
    "$$\n",
    "    \\tilde{x}_{i}=\\frac{x_{i}-\\mu_{X}}{\\sqrt{\\sigma_{X}}}\n",
    "$$ (eq1_6)\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    x_{i}, \\tilde{x}_{i}, \\mu_{X}, \\sigma_{X} \\in \\mathbb{R}^{d}\n",
    "$$\n",
    "\n",
    "defined as before and all operations in {eq}`eq1_6` are element-wise.\n",
    "\n",
    "Here we note that, by normalizing the data set, we have the next\n",
    "properties for new data $\\tilde{x} \\in \\tilde{X}$ with component\n",
    "$j=1,2, \\cdots, d$,\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}_{\\bar{X}}\\left[[\\tilde{x}]_{j}\\right]=\\frac{1}{N} \\sum_{i=1}^{N}\\left[\\tilde{x}_{i}\\right]_{j}=0\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}_{\\tilde{X}}\\left[[\\tilde{x}]_{j}\\right]=\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\left[\\tilde{x}_{i}\\right]_{j}-\\mathbb{E}_{\\tilde{X}}\\left[[\\tilde{x}]_{j}\\right]\\right)^{2}=1\n",
    "$$\n",
    "\n",
    "Finally, we will have a “new” data set\n",
    "\n",
    "$$\n",
    "    \\tilde{X}=\\left\\{\\tilde{x}_{1}, \\tilde{x}_{2}, \\cdots, \\tilde{x}_{N}\\right\\}\n",
    "$$\n",
    "\n",
    "with unchanged label set $Y$. For the next sections, without special notices, we use $X$ data set as the normalized one as default.\n",
    "\n",
    "### Data normalization for images in CNNs\n",
    "\n",
    "For images, consider we have a color image data set\n",
    "$(X, Y):=\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1}^{N}$ where\n",
    "\n",
    "$$\n",
    "    x_{i} \\in \\mathbb{R}^{3 \\times m \\times n}\n",
    "$$\n",
    "\n",
    "We further denote these the $(s, t)$ pixel value for data $x_{i}$ at channel $j$ as:\n",
    "\n",
    "$$\n",
    "    \\left[x_{i}\\right]_{j ; s t} \\longleftrightarrow(s, t) \\text { pixel value for } x_{i} \\text { at channel } j\n",
    "$$\n",
    "\n",
    "where $1 \\leq i \\leq N, 1 \\leq j \\leq 3,1 \\leq s \\leq m$, and\n",
    "$1 \\leq j \\leq n .$\n",
    "\n",
    "Then, the normalization for $x_{i}$ is defined by\n",
    "\n",
    "$$\n",
    "    \\left[\\tilde{x}_{i}\\right]_{j ; s t}=\\frac{\\left[x_{i}\\right]_{j ; s t}-\\left[\\mu_{X}\\right]_{j}}{\\sqrt{\\left[\\sigma_{X}\\right]_{j}}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\left[x_{i}\\right]_{j ; s t},\\left[\\tilde{x}_{i}\\right]_{j ; s t},\\left[\\mu_{X}\\right]_{j},\\left[\\sigma_{X}\\right]_{j} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "$$\n",
    "    \\left[\\mu_{X}\\right]_{j}=\\frac{1}{m \\times n \\times N} \\sum_{1 \\leq i \\leq N} \\sum_{1 \\leq s \\leq m, 1 \\leq t \\leq n}\\left[x_{i}\\right]_{j ; s t}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    \\left[\\sigma_{X}\\right]_{j}=\\frac{1}{N \\times m \\times n} \\sum_{1 \\leq i \\leq N} \\sum_{1 \\leq s \\leq m, 1 \\leq t \\leq n}\\left(\\left[x_{i}\\right]_{j ; s t}-\\left[\\mu_{X}\\right]_{j}\\right)^{2}\n",
    "$$\n",
    "\n",
    "In batch normalization, we confirmed with Lian by both numerical test\n",
    "and code checking that $\\mathrm{BN}$ also use the above formula to\n",
    "compute the variance in $\\mathrm{CNN}$ for each channel.\n",
    "\n",
    "Another way to compute the variance over each channel is to compute the\n",
    "standard deviation on each channel for every data, and then average them\n",
    "in the data direction.\n",
    "\n",
    "$$\n",
    "    \\sqrt{\\left[\\tilde{\\sigma}_{X}\\right]_{j}}=\\frac{1}{N} \\sum_{1 \\leq i \\leq N}\\left(\\frac{1}{m \\times n} \\sum_{1 \\leq s \\leq m, 1 \\leq t \\leq n}\\left(\\left[x_{i}\\right]_{j ; s t}-\\left[\\mu_{i}\\right]_{j}\\right)^{2}\\right)^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\left[\\mu_{i}\\right]_{j}=\\frac{1}{m \\times n} \\sum_{1 \\leq s \\leq m, 1 \\leq t \\leq n}\\left[x_{i}\\right]_{j ; s t} $$\n",
    "\n",
    "### Comparison of $\\sqrt{\\left[\\sigma_{X}\\right]_{j}}$ and $\\sqrt{\\left[\\tilde{\\sigma}_{X}\\right]_{j}}$ on CIFAR10.\n",
    "\n",
    "They share the same $\\mu_{X}$ as \n",
    "\n",
    "$$\n",
    "    \\mu_{X}=\\left(\\begin{array}{lll}\n",
    "0.49140105 & 0.48215663 & 0.44653168\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "But they had different standard deviation\n",
    "estimates: \n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "&\\sqrt{\\left[\\sigma_{X}\\right]_{j}}=(0.247032840 .243484990 .26158834) \\\\\n",
    "&\\sqrt{\\left[\\tilde{\\sigma}_{X}\\right]_{j}}=(0.202201930 .199316350 .20086373)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "##Initialization for deep neural networks\n",
    "\n",
    "\n",
    "### Xavier’s Initialization\n",
    "\n",
    "The goal of Xavier initialization is to initialize the deep neural\n",
    "network to avoid gradient vanishing or blowup when the input is white\n",
    "noise.\n",
    "\n",
    "Let us denote the DNN models as:\n",
    "\n",
    "$$\n",
    "    \\begin{cases}f^{1}(x) & =W^{1} x+b^{1} \\\\ f^{\\ell}(x) & =W^{\\ell} \\sigma\\left(f^{\\ell-1}(x)\\right)+b^{\\ell} \\quad \\ell=2: L, \\\\ f(x) & =f^{L}\\end{cases}\n",
    "$$\n",
    "\n",
    "with $x \\in \\mathbb{R}^{n_{0}}$ and\n",
    "$f^{\\ell} \\in \\mathbb{R}^{n_{\\ell}}$. More precisely, we have\n",
    "\n",
    "$$\n",
    "    W^{\\ell} \\in \\mathbb{R}^{n_{\\ell} \\times n_{\\ell-1}} \n",
    "$$ \n",
    "\n",
    "The basic assumptions that we make are:\n",
    "\n",
    "-   The initial weights $W_{i j}^{\\ell}$ are i.i.d symmetric random\n",
    "    variables with mean 0, namely the probability density function of\n",
    "    $W_{i j}^{\\ell}$ is even.\n",
    "\n",
    "-   The initial bias $b^{\\ell}=0$.\n",
    "\n",
    "Now we choose the variance of the initial weights to ensure that the\n",
    "features $f^{L}$ and gradients don’t blow up or vanish. To this end we\n",
    "have the following lemma.\n",
    "\n",
    "```{prf:lemma}\n",
    ":label: lemma51_1\n",
    "Under the previous assumptions $f_{i}^{\\ell}$ is a symmetric\n",
    "random variable with $\\mathbb{E}\\left[f^{\\ell}\\right]=0 .$ Moreover, we\n",
    "have the following identity\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}\\left[\\left(f_{i}^{\\ell}\\right)^{2}\\right]=\\sum_{k} \\mathbb{E}\\left[\\left(W_{i k}^{\\ell}\\right)^{2}\\right] \\mathbb{E}\\left[\\sigma\\left(f_{k}^{\\ell-1}\\right)^{2}\\right]\n",
    "$$\n",
    "```\n",
    "\n",
    "Now, if $\\sigma=i d$, we can prove by induction from $\\ell=1$ that\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[f_{i}^{L}\\right]=\\left(\\Pi_{\\ell=2}^{L} n_{\\ell-1} \\operatorname{Var}\\left[W_{s t}^{\\ell}\\right]\\right)\\left(\\mathbb{V}\\left[W_{s t}^{1}\\right] \\sum_{k} \\mathbb{E}\\left[\\left([x]_{k}\\right)^{2}\\right]\\right)\n",
    "$$\n",
    "\n",
    "We make this assumption that $\\sigma=i d$, which is pretty reasonably\n",
    "since most activation functions in use at the time (such as the\n",
    "hyperbolic tangent) were close to the identity near 0 .\n",
    "\n",
    "Now, if we set\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[W_{i k}^{\\ell}\\right]=\\frac{1}{n_{\\ell-1}}, \\quad \\forall \\ell \\geq 2\n",
    "$$\n",
    "\n",
    "we will obtain\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[f_{i}^{L}\\right]=\\mathbb{V}\\left[f_{j}^{L-1}\\right]=\\cdots=\\mathbb{V}\\left[f_{k}^{1}\\right]=\\mathbb{V}\\left[W_{s t}^{1}\\right] \\sum_{k} \\mathbb{E}\\left[\\left([x]_{k}\\right)^{2}\\right]\n",
    "$$\n",
    "\n",
    "Thus, in pure DNN models, it is enough to just control\n",
    "$\\sum_{k} \\mathbb{E}\\left[\\left([x]_{k}\\right)^{2}\\right] .$\n",
    "\n",
    "A similar analysis of the propagation of the gradient\n",
    "$\\left(\\frac{\\partial L(\\theta)}{\\partial f^{t}}\\right)$ suggests that\n",
    "we set \n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[W_{i k}^{\\ell}\\right]=\\frac{1}{n_{\\ell}}\n",
    "$$\n",
    "\n",
    "Thus, the Xavier’s initialization suggests to initialize\n",
    "$W_{i k}^{\\ell}$ with variance as:\n",
    "\n",
    "-   To control $\\mathbb{V}\\left[f_{i}^{\\ell}\\right]:$\n",
    "\n",
    "$$\n",
    "    \\operatorname{Var}\\left[W_{i k}^{\\ell}\\right]=\\frac{1}{n_{\\ell-1}}\n",
    "$$\n",
    "\n",
    "-   To control\n",
    "    $\\mathbb{V}\\left[\\frac{\\partial L(\\theta)}{\\partial f_{i}^{l}}\\right]:$\n",
    "\n",
    "$$\n",
    "    \\operatorname{Var}\\left[W_{i k}^{\\ell}\\right]=\\frac{1}{n_{\\ell}}\n",
    "$$\n",
    "\n",
    "-   Trade-off to control\n",
    "    $\\mathbb{V}\\left[\\frac{\\partial L(\\theta)}{\\partial W_{i k}^{l}}\\right]:$\n",
    "\n",
    "$$\n",
    "    \\operatorname{Var}\\left[W_{i k}^{\\ell}\\right]=\\frac{2}{n_{\\ell-1}+n_{\\ell}}\n",
    "$$\n",
    "\n",
    "Here we note that, this analysis works for all symmetric type\n",
    "distribution around zero, but we often just choose uniform distribution\n",
    "$\\mathcal{U}(-a, a)$ and normal distribution\n",
    "$\\mathcal{N}\\left(0, s^{2}\\right) .$ Thus, the final version of Xavier’s\n",
    "initialization takes the trade-off type as\n",
    "\n",
    "$$\n",
    "    W_{i k}^{\\ell} \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{\\ell}+n_{\\ell-1}}}, \\sqrt{\\frac{6}{n_{\\ell}+n_{\\ell-1}}}\\right)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "    W_{i k}^{\\ell} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\ell}+n_{\\ell-1}}\\right) \n",
    "$$\n",
    "\n",
    "### Kaiming’s initialization\n",
    "\n",
    "We first have the following lemma for symmetric distribution.\n",
    "\n",
    "```{prf:lemma}\n",
    ":label: lemma51_2\n",
    "If $X_{i} \\in \\mathbb{R}$ for $i=1:$ n are i.i.d with symmetric\n",
    "probability density function $p(x)$, i.e. $p(x)$ is even. Then for any\n",
    "nonzero random vector\n",
    "$Y=\\left(Y_{1}, Y_{2}, \\cdots, Y_{n}\\right) \\in \\mathbb{R}^{n}$ which is\n",
    "independent with $X_{i}$, the following random variable\n",
    "\n",
    "$$\n",
    "    Z=\\sum_{i=1}^{n} X_{i} Y_{i}\n",
    "$$\n",
    "\n",
    "is also symmetric.\n",
    "```\n",
    "\n",
    "Then state the following result for ReLU function and random variable\n",
    "with symmetric distribution around 0 .\n",
    "\n",
    "```{prf:lemma}\n",
    ":label: lemma51_3\n",
    "If $X$ is a random variable on $\\mathbb{R}$ with symmetric\n",
    "probability density $p(x)$ around zero, i.e., \n",
    "$$\n",
    "    p(x)=p(-x)\n",
    "$$\n",
    "```\n",
    "\n",
    "Then we have $\\mathbb{E} X=0$ and\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}\\left[[\\operatorname{ReLU}(X)]^{2}\\right]=\\frac{1}{2} \\operatorname{Var}[X]\n",
    "$$\n",
    "\n",
    "Based on the previous {prf:ref}`lemma51_1`, we know that $f_{k}^{\\ell-1}$ is a\n",
    "symmetric distribution around 0 . The most important observation in\n",
    "Kaiming’s paper is that:\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[f_{i}^{\\ell}\\right]=n_{\\ell-1} \\mathbb{V}\\left[W_{i j}^{\\ell}\\right] \\mathbb{E}\\left[\\left[\\sigma\\left(f_{j}^{\\ell-1}\\right)\\right]^{2}\\right]=n_{\\ell-1} \\mathbb{V}\\left[W_{i k}^{\\ell}\\right] \\frac{1}{2} \\mathbb{V}\\left[f_{k}^{\\ell-1}\\right]\n",
    "$$\n",
    "\n",
    "if $\\sigma=$ ReLU. Thus, Kaiming’s initialization suggests to take:\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[W_{i k}^{\\ell}\\right]=\\frac{2}{n_{\\ell-1}}, \\quad \\forall \\ell \\geq 2\n",
    "$$\n",
    "\n",
    "For the first layer $\\ell=1$, by definition \n",
    "\n",
    "$$\n",
    "    f^{1}=W^{1} x+b^{1}\n",
    "$$\n",
    "\n",
    "there is no ReLU, thus it should be\n",
    "$\\mathbb{V}\\left[W_{i k}^{1}\\right]=\\frac{1}{d} .$ For simplicity, they\n",
    "still use $\\mathbb{V}\\left[W_{i k}^{1}\\right]=$ $\\frac{2}{d}$ in the\n",
    "paper. Similarly, an analysis of the propagation of the gradient\n",
    "suggests that we set\n",
    "$\\mathbb{V}\\left[W_{i k}^{\\ell}\\right]=\\frac{2}{n_{\\ell}}$. However, in\n",
    "paper authors did not suggest to take the trade-off version, they\n",
    "just chose\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[W_{i k}^{\\ell}\\right]=\\frac{2}{n_{\\ell-1}}\n",
    "$$\n",
    "\n",
    "as default.\n",
    "\n",
    "Thus, the final version of Kaiming’s initialization takes the forward\n",
    "type as\n",
    "\n",
    "$$\n",
    "    W_{i k}^{\\ell} \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{\\ell-1}}}, \\sqrt{\\frac{6}{n_{\\ell-1}}}\\right)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "    W_{i k}^{\\ell} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\ell-1}}\\right)\n",
    "$$\n",
    "\n",
    "### Initialization in CNN models and experiments\n",
    "\n",
    "For CNN models, following the analysis above we have the next iterative\n",
    "scheme in CNNs\n",
    "\n",
    "$$\n",
    "    f^{\\ell, i}=K^{\\ell, i} * \\sigma\\left(f^{\\ell, i-1}\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "$f^{\\ell, i-1} \\in \\mathbb{R}^{c_{\\ell} \\times n_{\\ell} \\times m_{\\ell}}, f^{\\ell, i} \\in \\mathbb{R}^{h_{\\ell} \\times n_{\\ell} \\times m_{\\ell}}$\n",
    "and\n",
    "$K \\in \\mathbb{R}^{(2 k+1) \\times(2 k+1) \\times h_{\\ell} \\times c_{\\ell}}$.\n",
    "Thus we have\n",
    "\n",
    "$$\n",
    "    \\left[f^{\\ell, i}\\right]_{h ; p, q}=\\sum_{c=1}^{c_{l}} \\sum_{s, t=-k}^{k} K_{h, c ; s, t}^{\\ell, i} * \\sigma\\left(\\left[f^{\\ell, i-1}\\right]_{c ; p+s, q+t}\\right)\n",
    "$$\n",
    "\n",
    "Take variance on both sides, we will get\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[\\left[f^{\\ell, i}\\right]_{h ; p, q}\\right]=c_{\\ell}(2 k+1)^{2} \\mathbb{V}\\left[K_{h, o ; s, t}^{\\ell, i}\\right] \\mathbb{E}\\left[\\left(\\left[f^{\\ell, i-1}\\right]_{o ; p+s, q+t}\\right)^{2}\\right]\n",
    "$$\n",
    "\n",
    "thus we have the following initialization strategies: Xavier’s\n",
    "initialization\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[K_{h, o ; s, t}^{\\ell, i}\\right]=\\frac{2}{\\left(c_{\\ell}+h_{\\ell}\\right)(2 k+1)^{2}}\n",
    "$$\n",
    "\n",
    "Kaiming’s initialization\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[K_{h, o ; s, t}^{\\ell, i}\\right]=\\frac{2}{c_{\\ell}(2 k+1)^{2}}\n",
    "$$\n",
    "\n",
    "Here we can take this Kaiming’s initialization as:\n",
    "\n",
    "-   Double the Xavier’s choice, and get\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[K_{h, o ; s, t}^{\\ell, i}\\right]=\\frac{4}{\\left(c_{\\ell}+h_{\\ell}\\right)(2 k+1)^{2}} \n",
    "$$\n",
    "\n",
    "-   Then pick $c_{\\ell}$ or $h_{\\ell}$ for final result\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}\\left[K_{h, o ; s, t}^{\\ell, i}\\right]=\\frac{4}{\\left(c_{\\ell}+h_{\\ell}\\right)(2 k+1)^{2}}=\\frac{2}{c_{\\ell}(2 k+1)^{2}} \n",
    "$$\n",
    "\n",
    "And they have the both uniform and normal distribution type.\n",
    "\n",
    "\n",
    "```{figure} ./images/img1.png\n",
    ":height: 150px\n",
    ":name: fig51_1\n",
    "The convergence of a 22-layer large model.\n",
    "```\n",
    "\n",
    "The $x$-axis is the\n",
    "number of training epochs. The y-axis is the top-1 error of 3,000 random\n",
    "val samples, evaluated on the center crop. Use ReLU as the activation\n",
    "for both cases. Both Kaiming’s initialization (red) and “Xavier’s”\n",
    "(blue) lead to convergence, but Kaiming’s initialization starts\n",
    "reducing error earlier.\n",
    "\n",
    "```{figure} ./images/img2.png\n",
    ":height: 150px\n",
    ":name: fig51_2\n",
    "The convergence of a 30-layer small model \n",
    "```\n",
    "\n",
    "Use ReLU as the activation for both cases. Kaiming’s initialization\n",
    "(red) is able to make it converge. But “Xavier’s” (blue)\n",
    "completely stalls - It is also verified that that its gradients are all\n",
    "diminishing. It does not converge even given more epochs. Given a\n",
    "22-layer model, in cifar10 the convergence with Kaiming’s initialization\n",
    "is faster than Xavier’s, but both of them are able to converge and the\n",
    "validation accuracies with two different initialization are about the\n",
    "same(error is $33.82,33.90)$.\n",
    "\n",
    "With extremely deep model with up to 30 layers, Kaiming’s initialization\n",
    "is able to make the model convergence. On the contrary, Xavier’s method\n",
    "completely stalls the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}