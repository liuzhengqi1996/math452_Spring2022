{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Convolutional Neural Networks (CNNs) with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture includes:\n",
    "\n",
    "1. Build CNNs\n",
    "2. Train MNIST with CNNs\n",
    "3. Train CIFAR10 with CNNs\n",
    "4. Improve the test accuracy\n",
    "    * Normalize the data\n",
    "    * Weight decay\n",
    "    * learning rate schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xz/fhb2zgq551315f5zf88k__ph0000gn/T/ipykernel_27449/2524240931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#stride default value: 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#padding default vaule: 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "#stride default value: 1\n",
    "#padding default vaule: 0\n",
    "conv1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([2, 1, 3, 3])\n",
      "torch.Size([2, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3)\n",
    "        self.conv2 = nn.Conv2d(1, 2, 3)\n",
    "        self.conv3 = nn.Conv2d(3, 2, 3)\n",
    "\n",
    "\n",
    "my_model=model()\n",
    "print(my_model.conv1.weight.size()) # (out_channels, in_channels, kernel_size, kernel_size)\n",
    "print(my_model.conv2.weight.size()) # (out_channels, in_channels, kernel_size, kernel_size)\n",
    "print(my_model.conv3.weight.size()) # (out_channels, in_channels, kernel_size, kernel_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 4, 4) # batch_size=1, channel =1, image size =  4 * 4 \n",
    "\n",
    "print(x)\n",
    "\n",
    "print(my_model(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "out = F.max_pool2d(input, kernel_size)\n",
    "\n",
    "out = F.avg_pool2d(input, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 3., 2., 1.],\n",
      "         [1., 3., 2., 1.],\n",
      "         [2., 1., 1., 1.],\n",
      "         [3., 5., 1., 1.]]], dtype=torch.float64)\n",
      "tensor([[[3., 2.],\n",
      "         [5., 1.]]], dtype=torch.float64)\n",
      "tensor([[[2.0000, 1.5000],\n",
      "         [2.7500, 1.0000]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1,3,2,1],[1,3,2,1],[2,1,1,1],[3,5,1,1]]],dtype=float)\n",
    "print(x)\n",
    "\n",
    "max_x = F.max_pool2d(x,2)\n",
    "print(max_x)\n",
    "\n",
    "avg_x = F.avg_pool2d(x,2)\n",
    "print(avg_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train MNIST with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU? False\n",
      "Epoch: 1, the training accuracy: 0.8597166666666667, the test accuracy: 0.8699\n",
      "Epoch: 2, the training accuracy: 0.9314, the test accuracy: 0.9323\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use GPU?', use_cuda)\n",
    "\n",
    "# Define a LeNet-5\n",
    "# Note that we need to reshape MNIST imgaes 28*28 to 32*32\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        # out.size() = [batch_size, channels, size, size], -1 here means channels*size*size\n",
    "        # out.view(out.size(0), -1) is similar to out.reshape(out.size(0), -1), but more efficient\n",
    "        # Think about why we need to reshape the out?\n",
    "        out = out.view(out.size(0), -1) \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "minibatch_size = 128\n",
    "num_epochs = 2\n",
    "lr = 0.1\n",
    "\n",
    "# Step 1: Define a model\n",
    "my_model =model()\n",
    "if use_cuda:\n",
    "    my_model = my_model.cuda()\n",
    "\n",
    "# Step 2: Define a loss function and training algorithm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Step 3: load dataset\n",
    "\n",
    "MNIST_transform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\n",
    "                                                  torchvision.transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train= True, download=True, transform=MNIST_transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train= False, download=True, transform=MNIST_transform)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset)) \n",
    "\n",
    "\n",
    "\n",
    "#Step 4: Train the NNs\n",
    "# One epoch is when an entire dataset is passed through the neural network only once.\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # Forward pass to get the loss\n",
    "        outputs = my_model(images) \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  #backpropragation\n",
    "        optimizer.step() #update the weights/parameters\n",
    "        \n",
    "    # Training accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()  \n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    training_accuracy = float(correct)/total\n",
    "\n",
    "    \n",
    "    # Test accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    test_accuracy = float(correct)/total\n",
    "        \n",
    "    print('Epoch: {}, the training accuracy: {}, the test accuracy: {}' .format(epoch+1,training_accuracy,test_accuracy))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train CIFAR10 with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU? False\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 1, the training accuracy: 0.1918, the test accuracy: 0.1957\n",
      "Epoch: 2, the training accuracy: 0.3463, the test accuracy: 0.3463\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use GPU?', use_cuda)\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # change the input channels from 1 to 3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        # out.size() = [batch_size, channels, size, size], -1 here means channels*size*size\n",
    "        # out.view(out.size(0), -1) is similar to out.reshape(out.size(0), -1), but more efficient\n",
    "        # Think about why we need to reshape the out?\n",
    "        out = out.view(out.size(0), -1) \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "minibatch_size = 128\n",
    "num_epochs = 2\n",
    "lr = 0.1\n",
    "\n",
    "# Step 1: Define a model\n",
    "my_model =model()\n",
    "if use_cuda:\n",
    "    my_model = my_model.cuda()\n",
    "\n",
    "# Step 2: Define a loss function and training algorithm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Step 3: load dataset\n",
    "\n",
    "CIFAR10_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=CIFAR10_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=CIFAR10_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "#Step 4: Train the NNs\n",
    "# One epoch is when an entire dataset is passed through the neural network only once.\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "        # Forward pass to get the loss\n",
    "        outputs = my_model(images) \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  #backpropragation\n",
    "        optimizer.step() #update the weights/parameters\n",
    "        \n",
    "    # Training accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()  \n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    training_accuracy = float(correct)/total\n",
    "\n",
    "    \n",
    "    # Test accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "        if use_cuda:\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    test_accuracy = float(correct)/total\n",
    "        \n",
    "    print('Epoch: {}, the training accuracy: {}, the test accuracy: {}' .format(epoch+1,training_accuracy,test_accuracy))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improve the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data with the mean and standard deviation of the dataset\n",
    "\n",
    "\n",
    "$$ \\tilde{x}[i,j,:,:] = \\frac{x[i,j,:,:]-mean[j]}{std[j]},~~~~i=1,2,...,60000,~~~~j=1,2,3$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CIFAR10_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=CIFAR10_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=CIFAR10_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function with $\\ell_2$ regularization:\n",
    "$$L(\\theta) :=\\frac{1}{N} \\sum_{j=1}^N\\ell(y_j, h(x_j; \\theta)) +  + \\lambda (\\|\\theta\\|_2^2).$$\n",
    "\n",
    "The parameter $\\lambda$ is called \"weight_decay\" in Pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(my_model.parameters(), lr=lr, weight_decay = 0.0001)\n",
    "# weight_decay is usually small. Two suggested values: 0.0001, 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr):\n",
    "    #lr = 1.0 / (epoch + 1)\n",
    "    lr = init_lr * 0.1 ** (epoch // 10)  # epoch // 10, calculate the quotient \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Learning rate: 1.0\n",
      "Epoch: 2, Learning rate: 1.0\n",
      "Epoch: 3, Learning rate: 1.0\n",
      "Epoch: 4, Learning rate: 1.0\n",
      "Epoch: 5, Learning rate: 1.0\n",
      "Epoch: 6, Learning rate: 1.0\n",
      "Epoch: 7, Learning rate: 1.0\n",
      "Epoch: 8, Learning rate: 1.0\n",
      "Epoch: 9, Learning rate: 1.0\n",
      "Epoch: 10, Learning rate: 1.0\n",
      "Epoch: 11, Learning rate: 0.1\n",
      "Epoch: 12, Learning rate: 0.1\n",
      "Epoch: 13, Learning rate: 0.1\n",
      "Epoch: 14, Learning rate: 0.1\n",
      "Epoch: 15, Learning rate: 0.1\n",
      "Epoch: 16, Learning rate: 0.1\n",
      "Epoch: 17, Learning rate: 0.1\n",
      "Epoch: 18, Learning rate: 0.1\n",
      "Epoch: 19, Learning rate: 0.1\n",
      "Epoch: 20, Learning rate: 0.1\n",
      "Epoch: 21, Learning rate: 0.010000000000000002\n",
      "Epoch: 22, Learning rate: 0.010000000000000002\n",
      "Epoch: 23, Learning rate: 0.010000000000000002\n",
      "Epoch: 24, Learning rate: 0.010000000000000002\n",
      "Epoch: 25, Learning rate: 0.010000000000000002\n",
      "Epoch: 26, Learning rate: 0.010000000000000002\n",
      "Epoch: 27, Learning rate: 0.010000000000000002\n",
      "Epoch: 28, Learning rate: 0.010000000000000002\n",
      "Epoch: 29, Learning rate: 0.010000000000000002\n",
      "Epoch: 30, Learning rate: 0.010000000000000002\n"
     ]
    }
   ],
   "source": [
    "init_lr = 1\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=init_lr, weight_decay = 0.0001)\n",
    "num_epochs = 30\n",
    "init_lr = 1\n",
    "for epoch in range(num_epochs):\n",
    "    current_lr = adjust_learning_rate(optimizer, epoch, init_lr)\n",
    "    print('Epoch: {}, Learning rate: {}'.format(epoch+1,current_lr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading material\n",
    "\n",
    "1. LeNet-5: https://engmrk.com/lenet-5-a-classic-cnn-architecture/\n",
    "2. torch.nn.Conv2d: https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d\n",
    "3. Understand Convolutions:\n",
    "https://medium.com/apache-mxnet/convolutions-explained-with-ms-excel-465d6649831c#f17e\n",
    "https://medium.com/apache-mxnet/multi-channel-convolutions-explained-with-ms-excel-9bbf8eb77108\n",
    "https://gfycat.com/plasticmenacingdegu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional material) How to compute the mean and standard deviation of CIFAR10 dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "mean: [0.49140054 0.48215687 0.44652957]\n",
      "std1: [0.20230146 0.19941428 0.20096211]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "CIFAR10_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=CIFAR10_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=True)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for i, (images, labels) in enumerate(trainloader):\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean = images.mean(2).sum(0)\n",
    "    std = images.std(2).sum(0)\n",
    "\n",
    "mean /= len(trainloader.dataset)\n",
    "std /= len(trainloader.dataset)\n",
    "print('mean:', mean.numpy())\n",
    "print('std1:', std.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}