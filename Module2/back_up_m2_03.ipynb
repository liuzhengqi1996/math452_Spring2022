{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf3353c",
   "metadata": {},
   "source": [
    "# Convex functions and convergence of gradient descen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1cf13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_ts79w2r0&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_0pfj6aet\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe89629b5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_ts79w2r0&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_0pfj6aet\" ,width='800', height='500')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27dc867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_yzpt1q76&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_ct99p37d\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe89629bb50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_yzpt1q76&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_ct99p37d\" ,width='800', height='500')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d2131",
   "metadata": {},
   "source": [
    "## Download the lecture notes here: [Notes](https://sites.psu.edu/math452/files/2022/01/B03ConvexFunctionGD_Video_Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convex functions and convergence of gradient descent\n",
    "\n",
    "### Convex function\n",
    "\n",
    "Then, let us first give the definition of convex sets.\n",
    "\n",
    "```{prf:definition} Convex set\n",
    ":label: def23_1\n",
    "A set $C$ is convex, if the line segment\n",
    "between any two points in $C$ lies in $C$, i.e., if any $x, y \\in C$ and\n",
    "any $\\alpha$ with $0 \\leq \\alpha \\leq 1$, there holds\n",
    "$$\\alpha x+(1-\\alpha) y \\in C$$ \n",
    "```\n",
    "\n",
    "Here are two diagrams for this definition about convex and non-convex sets.\n",
    "```{figure} ./images/2022_03_25_9faca01b68c57c6da639g-3.jpg\n",
    ":height: 150px\n",
    ":name: fig23_1\n",
    "Two diagrams about convex and non-convex sets\n",
    "```\n",
    "\n",
    "\n",
    "Following the definition of convex set, we define convex function as\n",
    "following.\n",
    "\n",
    "```{prf:definition} Convex function\n",
    ":label: def23_2\n",
    "Let $C \\subset \\mathbb{R}^{n}$ be a\n",
    "convex set and $f: C \\rightarrow \\mathbb{R}$ :\n",
    "\n",
    "- $f$ is called convex if for any $x, y \\in C$ and $\\alpha \\in[0,1]$\n",
    "\n",
    "$$\n",
    "    f(\\alpha x+(1-\\alpha) y) \\leq \\alpha f(x)+(1-\\alpha) f(y) \n",
    "$$\n",
    "\n",
    "- $f$ is called strictly convex if for any $x \\neq y \\in C$ and\n",
    "    $\\alpha \\in(0,1)$ :\n",
    "\n",
    "$$\n",
    "    f(\\alpha x+(1-\\alpha) y)<\\alpha f(x)+(1-\\alpha) f(y) \n",
    "$$ \n",
    "\n",
    "- A function $f$ is said to be (strictly) concave if $-f$ is\n",
    "    (strictly) convex.\n",
    "```\n",
    "We also have the next diagram for convex function definition.\n",
    "```{figure} ./images/2022_03_25_9faca01b68c57c6da639g-4.jpg\n",
    ":height: 150px\n",
    ":name: fig23_2\n",
    "```\n",
    "\n",
    "```{prf:lemma}\n",
    ":label: lemma23_1\n",
    "If $f(x)$ is differentiable on $\\mathbb{R}^{n}$, then $f(x)$ is\n",
    "convex if and only if\n",
    "\n",
    "$$\n",
    "    f(x) \\geq f(y)+\\nabla f(y) \\cdot(x-y), \\forall x, y \\in \\mathbb{R}^{n} \n",
    "$$\n",
    "```\n",
    "Based on the lemma, we can first have the next new diagram for convex\n",
    "functions.\n",
    "```{figure} ./images/2022_03_25_9faca01b68c57c6da639g-4(1).jpg\n",
    ":height: 150px\n",
    ":name: fig23_3\n",
    "```\n",
    "\n",
    "\n",
    "```{prf:proof}\n",
    "Let $z=\\alpha x+(1-\\alpha) y, 0 \\leq \\alpha \\leq 1, \\forall x, y \\in \\mathbb{R}^{n}$,\n",
    "we have these next two Taylor expansion: $$\\begin{aligned}\n",
    "&f(x) \\geq f(z)+\\nabla f(z)(x-z) \\\\\n",
    "&f(y) \\geq f(z)+\\nabla f(z)(y-z)\n",
    "\\end{aligned}$$ Then we have $$\\begin{aligned}\n",
    "& \\alpha f(x)+(1-\\alpha) f(y) \\\\\n",
    "\\geq & f(z)+\\nabla f(z)[\\alpha(x-z)+(1-\\alpha)(y-z)] \\\\\n",
    "=& f(z) \\\\\n",
    "=& f(\\alpha x+(1-\\alpha) y) .\n",
    "\\end{aligned}$$ Thus we have\n",
    "$$\\alpha f(x)+(1-\\alpha) f(y) \\geq f(\\alpha x+(1-\\alpha) y)$$ This\n",
    "finishes the proof.\n",
    "\n",
    "On the other hand: if $f(x)$ is differentiable on\n",
    "$\\mathbb{R}^{n}$, then $f(x) \\geq f(y)+$\n",
    "$\\nabla f(y) \\cdot(x-y), \\forall x, y \\in \\mathbb{R}^{n}$ if $f(x)$ is\n",
    "convex.\n",
    "```\n",
    "\n",
    "```{prf:definition} $\\lambda$-strongly convex\n",
    "We say that $f(x)$ is $\\lambda$-strongly convex if\n",
    "$$f(x) \\geq f(y)+\\nabla f(y) \\cdot(x-y)+\\frac{\\lambda}{2}\\|x-y\\|^{2}, \\quad \\forall x, y \\in C,$$\n",
    "for some $\\lambda>0$.\n",
    "```\n",
    "\n",
    "Example . Consider $f(x)=\\|x\\|^{2}$, then we have\n",
    "$$\\frac{\\partial f}{\\partial x_{i}}=2 x_{i}, \\nabla f=2 x \\in R^{n}$$\n",
    "So, we have $$\\begin{aligned}\n",
    "& f(x)-f(y)-\\nabla f(y)(x-y) \\\\\n",
    "=&\\|x\\|^{2}-\\|y\\|^{2}-2 y(x-y) \\\\\n",
    "=&\\|x\\|^{2}-\\|y\\|^{2}-2 x y+2\\|y\\|^{2} \\\\\n",
    "=&\\|x\\|^{2}-2 x y+\\|y\\|^{2} \\\\\n",
    "=&\\|x-y\\|^{2} \\\\\n",
    "=& \\lambda\\|x-y\\|^{2}, \\quad \\lambda=2 .\n",
    "\\end{aligned}$$ Thus, $f(x)=\\|x\\|^{2}$ is 2-strongly convex\n",
    "\n",
    "Example. Actually, the loss function of the logistic\n",
    "regression model $$L(\\theta)=-\\log P(\\theta)$$ is convex as a function\n",
    "of $\\theta$.\n",
    "\n",
    "Furthermore, the loss function of the regularized logistic regression\n",
    "model\n",
    "$$L_{\\lambda}(\\theta)=-\\log P(\\theta)+\\lambda\\|\\theta\\|_{F}^{2}, \\lambda>0$$\n",
    "is $\\lambda^{\\prime}$-strongly convex $\\left(\\lambda^{\\prime}\\right.$ is\n",
    "related to $\\left.\\lambda\\right)$ as a function of $\\theta$.\n",
    "\n",
    "We also have these following interesting properties of convex function.\n",
    "\n",
    "Properties 1 (basic properties of convex function) \n",
    "\n",
    "- If $f(x), g(x)$ are both convex, then $\\alpha f(x)+\\beta g(x)$ is\n",
    "    also convex, if $\\alpha, \\beta \\geq 0$\n",
    "\n",
    "- Linear function is both convex and concave. Here,  $f(x)$ is concave if and only if $-f(x)$ is convex.\n",
    "\n",
    "-  If $f(x)$ is a convex convex function on $\\mathbb{R}^{n}$, then\n",
    "    $g(y)=f(A y+b)$ is a convex function on $\\mathbb{R}^{m}$. Here\n",
    "    $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$.\n",
    "\n",
    "- If $g(x)$ is a convex function on $\\mathbb{R}^{n}$, and the function\n",
    "    $f(u)$ is convex function on $\\mathbb{R}$ and non-decreasing, then\n",
    "    the composite function $f \\circ g(x)=f(g(x))$ is convex.\n",
    "\n",
    "\n",
    "\n",
    "### On the Convergence of GD\n",
    "\n",
    "For the next optimization problem \n",
    "\n",
    "$$\n",
    "    \\min _{x \\in \\mathbb{R}^{n}} f(x)\n",
    "$$\n",
    "\n",
    "We assume that $f(x)$ is convex. Then we say that $x^{*}$ is a minimizer\n",
    "if $f\\left(x^{*}\\right)=$ $\\min _{x \\in \\mathbb{R}^{n}} f(x) $\n",
    "\n",
    "Let's recall that, for minimizer $x^{*}$ we have\n",
    "\n",
    "$$\n",
    "    \\nabla f\\left(x^{*}\\right)=0\n",
    "$$\n",
    "\n",
    "Then we have the next tw properties of minimizer for convex functions:\n",
    "\n",
    "1.  If $f(x) \\geq c_{0}$, for some $c_{0} \\in \\mathbb{R}$, then we have\n",
    "\n",
    "$$\n",
    "    \\arg \\min f \\neq \\emptyset\n",
    "$$\n",
    "\n",
    "2.  If $f(x)$ is $\\lambda$-strongly convex, then $f(x)$ has a unique\n",
    "    minimizer, namely, there exists a unique $x^{*} \\in \\mathbb{R}^{n}$\n",
    "    such that\n",
    "\n",
    "$$\n",
    "    f\\left(x^{*}\\right)=\\min _{x \\in \\mathbb{R}^{n}} f(x)\n",
    "$$\n",
    "\n",
    "To investigate the convergence of gradient descent method, let recall the gradient\n",
    "descent method:\n",
    "\n",
    "```{prf:algorithm} FGD\n",
    ":label: FGD\n",
    "**For** $t = 1,2,\\cdots$ do\n",
    "\n",
    "$$\n",
    "  \\quad x_{t+1} = x_{t} - \\eta_t \\nabla f(x_t)\n",
    "$$\n",
    "\n",
    "**EndFor**\n",
    "\n",
    "where $\\eta_t$ is the stepsize/leaning rate\n",
    "\n",
    "```\n",
    "\n",
    "```{admonition} Assumption\n",
    "We make the following assumptions \n",
    "\n",
    "1. $f(x)$ is $\\lambda$-strongly convex for some $\\lambda>0$. Recall the definition, we have\n",
    "\n",
    "$$\n",
    "    f(x) \\geq f(y)+\\nabla f(y) \\cdot(x-y)+\\frac{\\lambda}{2}\\|x-y\\|^{2}\n",
    "$$\n",
    "\n",
    "then note $x^{*}=\\arg \\min f(x)$. Then we have\n",
    "\n",
    "- Take $y=x^{*}$, this leads to\n",
    "\n",
    "$$\n",
    "    f(x) \\geq f\\left(x^{*}\\right)+\\frac{\\lambda}{2}\\|x-y\\|^{2} \n",
    "$$\n",
    "\n",
    "- Take $x=x^{*}$, this leads to\n",
    "\n",
    "$$\n",
    "    0 \\geq f\\left(x^{*}\\right)-f(y) \\geq \\nabla f(y) \\cdot\\left(x^{*}-y\\right)+\\frac{\\lambda}{2}\\left\\|x^{*}-y\\right\\|^{2}\n",
    "$$\n",
    "\n",
    "which means that\n",
    "\n",
    "$$\n",
    "    \\nabla f(x) \\cdot\\left(x-x^{*}\\right) \\geq \\frac{\\lambda}{2}\\left\\|x-x^{*}\\right\\|^{2}\n",
    "$$\n",
    "\n",
    "2.  $\\nabla f$ is Lipschitz for some $L>0$, i.e.,\n",
    "\n",
    "$$\n",
    "    \\|\\nabla f(x)-\\nabla f(y)\\| \\leq L\\|x-y\\|, \\forall x, y \n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "Thus, we have the next theorem about the convergence of gradient descent method.\n",
    "\n",
    " \n",
    "```{prf:theorem}\n",
    ":label: thm23_2\n",
    "For {prf:ref}`FGD`, if $f(x)$ is $\\lambda$-strongly convex and\n",
    "$\\nabla f$ is Lipschitz for some $L>0$, then\n",
    "\n",
    "$$\n",
    "    \\left\\|x_{t}-x^{*}\\right\\|^{2} \\leq \\alpha^{t}\\left\\|x_{0}-x^{*}\\right\\|^{2}\n",
    "$$\n",
    "\n",
    "if $0<\\eta_{t} \\leq \\eta_{0}=\\frac{\\lambda}{2 L^{2}}$ and\n",
    "$\\alpha=1-\\frac{\\lambda^{2}}{4 L^{2}}<1 .$\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "If we minus any $x \\in \\mathbb{R}^{n}$, we can only get:\n",
    "$$x_{t+1}-x=x_{t}-\\eta_{t} \\nabla f\\left(x_{t}\\right)-x .$$ If we take\n",
    "$L^{2}$ norm for both side, we get:\n",
    "$$\\left\\|x_{t+1}-x\\right\\|^{2}=\\left\\|x_{t}-\\eta_{t} \\nabla f\\left(x_{t}\\right)-x\\right\\|^{2} .$$\n",
    "So we have the following inequality and take $x=x^{*}$ :\n",
    "\n",
    "$\\left\\|x_{t+1}-x^{*}\\right\\|^{2}=\\left\\|x_{t}-\\eta_{t} \\nabla f\\left(x_{t}\\right)-x^{*}\\right\\|^{2}$\n",
    "\n",
    "$=\\left\\|x_{t}-x^{*}\\right\\|^{2}-2 \\eta_{t} \\nabla f\\left(x_{t}\\right)^{\\top}\\left(x_{t}-x^{*}\\right)+\\eta_{t}^{2}\\left\\|\\nabla f\\left(x_{t}\\right)-\\nabla f\\left(x^{*}\\right)\\right\\|^{2}$\n",
    "\n",
    "$\\leq\\left\\|x_{t}-x^{*}\\right\\|^{2}-\\eta_{t} \\lambda\\left\\|x_{t}-x^{*}\\right\\|^{2}+\\eta_{t}^{2} L^{2}\\left\\|x_{t}-x^{*}\\right\\|^{2} \\quad(\\lambda-$\n",
    "strongly convex and Lipschitz $)$\n",
    "\n",
    "$\\leq\\left(1-\\eta_{t} \\lambda+\\eta_{t}^{2} L^{2}\\right)\\left\\|x_{t}-x^{*}\\right\\|$.\n",
    "\n",
    "So, if $\\eta_{t} \\leq \\frac{\\lambda}{2 L^{2}}$, then\n",
    "$\\alpha=\\left(1-\\eta_{t} \\lambda+\\eta_{t}^{2} L^{2}\\right) \\leq 1-\\frac{\\lambda^{2}}{4 L^{2}}<1$,\n",
    "which finishes the proof.\n",
    "```\n",
    "\n",
    "Some issues on GD:\n",
    "-   $\\nabla f\\left(x_{t}\\right)$ is very expensive to compete.\n",
    "\n",
    "-   GD does not yield generalization accuracy.\n",
    "\n",
    "The stochastic gradient descent (SGD) method which we will discuss in\n",
    "the next section will focus on these two issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
