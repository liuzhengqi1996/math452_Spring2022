
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.1 Line search and gradient descent method &#8212; Math 452 Site</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.2 Stochastic gradient descent method and convergence theory" href="ch4_2.html" />
    <link rel="prev" title="Chapter 4 Training Algorithms" href="ch4_.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/PSU_SCI_RGB_2C.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Math 452 Site</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to Math 452
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  contents
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch00/ch0_.html">
   Ch0 Get started: course information and preparations:
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_1.html">
     0.1 Course information, requirements and reference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_2.html">
     0.2 Course background and introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_3.html">
     0.3 Introduction to Python and Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch01/ch1_.html">
   Ch1 Machine Learning and Image Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/video.html">
     Chapter 1 video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_1_video.html">
     1.1 A basic machine learning problem: image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_2_video.html">
     1.2 Image classification problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_3_video.html">
     1.3 Some popular data sets in image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/hw2.html">
     Homework 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch02/ch2_.html">
   Ch2 Linear Machine Learning Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch02/ch2_1_video.html">
     2.1 Definition of linearly separable sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch02/ch2_2.html">
     2.2 Introduction to logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch02/ch2_3.html">
     2.3 KL divergence and cross-entropy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch02/ch2_4.html">
     2.4 Support vector machine
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch03/ch3_.html">
   Ch3 Probability
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_1.html">
     3.1 Introduction to probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_2.html">
     3.2 Basic probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_3.html">
     3.3 Basic Probability Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_4.html">
     3.4 Random, Variable, Mean, Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_5.html">
     3.5 Probability interpretation of logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_6.html">
     3.6 Maximamum Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_7.html">
     3.7 Basic Statistical Learning Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_8.html">
     3.8 Classfication/ Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_9.html">
     3.9 Bayesian Approach to Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_10.html">
     3.10 General Covariance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch4_.html">
   Chapter 4 Training Algorithms
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.1 Line search and gradient descent method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ch4_2.html">
     4.2 Stochastic gradient descent method and convergence theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch05/ch5_.html">
   Chapter 5 Polynomials and Weierstrass theorem
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch05/ch5_1.html">
     5.1 Weierstrass Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch05/ch5_2.html">
     5.2 Fourier transform and Fourier series
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch07/ch7_.html">
   Chapter 7 Deep Neural Network Functions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch07/ch7_1.html">
     7.1 Motivation: from finite element to neural network {#FE2NN}
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch07/ch7_2.html">
     7.2 Why we need deep neural networks via composition {#whydeep}
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch07/ch7_3.html">
     7.3.4 Fourier transform of polynomials
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/ch04/ch4_1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/liuzhengqi1996/math452/main?urlpath=lab/tree/ch04/ch4_1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-method">
   4.1.1 Gradient descent method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-general-approach-line-search-method">
     A general approach: line search method
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convergence-of-gradient-descent-method">
   4.1.2 Convergence of Gradient Descent method
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="line-search-and-gradient-descent-method">
<h1>4.1 Line search and gradient descent method<a class="headerlink" href="#line-search-and-gradient-descent-method" title="Permalink to this headline">¶</a></h1>
<div class="section" id="gradient-descent-method">
<h2>4.1.1 Gradient descent method<a class="headerlink" href="#gradient-descent-method" title="Permalink to this headline">¶</a></h2>
<p>For simplicity, let us just consider a general optimization problem</p>
<div class="math notranslate nohighlight" id="equation-problem">
<span class="eqno">(17)<a class="headerlink" href="#equation-problem" title="Permalink to this equation">¶</a></span>\[
    \label{optmodel}
    \min_{x\in \mathbb{R}^n } f(x).
\]</div>
<p><img alt="image" src="../_images/diag_GD.png" />{height=”5cm” width=”7cm”}</p>
<div class="section" id="a-general-approach-line-search-method">
<h3>A general approach: line search method<a class="headerlink" href="#a-general-approach-line-search-method" title="Permalink to this headline">¶</a></h3>
<p>Given any initial guess <span class="math notranslate nohighlight">\(x_1\)</span>, the line search method uses the following
algorithm</p>
<div class="math notranslate nohighlight">
\[
    \eta_t= argmin_{\eta\in \mathbb{R}^1} f(x_t - \eta p_t)\qquad \mbox{(1D minimization problem)}
\]</div>
<p>to produce <span class="math notranslate nohighlight">\(\{ x_{t}\}_{t=1}^{\infty}\)</span></p>
<div class="math notranslate nohighlight">
\[
    \label{line-search}
    x_{t+1} = x_{t} - \eta_t p_t.
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\eta_t\)</span> is called the step size in
optimization and also learning rate in machine learn
ing, <span class="math notranslate nohighlight">\(p_t\)</span> is called
the descent direction, which is the critical component of this
algorithm. And <span class="math notranslate nohighlight">\(x_t\)</span> tends to</p>
<div class="math notranslate nohighlight">
\[
    x^*= argmin_{x\in \mathbb{R}^n} f(x) \iff f(x^*)=\min_{x\in \mathbb{R}^n} f(x)
\]</div>
<p>as <span class="math notranslate nohighlight">\(t\)</span> tends to infinity. There is a series of optimization algorithms
which follow the above form just using different choices of <span class="math notranslate nohighlight">\(p_t\)</span>.</p>
<p>Then, the next natural question is what a good choice of <span class="math notranslate nohighlight">\(p_t\)</span> is? We
have the following theorem to show why gradient direction is a good
choice for <span class="math notranslate nohighlight">\(p_t\)</span>.</p>
<div class="admonition-lemma admonition">
<p class="admonition-title">lemma</p>
<p>Given <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span>, if <span class="math notranslate nohighlight">\(\nabla f(x)\neq 0\)</span>, the fast descent
direction of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span> is the negative gradient direction, namely</p>
<div class="math notranslate nohighlight">
\[
    -\frac{\nabla f(x)}{\|\nabla f(x)\|} = \mathop{\arg\min}_{ p \in \mathbb{R}^n, \|p\|=1} \left. \frac{\partial f(x + \eta p)}{\partial \eta} \right|_{\eta=0}.
\]</div>
<p>It means that <span class="math notranslate nohighlight">\(f(x)\)</span> decreases most rapidly along the negative gradient
direction.</p>
</div>
<div class="admonition-proof admonition">
<p class="admonition-title">proof</p>
<p><em>Proof.</em> Let <span class="math notranslate nohighlight">\(p\)</span> be a direction in <span class="math notranslate nohighlight">\(\mathbb{R}^{n},\|p\|=1\)</span>. Consider
the local decrease of the function <span class="math notranslate nohighlight">\(f(\cdot)\)</span> along direction <span class="math notranslate nohighlight">\(p\)</span></p>
<div class="math notranslate nohighlight">
\[
    \Delta(p)=\lim _{\eta \downarrow 0} \frac{1}{\eta}\left(f(x+\eta p)-f(x)\right)=\left. \frac{\partial f(x + \eta p)}{\partial \eta} \right|_{\eta=0}.
\]</div>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[
    \begin{split}
\left. \frac{\partial f(x + \eta p)}{\partial \eta} \right|_{\eta=0}=\sum_{i=1}^n\left. \frac{\partial f}{\partial x_i}(x + \eta p)p_i \right|_{\eta=0} =(\nabla f, p),
\end{split}
\]</div>
<p>which means that</p>
<div class="math notranslate nohighlight">
\[
    f(x+\eta p)-f(x)=\eta(\nabla f(x), p)+o(\eta) .
\]</div>
<p>Therefore</p>
<div class="math notranslate nohighlight">
\[
    \Delta(p)=(\nabla f(x), p).
\]</div>
<p>Using the Cauchy-Schwarz inequality
<span class="math notranslate nohighlight">\(-\|x\| \cdot\|y\| \leq( x, y) \leq\|x\| \cdot\|y\|,\)</span> we obtain</p>
<div class="math notranslate nohighlight">
\[
    -\|\nabla f(x)\| \le (\nabla f(x), p)\le \|\nabla f(x)\| .
\]</div>
<p>Let us take</p>
<div class="math notranslate nohighlight">
\[
    \bar{p}=-\nabla f(x) /\|\nabla f(x)\|.
\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
    \Delta(\bar{p})=-(\nabla f(x), \nabla f(x)) /\|\nabla f(x)\|=-\|\nabla f(x)\|.
\]</div>
<p>The direction <span class="math notranslate nohighlight">\(-\nabla f(x)\)</span> (the antigradient) is the direction of the
fastest local decrease of the function <span class="math notranslate nohighlight">\(f(\cdot)\)</span> at point <span class="math notranslate nohighlight">\(x.\)</span> ◻</p>
</div>
<p>Here is a simple diagram for this property.</p>
<p>Since at each point, <span class="math notranslate nohighlight">\(f(x)\)</span> decreases most rapidly along the negative
gradient direction, it is then natural to choose the search direction in
<a class="reference external" href="#line-search">[line-search]</a>{reference-type=”eqref”
reference=”line-search”} in the negative gradient direction and the
resulting algorithm is the so-called gradient descent method.</p>
<div class="proof algorithm admonition" id="my-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Algrihthm)</p>
<div class="algorithm-content section" id="proof-content">
<p>Given the initial guess <span class="math notranslate nohighlight">\(x_0\)</span>, learning rate <span class="math notranslate nohighlight">\(\eta_t&gt;0\)</span></p>
<p><strong>For</strong> t=1,2,<span class="math notranslate nohighlight">\(\cdots\)</span>,\</p>
<div class="math notranslate nohighlight">
\[
    x_{t+1} =  x_{t} - \eta_{t} \nabla f({x}_{t}),
\]</div>
</div>
</div><p>In practice, we need a “stopping criterion” that determines when the
above gradient descent method to stop. One possibility is</p>
<blockquote>
<div><p><strong>While</strong> <span class="math notranslate nohighlight">\(S(x_t; f) = \|\nabla f(x_t)\|\le \epsilon\)</span> or <span class="math notranslate nohighlight">\(t \ge T\)</span></p>
</div></blockquote>
<p>for some small tolerance <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> or maximal number of iterations
<span class="math notranslate nohighlight">\(T\)</span>. In general, a good stopping criterion is hard to come by and it is
a subject that has called a lot of research in optimization for machine
learning.</p>
<p>In the gradient method, the scalar factors for the gradients,
<span class="math notranslate nohighlight">\(\eta_{t},\)</span> are called the step sizes. Of course, they must be positive.
There are many variants of the gradient method, which differ one from
another by the step-size strategy. Let us consider the most important
examples.</p>
<ol>
<li><p>The sequence <span class="math notranslate nohighlight">\(\left\{\eta_t\right\}_{t=0}^{\infty}\)</span> is chosen in
advance. For example, (constant step)</p>
<div class="math notranslate nohighlight">
\[
        \eta_t=\frac{\eta}{\sqrt{t+1}};
    \]</div>
</li>
<li><p>Full relaxation:</p>
<div class="math notranslate nohighlight">
\[
        \eta_t=\arg \min _{\eta \geq 0} f\left(x_t-\eta \nabla f\left(x_t\right)\right);
    \]</div>
</li>
<li><p>The Armijo rule: Find <span class="math notranslate nohighlight">\(x_{t+1}=x_t-\eta \nabla f\left(x_t\right)\)</span>
with <span class="math notranslate nohighlight">\(\eta&gt;0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
        \alpha\left(\nabla f\left(x_t\right), x_t-x_{t+1}\right) \leq f\left(x_t\right)-f\left(x_{t+1}\right),
    \]</div>
<div class="math notranslate nohighlight">
\[
        \beta\left(\nabla f\left(x_t\right), x_t-x_{t+1}\right) \geq f\left(x_t\right)-f\left(x_{t+1}\right),
    \]</div>
<p>where <span class="math notranslate nohighlight">\(0&lt;\alpha&lt;\beta&lt;1\)</span> are some fixed parameters.</p>
</li>
</ol>
<p>Comparing these strategies, we see that</p>
<ol>
<li><p>The first strategy is the simplest one. It is often used in the
context of convex optimization. In this framework, the behavior of
functions is much more predictable than in the general nonlinear
case.</p></li>
<li><p>The second strategy is completely theoretical. It is never used in
practice since even in one-dimensional case we cannot find the exact
minimum in finite time.</p></li>
<li><p>The third strategy is used in the majority of practical algorithms.
It has the following geometric interpretation. Let us fix
<span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n}\)</span> assuming that <span class="math notranslate nohighlight">\(\nabla f(x) \neq 0\)</span>. Consider
the following function of one variable:</p>
<div class="math notranslate nohighlight">
\[
        \phi (\eta)=f(x-\eta \nabla f(x)),\quad \eta\ge0.
    \]</div>
<p>Then the
step-size values acceptable for this strategy belong to the part of
the graph of <span class="math notranslate nohighlight">\(\phi\)</span> which is located between two linear functions:</p>
<div class="math notranslate nohighlight">
\[
        \phi_{1}(\eta)=f(x)-\alpha \eta\|\nabla f(x)\|^{2}, \quad \phi_{2}(\eta)=f(x)-\beta \eta\|\nabla f(x)\|^{2}
    \]</div>
<p>Note that <span class="math notranslate nohighlight">\(\phi(0)=\phi_{1}(0)=\phi_{2}(0)\)</span> and
<span class="math notranslate nohighlight">\(\phi^{\prime}(0)&lt;\phi_{2}^{\prime}(0)&lt;\phi_{1}^{\prime}(0)&lt;0 .\)</span>
Therefore, the acceptable values exist unless <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span> is not
bounded below. There are several very fast one-dimensional
procedures for finding a point satisfying the Armijo conditions.
However, their detailed description is not important for us now.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="convergence-of-gradient-descent-method">
<h2>4.1.2 Convergence of Gradient Descent method<a class="headerlink" href="#convergence-of-gradient-descent-method" title="Permalink to this headline">¶</a></h2>
<p>Now we are ready to study the rate of convergence of unconstrained
minimization schemes. For the optimization problem <a class="reference internal" href="#equation-problem">(17)</a></p>
<div class="math notranslate nohighlight">
\[
    \min_{x\in \mathbb{R}^n} f(x).
\]</div>
<p>We assume that <span class="math notranslate nohighlight">\(f(x)\)</span> is convex. Then we say that <span class="math notranslate nohighlight">\(x^*\)</span> is a minimizer if</p>
<div class="math notranslate nohighlight">
\[
    f(x^*) = \min_{x \in \mathbb{R}^n} f(x).
\]</div>
<p>For minimizer <span class="math notranslate nohighlight">\(x^*\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    \label{key}
    \nabla f(x^*) = 0.
\]</div>
<p>We have the next two properties of the minimizer
for convex functions:</p>
<ol>
<li><p>If <span class="math notranslate nohighlight">\(f(x) \ge c_0\)</span>, for some <span class="math notranslate nohighlight">\(c_0 \in \mathbb{R}\)</span>, then we have</p>
<div class="math notranslate nohighlight">
\[
        \mathop{\arg\min} f \neq \emptyset.
    \]</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(f(x)\)</span> is <span class="math notranslate nohighlight">\(\lambda\)</span>-strongly convex, then <span class="math notranslate nohighlight">\(f(x)\)</span> has a unique
minimizer, namely, there exists a unique <span class="math notranslate nohighlight">\(x^*\in \mathbb{R}^n\)</span> such
that</p>
<div class="math notranslate nohighlight">
\[
        f(x^*) = \min_{x\in \mathbb{R}^n }f(x).
    \]</div>
</li>
</ol>
<p>To investigate the convergence of gradient descent method, let us recall
the gradient descent method:</p>
<div class="proof algorithm admonition" id="my-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Algorithm)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>For</strong>: <span class="math notranslate nohighlight">\(t = 1, 2, \cdots\)</span></p>
<div class="math notranslate nohighlight">
\[
    \label{equ:fgd-iteration}
    x_{t+1} =  x_{t} - \eta_t \nabla f(x_t),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta_t\)</span> is the stepsize / learning rate.</p>
</div>
</div><p>We have the next theorem about the convergence of gradient descent
method under the Assumption.</p>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>For Gradient Descent Algorithm <a class="reference internal" href="#my-algorithm">Algorithm 2</a> , if
<span class="math notranslate nohighlight">\(f(x)\)</span> satisfies Assumption, then</p>
<div class="math notranslate nohighlight">
\[
    \|x_t - x^*\|^2 \le  \alpha^t \|x_0 - x^*\|^2
\]</div>
<p>if <span class="math notranslate nohighlight">\(0&lt;\eta_t &lt;\frac{2\lambda}{L^2}\)</span> and <span class="math notranslate nohighlight">\(\alpha &lt; 1\)</span>.</p>
<p>Particularly, if <span class="math notranslate nohighlight">\(\eta_t = \frac{\lambda}{L^2}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
    \|x_t - x^*\|^2 \le  \left(1 - \frac{\lambda^2}{L^2}\right)^t \|x_0 - x^*\|^2.
\]</div>
</div>
<div class="admonition-proof admonition">
<p class="admonition-title">Proof</p>
<p><em>Proof.</em> Note that</p>
<div class="math notranslate nohighlight">
\[
    x_{t+1} - x =  x_{t} - \eta_t \nabla f(x_t)  - x.
\]</div>
<p>By taking <span class="math notranslate nohighlight">\(L^2\)</span> norm for both sides, we get</p>
<div class="math notranslate nohighlight">
\[
    \|x_{t+1} - x \|^2 = \|x_{t} - \eta_t \nabla f(x_t) - x \|^2.
\]</div>
<p>Let
<span class="math notranslate nohighlight">\(x = x^*\)</span>. It holds that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \|x_{t+1} - x^* \|^2 &amp;=  \| x_{t} - \eta_t \nabla f(x_t) - x^* \|^2 \\
    &amp;= \|x_t-x^*\|^2 - 2\eta_t \nabla f(x_t)^\top (x_t - x^*) + \eta_t^2 \|\nabla f(x_t) - \nabla f(x^*)\|^2 \qquad \mbox{ (by $\nabla f(x^*)=0$)}\\
    &amp;\le \|x_t - x^*\|^2 - 2\eta_t \lambda \|x_t - x^*\|^2 + \eta_t ^2 L^2 \|x_t - x^*\|^2  \quad
    \mbox{(by $\lambda$- strongly convex \eqref{strongConvIneq} and Lipschitz)}\\
    &amp;\le (1 - 2\eta_t \lambda + \eta_t^2 L^2) \|x_t - x^*\|^2
    =\alpha \|x_t - x^*\|^2,
    \end{aligned}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
    \alpha = \left(L^2 (\eta_t  -{\lambda\over L^2})^2 + 1-{\lambda^2\over L^2}\right)&lt;1\  \mbox{if } 0&lt; \eta_t&lt;\frac{2\lambda}{L^2}.
\]</div>
<p>Particularly, if <span class="math notranslate nohighlight">\(\eta_t =\frac{\lambda}{L^2}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
    \alpha=1-{\lambda^2\over L^2},
\]</div>
<p>which finishes the proof. ◻</p>
</div>
<p>This means that if the learning rate is chosen appropriatly,
<span class="math notranslate nohighlight">\(\{x_t\}_{t=1}^\infty\)</span> from the gradient descent method will converge to
the minimizer <span class="math notranslate nohighlight">\(x^*\)</span> of the function.</p>
<p>There are some issues on Gradient Descent method:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\nabla f(x_{t})\)</span> is very expensive to compute.</p></li>
<li><p>Gradient Descent method does not yield generalization accuracy.</p></li>
</ul>
<p>The stochastic gradient descent (SGD) method in the next section will
focus on these two issues.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch04"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="ch4_.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Chapter 4 Training Algorithms</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="ch4_2.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">4.2 Stochastic gradient descent method and convergence theory</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Department of Mathematics, Penn State University Park<br/>
        
            &copy; Copyright The Pennsylvania State University, 2021. This material is not licensed for resale.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-206078372-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>