
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2.4 Support vector machine &#8212; Math 452 Site</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Ch3 Probability" href="../ch03/ch3_.html" />
    <link rel="prev" title="2.3 KL divergence and cross-entropy" href="ch2_3.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/PSU_SCI_RGB_2C.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Math 452 Site</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to Math 452
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  contents
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch00/ch0_.html">
   Ch0 Get started: course information and preparations:
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_1.html">
     0.1 Course information, requirements and reference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_2.html">
     0.2 Course background and introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_3.html">
     0.3 Introduction to Python and Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch01/ch1_.html">
   Ch1 Machine Learning and Image Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/video.html">
     Chapter 1 video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_1_video.html">
     1.1 A basic machine learning problem: image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_2_video.html">
     1.2 Image classification problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_3_video.html">
     1.3 Some popular data sets in image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/hw2.html">
     Homework 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch2_.html">
   Ch2 Linear Machine Learning Models
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="ch2_1_video.html">
     2.1 Definition of linearly separable sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ch2_2.html">
     2.2 Introduction to logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ch2_3.html">
     2.3 KL divergence and cross-entropy
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     2.4 Support vector machine
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch03/ch3_.html">
   Ch3 Probability
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_1.html">
     3.1 Introduction to probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_2.html">
     3.2 Basic probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_3.html">
     3.3 Basic Probability Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_4.html">
     3.4 Random, Variable, Mean, Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_5.html">
     3.5 Probability interpretation of logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_6.html">
     3.6 Maximamum Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_7.html">
     3.7 Basic Statistical Learning Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_8.html">
     3.8 Classfication/ Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_9.html">
     3.9 Bayesian Approach to Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_10.html">
     3.10 General Covariance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch04/ch4_.html">
   Chapter 4 Training Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch04/ch4_1.html">
     4.1 Line search and gradient descent method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch04/ch4_2.html">
     4.2 Stochastic gradient descent method and convergence theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch05/ch5_.html">
   Chapter 5 Polynomials and Weierstrass theorem
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch05/ch5_1.html">
     5.1 Weierstrass Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch05/ch5_2.html">
     5.2 Fourier transform and Fourier series
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch07/ch7_.html">
   Chapter 7 Deep Neural Network Functions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch07/ch7_1.html">
     7.1 Motivation: from finite element to neural network {#FE2NN}
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch07/ch7_2.html">
     7.2 Why we need deep neural networks via composition {#whydeep}
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch07/ch7_3.html">
     7.3.4 Fourier transform of polynomials
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/ch02/ch2_4.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/liuzhengqi1996/math452/main?urlpath=lab/tree/ch02/ch2_4.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binary-svm">
   2.4.1 Binary SVM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#soft-margin-maximization-and-kernel-methods">
   2.4.2 Soft margin maximization and kernel methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binary-logistic-regression">
   2.4.3 Binary logistic regression
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="support-vector-machine">
<h1>2.4 Support vector machine<a class="headerlink" href="#support-vector-machine" title="Permalink to this headline">¶</a></h1>
<p>There is a lot of work about SVM in literature , see
[&#64;drucker1997support; &#64;ben2001support; &#64;cortes1995support; &#64;cristianini2000introduction]
for example. Given a binary linearly separable classification dataset
<span class="math notranslate nohighlight">\({(x_i,y_i)}_{i = 1}^N\)</span>, where
<span class="math notranslate nohighlight">\(x_i\in \mathbb{R}^d, y_i\in \left \{\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix}\right \}\)</span>.
We use <span class="math notranslate nohighlight">\(A_1,A_2\)</span> to denote the data with label
<span class="math notranslate nohighlight">\(\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix}\)</span>,
respectively. Our goal is to find a <span class="math notranslate nohighlight">\(\theta = (w,b)\)</span> where
<span class="math notranslate nohighlight">\(w\in \mathbb{R}^{1\times d}, b\in \mathbb{R}\)</span> such that the hyperplane
<span class="math notranslate nohighlight">\(H_{\theta} = \{x:wx + b = 0\}\)</span> can separate <span class="math notranslate nohighlight">\(A_1,A_2\)</span>.</p>
<div class="section" id="binary-svm">
<h2>2.4.1 Binary SVM<a class="headerlink" href="#binary-svm" title="Permalink to this headline">¶</a></h2>
<p>Binary Support Vector Machine (SVM for short hereinafter) wants to find
the classifiable hyperplane which has the biggest distance with <span class="math notranslate nohighlight">\(A_1\)</span>
and <span class="math notranslate nohighlight">\(A_2\)</span>. Assume that we have the hyperplanes <span class="math notranslate nohighlight">\(wx+b=\pm 1\)</span> with
$<span class="math notranslate nohighlight">\(wx_i+b\ge 1 \quad \mbox{for}\quad x_i\in A_1,\quad wx_i+b\le -1 \quad \mbox{for}\quad x_i\in A_2,\)</span><span class="math notranslate nohighlight">\(
which is similar to the definition
[\[2classH\]](#2classH){reference-type=&quot;eqref&quot; reference=&quot;2classH&quot;}. Let
\)</span>y_1=\begin{pmatrix}1\0\end{pmatrix}<span class="math notranslate nohighlight">\( for \)</span>x_i\in A_1<span class="math notranslate nohighlight">\( and
\)</span>y_2=\begin{pmatrix}0\1\end{pmatrix}<span class="math notranslate nohighlight">\( for \)</span>x_i\in A_2<span class="math notranslate nohighlight">\(. Note that \)</span>w<span class="math notranslate nohighlight">\(
is normal to the hyperplane and the distance between the points
satisfying \)</span><span class="math notranslate nohighlight">\(wx_i+b=\pm 1\)</span><span class="math notranslate nohighlight">\( and the hyperplane \)</span>wx+b=0<span class="math notranslate nohighlight">\( is
\)</span>\displaystyle {1\over |w|_2}<span class="math notranslate nohighlight">\(. Thus, the width of the margin is
\)</span>\displaystyle {2\over |w|_2}$ as shown in Figure
<a class="reference external" href="#fig:margin">1</a>{reference-type=”ref” reference=”fig:margin”}.</p>
<p><img alt="SVM" src="ch02/margin" />{#fig:margin width=”2in”}</p>
<p>For any <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, the smallest distance between points and the
hyperplane is $<span class="math notranslate nohighlight">\(\frac{\min_{i} \ell_i(wx_i+b)}{\|w\|_2},\)</span><span class="math notranslate nohighlight">\( where
\)</span>\ell_i=1-2e_2^Ty_i<span class="math notranslate nohighlight">\(. Note that \)</span><span class="math notranslate nohighlight">\(\ell_i=\begin{cases}
1 &amp; \mbox{ if } y_i=\begin{pmatrix}1\\0\end{pmatrix},
\\
-1 &amp;\mbox{ if } y_i=\begin{pmatrix}0\\1\end{pmatrix}.
\end{cases}\)</span><span class="math notranslate nohighlight">\( Consider the problem
\)</span><span class="math notranslate nohighlight">\(\max_{w,b} \frac{\min_{i} \ell_i(wx_i+b)}{\|w\|_2}.\)</span><span class="math notranslate nohighlight">\( Intuitively, the
best separating hyperplane \)</span>H<span class="math notranslate nohighlight">\( is only determined by those data points
who are closest to \)</span>H$. Those data points are called support vector, and
this method are called support vector machine.</p>
<p>Without loss of generality, we may restrict the norm of <span class="math notranslate nohighlight">\(\|w\|\)</span> to be 1,
which leads to a equivalent optimization problem
$<span class="math notranslate nohighlight">\(\max_{\|w\|_2 = 1} \min_{i} \ell_i(wx_i+b)\)</span><span class="math notranslate nohighlight">\( Actually, we can prove
\)</span>\displaystyle \mathop{\rm argmax}_{|w|<em>2 = 1} \min</em>{i} \ell_i(wx_i+b)$
is nonempty, but here we just admit this fact and only prove the
uniqueness of the solution.</p>
<div class="highlight-lemma notranslate"><div class="highlight"><pre><span></span>If $A_1,A_2$ are linearly separable, then $$\label{binarySVM}
    \mathop{\rm argmax}_{\|w\|_2 = 1} \min_{i} \ell_i(wx_i+b)$$ is
nonempty.
</pre></div>
</div>
<div class="highlight-proof notranslate"><div class="highlight"><pre><span></span>*Proof.* Take $x_{i_1} \in A_1$ and $x_{i_2} \in A_2$, given
$(w,b)\in \{(w,b): l_i(wx_i +b)&gt;0, \forall i\}$, we have $$\begin{cases}
        wx_{i_1} + b &gt; 0,\\
        wx_{i_2} + b &lt; 0
        \end{cases}$$ which implies $|b| &lt; \max_{i} \|x_i\|_2$. So we
have
$$\mathop{\rm argmax}_{\|w\|_2 = 1} \min_{i} \ell_i(wx_i+b) = \mathop{\rm argmax}_{\|w\|_2 = 1, ~|b|\leq \max_{i} \|x_i\|_2} \min_{i} \ell_i(wx_i+b) \neq \emptyset.$$ ◻
</pre></div>
</div>
<div class="highlight-lemma notranslate"><div class="highlight"><pre><span></span>If $A_1,A_2$ are linearly separable, then
$$\mathop{\rm argmax}_{\|w\|_2 = 1} \min_{i} \ell_i(wx_i+b)$$ is a
singleton set.
</pre></div>
</div>
<div class="highlight-proof notranslate"><div class="highlight"><pre><span></span>*Proof.* Denote $\displaystyle m(w,b) = \min_{i} \ell_i(wx_i+b)$. Notice
that $m(w,b)$ is a concave homogeneous function w.r.t $w,b$ and
$\|\cdot\|_2$ is a strictly convex norm. Suppose there are two solution
$(w_1,b_1)$ and $(w_2,b_2)$ such that $w_1 \neq w_2$, take
$\overline{w} = \frac{w_1 + w_2}{2}, \overline{b} = \frac{b_1 + b_2}{2}$,
we must have
$$m(\overline{w},\overline{b}) \geq \frac{m(w_1,b_1)+ m(w_2,b_2)}{2} = \max_{\|w\|_2 = 1} m(w,b),$$
and $$\|\overline{w}\|_2 &lt; 1.$$ So
$$m(\frac{\overline{w}}{\|\overline{w}\|_2},\frac{\overline{b}}{\|\overline{w}\|_2}) = \frac{m(\overline{w},\overline{b}) }{\|\overline{w}\|_2} &gt; \max_{\|w\|_2 = 1} m(w,b),$$
which leads to a contradiction. So all the solutions must have the same
$w$, we denote it as $w^*$. Then if $(w^*,b^*)$ is a solution of problem
([\[binarySVM\]](#binarySVM){reference-type=&quot;ref&quot;
reference=&quot;binarySVM&quot;}), we must have
$$b^* \in \mathop{\rm argmax}_{b} m(w^*,b)$$ Actually,
$$m(w^*,b) = \min\{b+\min_{x\in A_1} w^*x, -b +\min_{x\in A_2} (-w^*x)\}.$$
It is easy to observe that
$\displaystyle \mathop{\rm argmax}_{b} m(w^*,b)$ is a singleton set and
$$b^* = \frac{\min_{x\in A_2} (-w^*x) - \min_{x\in A_1} w^*x}{2}.$$ ◻
</pre></div>
</div>
<p>Denote $<span class="math notranslate nohighlight">\(\label{maxSVM}
    \theta^*_{SVM} = (w_{SVM}^*,b_{SVM}^*) = \mathop{\rm argmax}_{\|w\|_2 = 1} \min_{i} \ell_i(wx_i+b).\)</span>$</p>
<div class="highlight-theorem notranslate"><div class="highlight"><pre><span></span>Let $\theta^*_{SVM} = (w_{SVM}^*,b_{SVM}^*)$ be the solution of
[\[maxSVM\]](#maxSVM){reference-type=&quot;eqref&quot; reference=&quot;maxSVM&quot;}. Then,
$w_{SVM}^*$ must be a linear combination of $x_i^T, i = 1,2,\cdots,N$.
</pre></div>
</div>
<div class="highlight-proof notranslate"><div class="highlight"><pre><span></span>*Proof.* Denote $$S = {\rm span} \{x_i^T\}_{i=1}^N.$$ Then we have
$$\mathbb{R}^{1\times d} = S \oplus^{\perp} S^{\perp}.$$ So $w_{SVM}^*$
can be uniquely decomposed as $w_{SVM}^* = w^*_S + w^*_{S^{\perp}}$
where $w_S\in S$ and $w^*_{S^{\perp}}\in S^{\perp}$. We will prove that
$w^*_{S^{\perp}} = 0$. Suppose not, we have
$$\|w^*_S\|_2 &lt; \|w_{SVM}^*\|_2 = 1.$$ Notice that
$$w_{SVM}^* x_i = w_S^* x_i,\ \forall i = 1,2,\cdots,N.$$ Thus we have
$$\min_{i} \ell_i(w_{SVM}^*x_i+b_{SVM}^*) = \min_{i} \ell_i(w_S^*x_i+b_{SVM}^*).$$
So
$$\min_{i} \ell_i(w_{SVM}^*x_i+b_{SVM}^*) &lt; \frac{\min_{i} \ell_i(w_S^*x_i+b_{SVM}^*)}{\|w_S^*\|} = \min_{i} \ell_i(\frac{w^*_S}{\|w_S^*\|_2}x_i+\frac{b_{SVM}^*}{\|w^*_S}\|_2),$$
which leads to a contradiction to the definition of $\theta_{SVM}^*$. ◻
</pre></div>
</div>
</div>
<div class="section" id="soft-margin-maximization-and-kernel-methods">
<h2>2.4.2 Soft margin maximization and kernel methods<a class="headerlink" href="#soft-margin-maximization-and-kernel-methods" title="Permalink to this headline">¶</a></h2>
<p>We may rewrite the SVM problem as $<span class="math notranslate nohighlight">\(\begin{aligned}
    \max_{w,b}&amp;\ {2\over \|w\|},\\
    s.t.&amp;\ \ell_i(wx_i+b) \geq 1,\ \forall i. \end{aligned}\)</span><span class="math notranslate nohighlight">\( or
equivalently, \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
{\label{SVM_Quad}}
    \min_{w,b}&amp;\ \|w\|^2,\\
    s.t.&amp;\ \ell_i(wx_i+b) \geq 1,\ \forall i. \end{aligned}\)</span>$ Notice
that the feasible domain of margin maximization is nonempty if and only
if dataset is linearly separable. So when the data is linearly
nonseparable, this method can’t even get a classifier even though it may
not be good. One way to handle this problem is to relax the constraint
by adding relaxation variables.</p>
<p>Define soft margin maximization problem $<span class="math notranslate nohighlight">\(\begin{aligned}
{\label{SVM_Quad_soft}}
\min_{w,b,\xi}&amp;\ \|w\|^2 + \lambda^{-1} \sum_{i = 1}^N\xi_i,\\
s.t.&amp;\ \ell_i(wx_i+b) + \xi_i \geq 1,\ \forall i. \\
     &amp;\ \xi_i \geq 0.\end{aligned}\)</span><span class="math notranslate nohighlight">\( where \)</span>\lambda&gt;0<span class="math notranslate nohighlight">\(. The above
problem is equivalent to \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
 \min_{w,b}&amp;\ \|w\|^2 + \lambda^{-1} \sum_{i = 1}^N {\rm ReLU}(1-\ell_i(wx_i+b)).
 \end{aligned}\)</span><span class="math notranslate nohighlight">\( Thus, soft margin maximization problem
[\[SVM_Quad_soft\]](#SVM_Quad_soft){reference-type=&quot;eqref&quot;
reference=&quot;SVM_Quad_soft&quot;} can be reformulated as \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
{\label{SVM_soft}}
\min_{w,b}&amp;\  \sum_{i = 1}^N {\rm ReLU}(1-\ell_i(wx_i+b)) + \lambda \|w\|^2.\end{aligned}\)</span><span class="math notranslate nohighlight">\(
We can still prove that the solution of
([\[SVM_soft\]](#SVM_soft){reference-type=&quot;ref&quot; reference=&quot;SVM_soft&quot;})
satisfies the representation theorem. Thus we can restrict \)</span>w<span class="math notranslate nohighlight">\( to be in
the set \)</span>S<span class="math notranslate nohighlight">\(. Assume that \)</span><span class="math notranslate nohighlight">\(w = \sum_{i = 1}^N \alpha_i x_i^T,\)</span><span class="math notranslate nohighlight">\( Denote
\)</span>\alpha = (\alpha_1,\cdots,\alpha_N)^T<span class="math notranslate nohighlight">\(. We can rewrite the problem
([\[SVM_soft\]](#SVM_soft){reference-type=&quot;ref&quot; reference=&quot;SVM_soft&quot;})
as
\)</span><span class="math notranslate nohighlight">\(\min_{\alpha}\ \sum_{i = 1}^N {\rm ReLU}(1-\ell_i(\sum_{j = 1}^N \langle x_i,x_j\rangle \alpha_j+b)) + \lambda\alpha^T \big(\langle x_i,x_j\rangle\big)_{N\times N} \alpha\\\)</span><span class="math notranslate nohighlight">\(
We can see that the whole problem is only determined by the inner
product of data points but not the data itself directly.\
Use the above formulation, we can induce nonlinearity in SVM. Denote the
input space as \)</span>X<span class="math notranslate nohighlight">\( where \)</span>{x_i}<em>{i=1}^N \subset X<span class="math notranslate nohighlight">\(. We use two steps
to obtain a nonlinear classification model. First, we use a nonlinear
feature mapping \)</span>\phi: X\rightarrow \mathcal{H}<span class="math notranslate nohighlight">\( to map input space \)</span>X<span class="math notranslate nohighlight">\(
to a feature space \)</span>\mathcal{H}<span class="math notranslate nohighlight">\(. Second, we use linear SVM to do
classification on \)</span>{\phi(x_i)}</em>{i=1}^N\subset \mathcal{H}<span class="math notranslate nohighlight">\(.\
We may just asssume dataset after feature mapping \)</span>\phi<span class="math notranslate nohighlight">\( is linearly
separable. Then, the SVM problem after doing feature mapping can be
formulated as problem ([\[SVM_soft\]](#SVM_soft){reference-type=&quot;ref&quot;
reference=&quot;SVM_soft&quot;}) as
\)</span><span class="math notranslate nohighlight">\(\min_{\alpha}\ \sum_{i = 1}^N {\rm ReLU}(1-\ell_i(\sum_{j = 1}^N \langle \phi(x_i),\phi(x_j)\rangle \alpha_j+b)) + \lambda\alpha^T \big(\langle \phi(x_i),\phi(x_j)\rangle\big)_{N\times N} \alpha\\\)</span>$</p>
<p>Notice that to obtain the above problem we don’t really need to know
what exactly is the nonlinear mapping <span class="math notranslate nohighlight">\(\phi\)</span>, but only need to compute
the value of <span class="math notranslate nohighlight">\(&lt;\phi(x_i),\phi(x_j)&gt;\)</span>. So we define a kernel function
<span class="math notranslate nohighlight">\(k: X\times X\rightarrow \mathbb{R}\)</span> such that
$<span class="math notranslate nohighlight">\(k(x,y) = \langle\phi(x),\phi(y)\rangle,\ x,y\in X.\)</span><span class="math notranslate nohighlight">\( Then the kernel
SVM can be formulated as
\)</span><span class="math notranslate nohighlight">\(\min_{\alpha}\ \sum_{i = 1}^N {\rm ReLU}(1-\ell_i(\sum_{j = 1}^N  k(x_i,x_j) \alpha_j+b)) + \lambda\alpha^T \big(k(x_i,x_j)\big)_{N\times N} \alpha\\\)</span>$
In practice, we just need to find a proper kernel function instead of a
good nonlinear feature mapping. Here we list some common used kernel
functions:</p>
<ul class="simple">
<li><p>Polynomial kernel:
<span class="math notranslate nohighlight">\(k(x,y) = (a\langle x,y\rangle+ b)^n, a &gt; 0, b\geq 0, n\in \mathbb{N}^+\)</span>.</p></li>
<li><p>Gaussian kernel: <span class="math notranslate nohighlight">\(k(x,y) = e^{-\gamma\|x-y\|^2}, \gamma &gt; 0\)</span>.</p></li>
<li><p>Laplacian kernel: <span class="math notranslate nohighlight">\(k(x,y) = e^{-\gamma\|x-y\|}, \gamma &gt; 0\)</span></p></li>
<li><p>Tanh kernel: <span class="math notranslate nohighlight">\(k(x,y) = \tanh(a\langle x,y\rangle+b), a&gt;0, b\geq 0.\)</span></p></li>
</ul>
</div>
<div class="section" id="binary-logistic-regression">
<h2>2.4.3 Binary logistic regression<a class="headerlink" href="#binary-logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>In multi-class Logistic regression, if we use <span class="math notranslate nohighlight">\(\|W\|\)</span> to replace
<span class="math notranslate nohighlight">\(\|\bm\theta\|\)</span> in regularization term, we can get another version of
logistic regression:
$<span class="math notranslate nohighlight">\(\mathcal L_\lambda(\bm \theta) = - \sum_{i=1}^k \sum_{x\in A_i} \log p_{i}(x;\bm \theta) + \lambda R(\|W\|),\)</span><span class="math notranslate nohighlight">\(
where \)</span>p_i(x;\bm \theta)<span class="math notranslate nohighlight">\( and \)</span>R(\cdot)<span class="math notranslate nohighlight">\( share the same definitions as
in previous sections of logistic regression. Let
\)</span><span class="math notranslate nohighlight">\(\bm\Theta_{\lambda} = \mathop{{\arg\min}}_{\bm\theta}  \mathcal L_\lambda(\bm\theta).\)</span><span class="math notranslate nohighlight">\(
The following lemma follows directly from the definition of
\)</span>p_{i}(x;\bm \theta)$.</p>
<div class="highlight-lemma notranslate"><div class="highlight"><pre><span></span>For any
$W\in \mathbb{R}^{k\times d}, b\in \mathbb{R}^k, \alpha \in \mathbb{R}$,
we have
$$\mathcal L_\lambda(W,b) = \mathcal L_\lambda(W,b + \alpha \bm 1),$$
where $\bm 1 = (1,1,\cdots,1)^T\in\mathbb{R}^k.$
</pre></div>
</div>
<p>Binary logistic regression refers to the case when <span class="math notranslate nohighlight">\(k=2\)</span>.</p>
<div class="highlight-lemma notranslate"><div class="highlight"><pre><span></span>If $k = 2$, given any $\bm\theta_\lambda = \begin{pmatrix}
    w_1 &amp;  b_1\\
    w_2 &amp; b_2 
    \end{pmatrix} \in \bm\Theta_\lambda$, we have $$w_1 = -w_2.$$
</pre></div>
</div>
<p>According to the above two lemmas, we can restrict <span class="math notranslate nohighlight">\(\bm\theta\)</span> to have
the form $<span class="math notranslate nohighlight">\(\bm\theta = \begin{pmatrix}
    \frac{w}{2}\ &amp;\frac{b}{2}\\
    -\frac{w}{2}\ &amp;-\frac{b}{2}
    \end{pmatrix},\)</span><span class="math notranslate nohighlight">\( so our score mapping can be written as
\)</span><span class="math notranslate nohighlight">\(\label{key}
\bm p(x;\theta) = \begin{pmatrix} 
    \frac{1}{1+ e^{-(wx+b)}}\\
    \frac{1}{1+e^{wx+b}}
\end{pmatrix}.\)</span><span class="math notranslate nohighlight">\( Denote \)</span>\theta = (w,b)<span class="math notranslate nohighlight">\( where
\)</span>w\in \mathbb{R}^d, b\in \mathbb{R}<span class="math notranslate nohighlight">\(, correspondingly, we have
\)</span><span class="math notranslate nohighlight">\(P(\theta) = \prod_{i = 1}^N \frac{1}{1+ e^{-y_i(wx+b)}}.\)</span><span class="math notranslate nohighlight">\( Here we
have the new &quot;label&quot; \)</span>y_i<span class="math notranslate nohighlight">\( defined by \)</span><span class="math notranslate nohighlight">\(\label{key}
y_i = \begin{cases}
1, \quad &amp;\text{if}  \quad x_i \in A_1 \\
-1, \quad &amp;\text{if} \quad x_i \in A_2
\end{cases}.\)</span><span class="math notranslate nohighlight">\( Thus we have
\)</span><span class="math notranslate nohighlight">\(L(\theta) = -\log P(\theta) = \sum_{i = 1}^N \log(1+ e^{-y_i(wx+b)}),\)</span><span class="math notranslate nohighlight">\(
and take \)</span>R(t) = t^2<span class="math notranslate nohighlight">\(, we have
\)</span><span class="math notranslate nohighlight">\(\mathcal L_{\lambda}(\theta)  = L(\theta) + \lambda \|w\|_2^2 = \sum_{i = 1}^N \log(1+ e^{-y_i(wx+b)}) + \lambda \|w\|_2^2.\)</span><span class="math notranslate nohighlight">\(
Here \)</span>L(\theta)$ is a strictly convex function without any global
minima.</p>
<div class="highlight-lemma notranslate"><div class="highlight"><pre><span></span>Assume that $A_1,A_2$ are linearly separable, and we follow the same
definition of linearly classifiable weights:
$$\bm{\Theta} = \left\{\bm\theta: p_i(x; \bm\theta)&gt;p_j(x; \bm\theta),~\forall x\in A_i, j\neq i, i= 1,2\right\},$$
where $$\label{key}
\bm p(x;\theta) = \begin{pmatrix} 
\frac{1}{1+ e^{-(wx+b)}}\\
\frac{1}{1+e^{wx+b}}
\end{pmatrix}.$$ Then, we have the following statement:\

1.  $\theta = (w,b) \in \Theta$ if and only if
    $$\frac{1}{1+ e^{-y_i(wx+b)}} &gt; \frac{1}{2},\ \forall i = 1,2\cdots,N.$$

2.  If $P(\theta) &gt; \frac{1}{2}$, then $\theta$ must be classifiable,
    i.e. $$\{\theta: P(\theta)&gt;\frac{1}{2}\}\subset \bm \Theta.$$

3.  Prove that
    $$\bm\Theta = \{\theta: \lim_{\alpha\rightarrow +\infty} P(\alpha \theta) = 1\}.$$
</pre></div>
</div>
<p>If <span class="math notranslate nohighlight">\(A_1,A_2\)</span> are linearly separable, then <span class="math notranslate nohighlight">\(\displaystyle 
\mathop{\rm argmin}_{w,b} \mathcal L_{\lambda}(\theta)\)</span> is nonempty for
<span class="math notranslate nohighlight">\(\lambda\)</span> sufficiently small.</p>
<div class="highlight-lemma notranslate"><div class="highlight"><pre><span></span>If $A_1,A_2$ are linearly separable, then $$\label{binaryLR}
    \mathop{\rm argmin}_{w,b} \mathcal L_{\lambda}(\theta)$$ is a
singleton set for $\lambda$ sufficiently small.
</pre></div>
</div>
<div class="highlight-proof notranslate"><div class="highlight"><pre><span></span>*Proof.* Because $L(\theta)$ is strictly convex w.r.t. $\theta$ and
$\|w\|^2$ is convex w.r.t. $\theta$, so
$\mathcal L(\theta,\lambda)  = L(\theta) + \lambda \|w\|_2^2$ is stricly
convex w.r.t. $\theta$, which implies our result directly. ◻
</pre></div>
</div>
<p>For <span class="math notranslate nohighlight">\(\lambda\)</span> sufficiently small, denote
$<span class="math notranslate nohighlight">\(\theta_{LR}(\lambda) = (w_{LR}(\lambda),b_{LR}(\lambda)) = \mathop{\rm argmin}_{w,b} \mathcal L_{\lambda}(\theta).\)</span>$</p>
<div class="highlight-lemma notranslate"><div class="highlight"><pre><span></span>If $A_1,A_2$ are linearly separable,

1.  $\mathcal L_{\lambda}(\theta) \rightarrow 0$ as
    $\lambda \rightarrow 0$.

2.  $\|w_{LR}(\lambda)\|\rightarrow \infty$ as $\lambda \rightarrow 0$.

3.  $\displaystyle \min_{i} y_i(w_{LR}(\lambda) x_i + b_{LR}(\lambda))\rightarrow \infty$
    as $\lambda \rightarrow 0$.

4.  If $A_1,A_2$ are also nonempty,
    $$\|b_{LR}(\lambda)\|\leq \max_{i} \|x_i\| \|w_{LR}(\lambda)\|$$ for
    $\lambda$ sufficiently small.
</pre></div>
</div>
<p>The above lemma implies that <span class="math notranslate nohighlight">\(\theta_{LR}(\lambda)/\|w_{LR}(\lambda)\|\)</span>
is bounded for <span class="math notranslate nohighlight">\(\lambda\)</span> sufficiently small.</p>
<div class="highlight-theorem notranslate"><div class="highlight"><pre><span></span>If $A_1,A_2$ are linearly separable, then
$\frac{\theta_{LR}(\lambda)}{\|w_{LR}(\lambda)\|}$ converge to
$\theta^*_{SVM}$ as $\lambda \rightarrow 0$, i.e.
$$\theta^*_{SVM} = \lim_{\lambda\rightarrow 0} \frac{\theta_{LR}(\lambda)}{\|w_{LR}(\lambda)\|}.$$
</pre></div>
</div>
<div class="highlight-proof notranslate"><div class="highlight"><pre><span></span>*Proof.* Because $\frac{\theta_{LR}(\lambda)}{\|w_{LR}(\lambda)\|}$ is
bounded for $\lambda$ sufficiently small, we only need to prove that it
has no convergence points other than $\theta^*_{SVM}$ as
$\lambda \rightarrow 0$.\
We first introduce a soft margin function
$m: \mathbb{R}^n \rightarrow \mathbb{R}$ such that
$$m(\theta) = \min_{i} y_i(wx_i + b).$$ Suppose that there exist a
sequence $\{\lambda_n\}_{n = 1}^\infty$ and
$\bar{\theta} = (\bar{w},\bar{b}) \neq \theta^*_{SVM}$ so that
$\lambda_n \downarrow 0$ and
$\frac{\theta_{LR}(\lambda_n)}{\|w_{LR}(\lambda_n)\|} \rightarrow \bar{\theta}$
as $n\rightarrow \infty$. Obviously $\|\bar{w}\| = 1$, and
$$0 \leq m(\bar{\theta}) &lt; m(\theta^*_{SVM}).$$ Take a positive number
$\epsilon &lt; m(\theta^*_{SVM}) - m(\bar{\theta})$. Then there exists
$K_0\in \mathbb{N}^+$ such that
$$0 &lt; m(\frac{\theta_{LR}(\lambda_n)}{\|w_{LR}(\lambda_n)\|}) &lt;  m(\theta^*_{SVM})- \epsilon,$$
for all $n \geq K_0$.\
Consider $\theta_n = (w_n,b_n) = \|w_{LR}(\lambda_n)\|\theta^*_{SVM}$.
Then
$$\lim_{n\rightarrow \infty} \frac{\log(1 + e^{-m(\theta_n)})}{\log(1 + e^{-m(\theta_{LR}(\lambda_n))}} = \lim_{n \rightarrow \infty} e^{\|w_{LR}(\lambda_n)\| (m(\frac{\theta_{LR}(\lambda_n)}{\|w_{LR}(\lambda_n)\|}) -  m(\theta^*_{SVM}))} \leq \lim_{n \rightarrow \infty} e^{-\|w_{LR}(\lambda_n)\| \epsilon} = 0.$$
So there exists a $K_1 \in \mathbb{N}^+$ such that
$$N \log(1 + e^{-m(\theta_n)}) &lt; \log(1 + e^{-m(\theta_{LR}(\lambda_n))})$$
for all $n \geq K_1$. Take $K  = \max\{K_0, K _1\}$. Then for all
$n \geq K$, we have
$$\mathcal{L}(\theta_n) \leq N \log(1 + e^{-m(\theta_n)}) + \lambda \|w_{\lambda_n}\|^2 &lt; \log(1 + e^{-m(\theta_{LR}(\lambda_n))})+ \lambda \|w_{\lambda_n}\|^2 \leq \mathcal{L}(\theta_{LR}(\lambda_n)),$$
which contradicts the definition of $\theta_{LR}(\lambda)$. ◻
</pre></div>
</div>
<p>For the proof of the above theorem, you can also refer to the paper by
Rosset Saharon, Zhu Ji and Trevor J. Hastie  [&#64;rosset2004margin].</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="ch2_3.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">2.3 KL divergence and cross-entropy</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="../ch03/ch3_.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Ch3 Probability</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Department of Mathematics, Penn State University Park<br/>
        
            &copy; Copyright The Pennsylvania State University, 2021. This material is not licensed for resale.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-206078372-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>