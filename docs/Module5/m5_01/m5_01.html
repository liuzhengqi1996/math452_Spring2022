
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Data normalization and weights initialization &#8212; Math 452 Site</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Batch normalization" href="../m5_02/m5_02.html" />
    <link rel="prev" title="Module 5: Normalization, ResNet and Multigrid" href="../module5_.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/PSU_SCI_RGB_2C.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Math 452 Site</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Welcome to Math 452
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  contents
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module0/ch0_.html">
   Module 0 Get started: course information and preparations:
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module0/ch0_1.html">
     Course information, requirements and reference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module0/ch0_2.html">
     Course background and introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module0/ch0_3.html">
     Introduction to Python and Pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module0/quiz0.html">
     Preliminary Quiz
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module1/module1_.html">
   Module 1: Linear machine learning models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_01.html">
     Machine learning basics, popular data sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_02.html">
     Linearly separable sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_03.html">
     Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_04.html">
     KL-divergence and cross-entropy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_05.html">
     Support vector machine and relation with LR
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_06.html">
     Optimization and gradient descent method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/m1_hw.html">
     Homework 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/Programming_Assignment_1.html">
     Module 1 Programming Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module1/quiz1.html">
     Quiz 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module2/module2_.html">
   Module 2: Probability and training algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_01.html">
     Introduction to probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_02.html">
     Probabilistic derivation of logistic regression models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_03.html">
     Convex functions and convergence of gradient descen
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_04.html">
     Stochastic gradient descent method and convergence theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_05.html">
     MNIST: training and generalization accuracy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/m2_hw.html">
     Homework 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/Programming_Assignment_2.html">
     Week 2 Programming Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module2/quiz2.html">
     Quiz 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module3/module3_.html">
   Module 3: Deep neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_01/m3_01.html">
     Nonlinear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_02/m3_02.html">
     Polynomials and Weierstrass theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_03/m3_03.html">
     Finite element method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_04/m3_04.html">
     Deep neural network functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_05/m3_05.html">
     Universal approximation properties
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_06.html">
     Application to data classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_07.html">
     DNN for image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_08/m3_08.html">
     Monte Carlo Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/C08_DNN.html">
     Building and Training Deep Neural Networks (DNNs) with Pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/m3_hw.html">
     Homework 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module3/Programming_Assignment_3.html">
     Week 3 Programming Assignment
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module4/module4_.html">
   Module 4: Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_01/m4_01.html">
     Convolutional neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_02/m4_02.html">
     Convolutional operations on images
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_03/m4_03.html">
     Some classic CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_04/m4_04.html">
     Training CNN with GPU on Colab
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_05.html">
     Building and Training Convolutional Neural Networks (CNNs) with Pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/m4_hw.html">
     Homework 4
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/Programming_Assignment_4.html">
     Week 4 Programming Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module4/quiz4.html">
     Quiz 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../module5_.html">
   Module 5: Normalization, ResNet and Multigrid
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Data normalization and weights initialization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../m5_02/m5_02.html">
     Batch normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../m5_03/m5_03.html">
     Building and Training ResNet with Pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../m5_04/m5_04.html">
     Multigrid Method for Finite Element
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../m5_hw.html">
     Homework 5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Programming_Assignment_5.html">
     Week 5 Programming Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../quiz5.html">
     Quiz 5
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Module6/module6_.html">
   Module 6: MgNet
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module6/m6_01.html">
     1D and 2D Finite Element and Multigrid
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module6/m6_02.html">
     Multigrid and MgNet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module6/MG_MgNet.html">
     Multigrid and MgNet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Module6/Final_Project.html">
     MATH 497: Final Project
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/Module5/m5_01/m5_01.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/liuzhengqi1996/math452_Spring2022/main?urlpath=lab/tree/Module5/m5_01/m5_01.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#download-the-lecture-notes-here-notes">
   Download the lecture notes here: Notes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-normalization-in-dnns-and-cnns">
   Data normalization in DNNs and CNNs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalization-for-input-data-of-dnns">
     Normalization for input data of DNNs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-normalization-for-images-in-cnns">
     Data normalization for images in CNNs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparison-of-sqrt-left-sigma-x-right-j-and-sqrt-left-tilde-sigma-x-right-j-on-cifar10">
     Comparison of
     <span class="math notranslate nohighlight">
      \(\sqrt{\left[\sigma_{X}\right]_{j}}\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(\sqrt{\left[\tilde{\sigma}_{X}\right]_{j}}\)
     </span>
     on CIFAR10.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xaviers-initialization">
     Xavier’s Initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kaimings-initialization">
     Kaiming’s initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialization-in-cnn-models-and-experiments">
     Initialization in CNN models and experiments
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="data-normalization-and-weights-initialization">
<h1>Data normalization and weights initialization<a class="headerlink" href="#data-normalization-and-weights-initialization" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">IFrame</span>

<span class="n">IFrame</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="s2">&quot;https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_01dyptyu&amp;flashvars[streamerType]=auto&amp;amp;flashvars[localizationCode]=en&amp;amp;flashvars[leadWithHTML5]=true&amp;amp;flashvars[sideBarContainer.plugin]=true&amp;amp;flashvars[sideBarContainer.position]=left&amp;amp;flashvars[sideBarContainer.clickToClose]=true&amp;amp;flashvars[chapters.plugin]=true&amp;amp;flashvars[chapters.layout]=vertical&amp;amp;flashvars[chapters.thumbnailRotator]=false&amp;amp;flashvars[streamSelector.plugin]=true&amp;amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;amp;flashvars[dualScreen.plugin]=true&amp;amp;flashvars[hotspots.plugin]=1&amp;amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;amp;&amp;wid=1_ri00enil&quot;</span>  <span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="s1">&#39;800&#39;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="s1">&#39;500&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="800"
    height="500"
    src="https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_01dyptyu&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_ri00enil"
    frameborder="0"
    allowfullscreen
></iframe>
</div></div>
</div>
<div class="section" id="download-the-lecture-notes-here-notes">
<h2>Download the lecture notes here: <a class="reference external" href="https://sites.psu.edu/math452/files/2022/03/E01-02InitializationNormalization.pdf">Notes</a><a class="headerlink" href="#download-the-lecture-notes-here-notes" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="data-normalization-in-dnns-and-cnns">
<h2>Data normalization in DNNs and CNNs<a class="headerlink" href="#data-normalization-in-dnns-and-cnns" title="Permalink to this headline">¶</a></h2>
<div class="section" id="normalization-for-input-data-of-dnns">
<h3>Normalization for input data of DNNs<a class="headerlink" href="#normalization-for-input-data-of-dnns" title="Permalink to this headline">¶</a></h3>
<p>Consider that we have the all training data as</p>
<div class="math notranslate nohighlight">
\[
    (X, Y):=\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{N}
\]</div>
<p>for <span class="math notranslate nohighlight">\(x_{i} \in \mathbb{R}^{d}\)</span> and <span class="math notranslate nohighlight">\(y_{i} \in \mathbb{R}^{k}\)</span>.</p>
<p>Before we input every data into a DNN model, we will apply the following
normalization for all data <span class="math notranslate nohighlight">\(x_{i}\)</span> for each component. Let denote</p>
<div class="math notranslate nohighlight">
\[
    \left[x_{i}\right]_{j} \longleftrightarrow \text { the } \mathrm{j} \text {-th component of data } x_{i}
\]</div>
<p>Then we have following formula of for all <span class="math notranslate nohighlight">\(j=1,2, \cdots, d\)</span></p>
<div class="math notranslate nohighlight">
\[
    \left[\tilde{x}_{i}\right]_{j}=\frac{\left[x_{i}\right]_{j}-\left[\mu_{X}\right]_{j}}{\sqrt{\left[\sigma_{X}\right]_{j}}}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
    \left[\mu_{X}\right]_{j}=\mathbb{E}_{x \sim X}\left[[x]_{j}\right]=\frac{1}{N} \sum_{i=1}^{N}\left[x_{i}\right]_{j}, \quad\left[\sigma_{X}\right]_{j}=\mathbb{V}_{x \sim X}\left[[x]_{j}\right]=\frac{1}{N} \sum_{i=1}^{N}\left(\left[x_{i}\right]_{j}-\left[\mu_{X}\right]_{j}\right)^{2} 
\]</div>
<p>Here <span class="math notranslate nohighlight">\(x \sim X\)</span> means that <span class="math notranslate nohighlight">\(x\)</span> is a discrete random variable on <span class="math notranslate nohighlight">\(X\)</span> with
probability</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{P}\left(x=x_{i}\right)=\frac{1}{N}
\]</div>
<p>for any <span class="math notranslate nohighlight">\(x_{i} \in X\)</span>.</p>
<p>For simplicity, we rewrite the element-wise definition above as the
following compact form</p>
<div class="math notranslate nohighlight" id="equation-eq1-6">
<span class="eqno">(42)<a class="headerlink" href="#equation-eq1-6" title="Permalink to this equation">¶</a></span>\[
    \tilde{x}_{i}=\frac{x_{i}-\mu_{X}}{\sqrt{\sigma_{X}}}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
    x_{i}, \tilde{x}_{i}, \mu_{X}, \sigma_{X} \in \mathbb{R}^{d}
\]</div>
<p>defined as before and all operations in <a class="reference internal" href="#equation-eq1-6">(42)</a> are element-wise.</p>
<p>Here we note that, by normalizing the data set, we have the next
properties for new data <span class="math notranslate nohighlight">\(\tilde{x} \in \tilde{X}\)</span> with component
<span class="math notranslate nohighlight">\(j=1,2, \cdots, d\)</span>,</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}_{\bar{X}}\left[[\tilde{x}]_{j}\right]=\frac{1}{N} \sum_{i=1}^{N}\left[\tilde{x}_{i}\right]_{j}=0
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}_{\tilde{X}}\left[[\tilde{x}]_{j}\right]=\frac{1}{N} \sum_{i=1}^{N}\left(\left[\tilde{x}_{i}\right]_{j}-\mathbb{E}_{\tilde{X}}\left[[\tilde{x}]_{j}\right]\right)^{2}=1
\]</div>
<p>Finally, we will have a “new” data set</p>
<div class="math notranslate nohighlight">
\[
    \tilde{X}=\left\{\tilde{x}_{1}, \tilde{x}_{2}, \cdots, \tilde{x}_{N}\right\}
\]</div>
<p>with unchanged label set <span class="math notranslate nohighlight">\(Y\)</span>. For the next sections, without special notices, we use <span class="math notranslate nohighlight">\(X\)</span> data set as the normalized one as default.</p>
</div>
<div class="section" id="data-normalization-for-images-in-cnns">
<h3>Data normalization for images in CNNs<a class="headerlink" href="#data-normalization-for-images-in-cnns" title="Permalink to this headline">¶</a></h3>
<p>For images, consider we have a color image data set
<span class="math notranslate nohighlight">\((X, Y):=\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{N}\)</span> where</p>
<div class="math notranslate nohighlight">
\[
    x_{i} \in \mathbb{R}^{3 \times m \times n}
\]</div>
<p>We further denote these the <span class="math notranslate nohighlight">\((s, t)\)</span> pixel value for data <span class="math notranslate nohighlight">\(x_{i}\)</span> at channel <span class="math notranslate nohighlight">\(j\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
    \left[x_{i}\right]_{j ; s t} \longleftrightarrow(s, t) \text { pixel value for } x_{i} \text { at channel } j
\]</div>
<p>where <span class="math notranslate nohighlight">\(1 \leq i \leq N, 1 \leq j \leq 3,1 \leq s \leq m\)</span>, and
<span class="math notranslate nohighlight">\(1 \leq j \leq n .\)</span></p>
<p>Then, the normalization for <span class="math notranslate nohighlight">\(x_{i}\)</span> is defined by</p>
<div class="math notranslate nohighlight">
\[
    \left[\tilde{x}_{i}\right]_{j ; s t}=\frac{\left[x_{i}\right]_{j ; s t}-\left[\mu_{X}\right]_{j}}{\sqrt{\left[\sigma_{X}\right]_{j}}}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
    \left[x_{i}\right]_{j ; s t},\left[\tilde{x}_{i}\right]_{j ; s t},\left[\mu_{X}\right]_{j},\left[\sigma_{X}\right]_{j} \in \mathbb{R}
\]</div>
<p>Here</p>
<div class="math notranslate nohighlight">
\[
    \left[\mu_{X}\right]_{j}=\frac{1}{m \times n \times N} \sum_{1 \leq i \leq N} \sum_{1 \leq s \leq m, 1 \leq t \leq n}\left[x_{i}\right]_{j ; s t}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
    \left[\sigma_{X}\right]_{j}=\frac{1}{N \times m \times n} \sum_{1 \leq i \leq N} \sum_{1 \leq s \leq m, 1 \leq t \leq n}\left(\left[x_{i}\right]_{j ; s t}-\left[\mu_{X}\right]_{j}\right)^{2}
\]</div>
<p>In batch normalization, we confirmed with Lian by both numerical test
and code checking that <span class="math notranslate nohighlight">\(\mathrm{BN}\)</span> also use the above formula to
compute the variance in <span class="math notranslate nohighlight">\(\mathrm{CNN}\)</span> for each channel.</p>
<p>Another way to compute the variance over each channel is to compute the
standard deviation on each channel for every data, and then average them
in the data direction.</p>
<div class="math notranslate nohighlight">
\[
    \sqrt{\left[\tilde{\sigma}_{X}\right]_{j}}=\frac{1}{N} \sum_{1 \leq i \leq N}\left(\frac{1}{m \times n} \sum_{1 \leq s \leq m, 1 \leq t \leq n}\left(\left[x_{i}\right]_{j ; s t}-\left[\mu_{i}\right]_{j}\right)^{2}\right)^{\frac{1}{2}}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
    \left[\mu_{i}\right]_{j}=\frac{1}{m \times n} \sum_{1 \leq s \leq m, 1 \leq t \leq n}\left[x_{i}\right]_{j ; s t} \]</div>
</div>
<div class="section" id="comparison-of-sqrt-left-sigma-x-right-j-and-sqrt-left-tilde-sigma-x-right-j-on-cifar10">
<h3>Comparison of <span class="math notranslate nohighlight">\(\sqrt{\left[\sigma_{X}\right]_{j}}\)</span> and <span class="math notranslate nohighlight">\(\sqrt{\left[\tilde{\sigma}_{X}\right]_{j}}\)</span> on CIFAR10.<a class="headerlink" href="#comparison-of-sqrt-left-sigma-x-right-j-and-sqrt-left-tilde-sigma-x-right-j-on-cifar10" title="Permalink to this headline">¶</a></h3>
<p>They share the same <span class="math notranslate nohighlight">\(\mu_{X}\)</span> as $<span class="math notranslate nohighlight">\(\mu_{X}=\left(\begin{array}{lll}
0.49140105 &amp; 0.48215663 &amp; 0.44653168
\end{array}\right)\)</span><span class="math notranslate nohighlight">\( But they had different standard deviation
estimates: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
&amp;\sqrt{\left[\sigma_{X}\right]_{j}}=(0.247032840 .243484990 .26158834) \\
&amp;\sqrt{\left[\tilde{\sigma}_{X}\right]_{j}}=(0.202201930 .199316350 .20086373)
\end{aligned}\)</span>$</p>
<p>##Initialization for deep neural networks</p>
</div>
<div class="section" id="xaviers-initialization">
<h3>Xavier’s Initialization<a class="headerlink" href="#xaviers-initialization" title="Permalink to this headline">¶</a></h3>
<p>The goal of Xavier initialization [1] is to initialize the deep neural
network to avoid gradient vanishing or blowup when the input is white
noise.</p>
<p>Let us denote the DNN models as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}f^{1}(x) &amp; =W^{1} x+b^{1} \\ f^{\ell}(x) &amp; =W^{\ell} \sigma\left(f^{\ell-1}(x)\right)+b^{\ell} \quad \ell=2: L, \\ f(x) &amp; =f^{L}\end{cases}
\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n_{0}}\)</span> and
<span class="math notranslate nohighlight">\(f^{\ell} \in \mathbb{R}^{n_{\ell}}\)</span>. More precisely, we have</p>
<div class="math notranslate nohighlight">
\[
    W^{\ell} \in \mathbb{R}^{n_{\ell} \times n_{\ell-1}} 
\]</div>
<p>The basic assumptions that we make are:</p>
<ul class="simple">
<li><p>The initial weights <span class="math notranslate nohighlight">\(W_{i j}^{\ell}\)</span> are i.i.d symmetric random
variables with mean 0, namely the probability density function of
<span class="math notranslate nohighlight">\(W_{i j}^{\ell}\)</span> is even.</p></li>
<li><p>The initial bias <span class="math notranslate nohighlight">\(b^{\ell}=0\)</span>.</p></li>
</ul>
<p>Now we choose the variance of the initial weights to ensure that the
features <span class="math notranslate nohighlight">\(f^{L}\)</span> and gradients don’t blow up or vanish. To this end we
have the following lemma.</p>
<p>Lemma 1. Under the previous assumptions <span class="math notranslate nohighlight">\(f_{i}^{\ell}\)</span> is a symmetric
random variable with <span class="math notranslate nohighlight">\(\mathbb{E}\left[f^{\ell}\right]=0 .\)</span> Moreover, we
have the following identity
$<span class="math notranslate nohighlight">\(\mathbb{E}\left[\left(f_{i}^{\ell}\right)^{2}\right]=\sum_{k} \mathbb{E}\left[\left(W_{i k}^{\ell}\right)^{2}\right] \mathbb{E}\left[\sigma\left(f_{k}^{\ell-1}\right)^{2}\right]\)</span><span class="math notranslate nohighlight">\(
Now, if \)</span>\sigma=i d<span class="math notranslate nohighlight">\(, we can prove by induction from \)</span>\ell=1$ that</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[f_{i}^{L}\right]=\left(\Pi_{\ell=2}^{L} n_{\ell-1} \operatorname{Var}\left[W_{s t}^{\ell}\right]\right)\left(\mathbb{V}\left[W_{s t}^{1}\right] \sum_{k} \mathbb{E}\left[\left([x]_{k}\right)^{2}\right]\right)
\]</div>
<p>We make this assumption that <span class="math notranslate nohighlight">\(\sigma=i d\)</span>, which is pretty reasonably
since most activation functions in use at the time (such as the
hyperbolic tangent) were close to the identity near 0 .</p>
<p>Now, if we set</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[W_{i k}^{\ell}\right]=\frac{1}{n_{\ell-1}}, \quad \forall \ell \geq 2
\]</div>
<p>we will obtain</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[f_{i}^{L}\right]=\mathbb{V}\left[f_{j}^{L-1}\right]=\cdots=\mathbb{V}\left[f_{k}^{1}\right]=\mathbb{V}\left[W_{s t}^{1}\right] \sum_{k} \mathbb{E}\left[\left([x]_{k}\right)^{2}\right]
\]</div>
<p>Thus, in pure DNN models, it is enough to just control
<span class="math notranslate nohighlight">\(\sum_{k} \mathbb{E}\left[\left([x]_{k}\right)^{2}\right] .\)</span></p>
<p>A similar analysis of the propagation of the gradient
<span class="math notranslate nohighlight">\(\left(\frac{\partial L(\theta)}{\partial f^{t}}\right)\)</span> suggests that
we set</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[W_{i k}^{\ell}\right]=\frac{1}{n_{\ell}}
\]</div>
<p>Thus, the Xavier’s initialization suggests to initialize
<span class="math notranslate nohighlight">\(W_{i k}^{\ell}\)</span> with variance as:</p>
<ul class="simple">
<li><p>To control <span class="math notranslate nohighlight">\(\mathbb{V}\left[f_{i}^{\ell}\right]:\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \operatorname{Var}\left[W_{i k}^{\ell}\right]=\frac{1}{n_{\ell-1}}
\]</div>
<ul class="simple">
<li><p>To control
<span class="math notranslate nohighlight">\(\mathbb{V}\left[\frac{\partial L(\theta)}{\partial f_{i}^{l}}\right]:\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \operatorname{Var}\left[W_{i k}^{\ell}\right]=\frac{1}{n_{\ell}}
\]</div>
<ul class="simple">
<li><p>Trade-off to control
<span class="math notranslate nohighlight">\(\mathbb{V}\left[\frac{\partial L(\theta)}{\partial W_{i k}^{l}}\right]:\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \operatorname{Var}\left[W_{i k}^{\ell}\right]=\frac{2}{n_{\ell-1}+n_{\ell}}
\]</div>
<p>Here we note that, this analysis works for all symmetric type
distribution around zero, but we often just choose uniform distribution
<span class="math notranslate nohighlight">\(\mathcal{U}(-a, a)\)</span> and normal distribution
<span class="math notranslate nohighlight">\(\mathcal{N}\left(0, s^{2}\right) .\)</span> Thus, the final version of Xavier’s
initialization takes the trade-off type as</p>
<div class="math notranslate nohighlight">
\[
    W_{i k}^{\ell} \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{\ell}+n_{\ell-1}}}, \sqrt{\frac{6}{n_{\ell}+n_{\ell-1}}}\right)
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
    W_{i k}^{\ell} \sim \mathcal{N}\left(0, \frac{2}{n_{\ell}+n_{\ell-1}}\right) 
\]</div>
</div>
<div class="section" id="kaimings-initialization">
<h3>Kaiming’s initialization<a class="headerlink" href="#kaimings-initialization" title="Permalink to this headline">¶</a></h3>
<p>In [2], Kaiming He and others extended this analysis to get an exact
result when the activation function is the ReLU.</p>
<p>We first have the following lemma for symmetric distribution.</p>
<p>Lemma 2. If <span class="math notranslate nohighlight">\(X_{i} \in \mathbb{R}\)</span> for <span class="math notranslate nohighlight">\(i=1:\)</span> n are i.i.d with symmetric
probability density function <span class="math notranslate nohighlight">\(p(x)\)</span>, i.e. <span class="math notranslate nohighlight">\(p(x)\)</span> is even. Then for any
nonzero random vector
<span class="math notranslate nohighlight">\(Y=\left(Y_{1}, Y_{2}, \cdots, Y_{n}\right) \in \mathbb{R}^{n}\)</span> which is
independent with <span class="math notranslate nohighlight">\(X_{i}\)</span>, the following random variable</p>
<div class="math notranslate nohighlight">
\[
    Z=\sum_{i=1}^{n} X_{i} Y_{i}
\]</div>
<p>is also symmetric.</p>
<p>Then state the following result for ReLU function and random variable
with symmetric distribution around 0 .</p>
<p>Lemma 3. If <span class="math notranslate nohighlight">\(X\)</span> is a random variable on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> with symmetric
probability density <span class="math notranslate nohighlight">\(p(x)\)</span> around zero, i.e., $<span class="math notranslate nohighlight">\(p(x)=p(-x)\)</span><span class="math notranslate nohighlight">\( Then we
have \)</span>\mathbb{E} X=0$ and</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}\left[[\operatorname{ReLU}(X)]^{2}\right]=\frac{1}{2} \operatorname{Var}[X]
\]</div>
<p>Based on the previous Lemma 1, we know that <span class="math notranslate nohighlight">\(f_{k}^{\ell-1}\)</span> is a
symmetric distribution around 0 . The most important observation in
Kaiming’s paper is that:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[f_{i}^{\ell}\right]=n_{\ell-1} \mathbb{V}\left[W_{i j}^{\ell}\right] \mathbb{E}\left[\left[\sigma\left(f_{j}^{\ell-1}\right)\right]^{2}\right]=n_{\ell-1} \mathbb{V}\left[W_{i k}^{\ell}\right] \frac{1}{2} \mathbb{V}\left[f_{k}^{\ell-1}\right]
\]</div>
<p>if <span class="math notranslate nohighlight">\(\sigma=\)</span> ReLU. Thus, Kaiming’s initialization suggests to take:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[W_{i k}^{\ell}\right]=\frac{2}{n_{\ell-1}}, \quad \forall \ell \geq 2
\]</div>
<p>For the first layer <span class="math notranslate nohighlight">\(\ell=1\)</span>, by definition</p>
<div class="math notranslate nohighlight">
\[
    f^{1}=W^{1} x+b^{1}
\]</div>
<p>there is no ReLU, thus it should be
<span class="math notranslate nohighlight">\(\mathbb{V}\left[W_{i k}^{1}\right]=\frac{1}{d} .\)</span> For simplicity, they
still use <span class="math notranslate nohighlight">\(\mathbb{V}\left[W_{i k}^{1}\right]=\)</span> <span class="math notranslate nohighlight">\(\frac{2}{d}\)</span> in the
paper. Similarly, an analysis of the propagation of the gradient
suggests that we set
<span class="math notranslate nohighlight">\(\mathbb{V}\left[W_{i k}^{\ell}\right]=\frac{2}{n_{\ell}}\)</span>. However, in
paper authors did not suggest to take the trade-off version, they
just chose</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[W_{i k}^{\ell}\right]=\frac{2}{n_{\ell-1}}
\]</div>
<p>as default.</p>
<p>Thus, the final version of Kaiming’s initialization takes the forward
type as</p>
<div class="math notranslate nohighlight">
\[
    W_{i k}^{\ell} \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{\ell-1}}}, \sqrt{\frac{6}{n_{\ell-1}}}\right)
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
    W_{i k}^{\ell} \sim \mathcal{N}\left(0, \frac{2}{n_{\ell-1}}\right)
\]</div>
</div>
<div class="section" id="initialization-in-cnn-models-and-experiments">
<h3>Initialization in CNN models and experiments<a class="headerlink" href="#initialization-in-cnn-models-and-experiments" title="Permalink to this headline">¶</a></h3>
<p>For CNN models, following the analysis above we have the next iterative
scheme in CNNs</p>
<div class="math notranslate nohighlight">
\[
    f^{\ell, i}=K^{\ell, i} * \sigma\left(f^{\ell, i-1}\right)
\]</div>
<p>where
<span class="math notranslate nohighlight">\(f^{\ell, i-1} \in \mathbb{R}^{c_{\ell} \times n_{\ell} \times m_{\ell}}, f^{\ell, i} \in \mathbb{R}^{h_{\ell} \times n_{\ell} \times m_{\ell}}\)</span>
and
<span class="math notranslate nohighlight">\(K \in \mathbb{R}^{(2 k+1) \times(2 k+1) \times h_{\ell} \times c_{\ell}}\)</span>.
Thus we have</p>
<div class="math notranslate nohighlight">
\[
    \left[f^{\ell, i}\right]_{h ; p, q}=\sum_{c=1}^{c_{l}} \sum_{s, t=-k}^{k} K_{h, c ; s, t}^{\ell, i} * \sigma\left(\left[f^{\ell, i-1}\right]_{c ; p+s, q+t}\right)
\]</div>
<p>Take variance on both sides, we will get</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[\left[f^{\ell, i}\right]_{h ; p, q}\right]=c_{\ell}(2 k+1)^{2} \mathbb{V}\left[K_{h, o ; s, t}^{\ell, i}\right] \mathbb{E}\left[\left(\left[f^{\ell, i-1}\right]_{o ; p+s, q+t}\right)^{2}\right]
\]</div>
<p>thus we have the following initialization strategies: Xavier’s
initialization</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[K_{h, o ; s, t}^{\ell, i}\right]=\frac{2}{\left(c_{\ell}+h_{\ell}\right)(2 k+1)^{2}}
\]</div>
<p>Kaiming’s initialization</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[K_{h, o ; s, t}^{\ell, i}\right]=\frac{2}{c_{\ell}(2 k+1)^{2}}
\]</div>
<p>Here we can take this Kaiming’s initialization as:</p>
<ul class="simple">
<li><p>Double the Xavier’s choice, and get</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[K_{h, o ; s, t}^{\ell, i}\right]=\frac{4}{\left(c_{\ell}+h_{\ell}\right)(2 k+1)^{2}} 
\]</div>
<ul class="simple">
<li><p>Then pick <span class="math notranslate nohighlight">\(c_{\ell}\)</span> or <span class="math notranslate nohighlight">\(h_{\ell}\)</span> for final result</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathbb{V}\left[K_{h, o ; s, t}^{\ell, i}\right]=\frac{4}{\left(c_{\ell}+h_{\ell}\right)(2 k+1)^{2}}=\frac{2}{c_{\ell}(2 k+1)^{2}} 
\]</div>
<p>And they have the both uniform and normal distribution type.</p>
<p><img alt="image" src="../../_images/img14.png" /></p>
<p>Fig. The convergence of a 22-layer large model. The <span class="math notranslate nohighlight">\(x\)</span>-axis is the
number of training epochs. The y-axis is the top-1 error of 3,000 random
val samples, evaluated on the center crop. Use ReLU as the activation
for both cases. Both Kaiming’s initialization (red) and “Xavier’s”
(blue) lead to convergence, but Kaiming’s initialization starts
reducing error earlier.</p>
<p><img alt="image" src="../../_images/img22.png" /></p>
<p>Fig. The convergence of a 30-layer small model (see the main text).
Use ReLU as the activation for both cases. Kaiming’s initialization
(red) is able to make it converge. But “Xavier’s” (blue) [1]
completely stalls - It is also verified that that its gradients are all
diminishing. It does not converge even given more epochs. Given a
22-layer model, in cifar10 the convergence with Kaiming’s initialization
is faster than Xavier’s, but both of them are able to converge and the
validation accuracies with two different initialization are about the
same(error is <span class="math notranslate nohighlight">\(33.82,33.90)\)</span>.</p>
<p>With extremely deep model with up to 30 layers, Kaiming’s initialization
is able to make the model convergence. On the contrary, Xavier’s method
completely stalls the learning.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "liuzhengqi1996/math452_Spring2022",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Module5/m5_01"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="../module5_.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Module 5: Normalization, ResNet and Multigrid</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="../m5_02/m5_02.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Batch normalization</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Department of Mathematics, Penn State University Park<br/>
        
            &copy; Copyright The Pennsylvania State University, 2021. This material is not licensed for resale.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>