{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "764db743",
   "metadata": {},
   "source": [
    "# 2.4 Support vector machine\n",
    "There is a lot of work about SVM in literature , see\n",
    "[@drucker1997support; @ben2001support; @cortes1995support; @cristianini2000introduction]\n",
    "for example. Given a binary linearly separable classification dataset\n",
    "${(x_i,y_i)}_{i = 1}^N$, where\n",
    "$x_i\\in \\mathbb{R}^d, y_i\\in \\left \\{\\begin{pmatrix}1\\\\0\\end{pmatrix}, \\begin{pmatrix}0\\\\1\\end{pmatrix}\\right \\}$.\n",
    "We use $A_1,A_2$ to denote the data with label\n",
    "$\\begin{pmatrix}1\\\\0\\end{pmatrix}, \\begin{pmatrix}0\\\\1\\end{pmatrix}$,\n",
    "respectively. Our goal is to find a $\\theta = (w,b)$ where\n",
    "$w\\in \\mathbb{R}^{1\\times d}, b\\in \\mathbb{R}$ such that the hyperplane\n",
    "$H_{\\theta} = \\{x:wx + b = 0\\}$ can separate $A_1,A_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b482662",
   "metadata": {},
   "source": [
    "## 2.4.1 Binary SVM\n",
    "Binary Support Vector Machine (SVM for short hereinafter) wants to find\n",
    "the classifiable hyperplane which has the biggest distance with $A_1$\n",
    "and $A_2$. Assume that we have the hyperplanes $wx+b=\\pm 1$ with\n",
    "$$wx_i+b\\ge 1 \\quad \\mbox{for}\\quad x_i\\in A_1,\\quad wx_i+b\\le -1 \\quad \\mbox{for}\\quad x_i\\in A_2,$$\n",
    "which is similar to the definition\n",
    "[\\[2classH\\]](#2classH){reference-type=\"eqref\" reference=\"2classH\"}. Let\n",
    "$y_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ for $x_i\\in A_1$ and\n",
    "$y_2=\\begin{pmatrix}0\\\\1\\end{pmatrix}$ for $x_i\\in A_2$. Note that $w$\n",
    "is normal to the hyperplane and the distance between the points\n",
    "satisfying $$wx_i+b=\\pm 1$$ and the hyperplane $wx+b=0$ is\n",
    "$\\displaystyle {1\\over \\|w\\|_2}$. Thus, the width of the margin is\n",
    "$\\displaystyle {2\\over \\|w\\|_2}$ as shown in Figure\n",
    "[1](#fig:margin){reference-type=\"ref\" reference=\"fig:margin\"}.\n",
    "\n",
    "![SVM](margin){#fig:margin width=\"2in\"}\n",
    "\n",
    "For any $w$ and $b$, the smallest distance between points and the\n",
    "hyperplane is $$\\frac{\\min_{i} \\ell_i(wx_i+b)}{\\|w\\|_2},$$ where\n",
    "$\\ell_i=1-2e_2^Ty_i$. Note that $$\\ell_i=\\begin{cases}\n",
    "1 & \\mbox{ if } y_i=\\begin{pmatrix}1\\\\0\\end{pmatrix},\n",
    "\\\\\n",
    "-1 &\\mbox{ if } y_i=\\begin{pmatrix}0\\\\1\\end{pmatrix}.\n",
    "\\end{cases}$$ Consider the problem\n",
    "$$\\max_{w,b} \\frac{\\min_{i} \\ell_i(wx_i+b)}{\\|w\\|_2}.$$ Intuitively, the\n",
    "best separating hyperplane $H$ is only determined by those data points\n",
    "who are closest to $H$. Those data points are called support vector, and\n",
    "this method are called support vector machine.\n",
    "\n",
    "Without loss of generality, we may restrict the norm of $\\|w\\|$ to be 1,\n",
    "which leads to a equivalent optimization problem\n",
    "$$\\max_{\\|w\\|_2 = 1} \\min_{i} \\ell_i(wx_i+b)$$ Actually, we can prove\n",
    "$\\displaystyle \\mathop{\\rm argmax}_{\\|w\\|_2 = 1} \\min_{i} \\ell_i(wx_i+b)$\n",
    "is nonempty, but here we just admit this fact and only prove the\n",
    "uniqueness of the solution.\n",
    "\n",
    "::: lemma\n",
    "If $A_1,A_2$ are linearly separable, then $$\\label{binarySVM}\n",
    "    \\mathop{\\rm argmax}_{\\|w\\|_2 = 1} \\min_{i} \\ell_i(wx_i+b)$$ is\n",
    "nonempty.\n",
    ":::\n",
    "\n",
    "::: proof\n",
    "*Proof.* Take $x_{i_1} \\in A_1$ and $x_{i_2} \\in A_2$, given\n",
    "$(w,b)\\in \\{(w,b): l_i(wx_i +b)>0, \\forall i\\}$, we have $$\\begin{cases}\n",
    "        wx_{i_1} + b > 0,\\\\\n",
    "        wx_{i_2} + b < 0\n",
    "        \\end{cases}$$ which implies $|b| < \\max_{i} \\|x_i\\|_2$. So we\n",
    "have\n",
    "$$\\mathop{\\rm argmax}_{\\|w\\|_2 = 1} \\min_{i} \\ell_i(wx_i+b) = \\mathop{\\rm argmax}_{\\|w\\|_2 = 1, ~|b|\\leq \\max_{i} \\|x_i\\|_2} \\min_{i} \\ell_i(wx_i+b) \\neq \\emptyset.$$ ◻\n",
    ":::\n",
    "\n",
    "::: lemma\n",
    "If $A_1,A_2$ are linearly separable, then\n",
    "$$\\mathop{\\rm argmax}_{\\|w\\|_2 = 1} \\min_{i} \\ell_i(wx_i+b)$$ is a\n",
    "singleton set.\n",
    ":::\n",
    "\n",
    "::: proof\n",
    "*Proof.* Denote $\\displaystyle m(w,b) = \\min_{i} \\ell_i(wx_i+b)$. Notice\n",
    "that $m(w,b)$ is a concave homogeneous function w.r.t $w,b$ and\n",
    "$\\|\\cdot\\|_2$ is a strictly convex norm. Suppose there are two solution\n",
    "$(w_1,b_1)$ and $(w_2,b_2)$ such that $w_1 \\neq w_2$, take\n",
    "$\\overline{w} = \\frac{w_1 + w_2}{2}, \\overline{b} = \\frac{b_1 + b_2}{2}$,\n",
    "we must have\n",
    "$$m(\\overline{w},\\overline{b}) \\geq \\frac{m(w_1,b_1)+ m(w_2,b_2)}{2} = \\max_{\\|w\\|_2 = 1} m(w,b),$$\n",
    "and $$\\|\\overline{w}\\|_2 < 1.$$ So\n",
    "$$m(\\frac{\\overline{w}}{\\|\\overline{w}\\|_2},\\frac{\\overline{b}}{\\|\\overline{w}\\|_2}) = \\frac{m(\\overline{w},\\overline{b}) }{\\|\\overline{w}\\|_2} > \\max_{\\|w\\|_2 = 1} m(w,b),$$\n",
    "which leads to a contradiction. So all the solutions must have the same\n",
    "$w$, we denote it as $w^*$. Then if $(w^*,b^*)$ is a solution of problem\n",
    "([\\[binarySVM\\]](#binarySVM){reference-type=\"ref\"\n",
    "reference=\"binarySVM\"}), we must have\n",
    "$$b^* \\in \\mathop{\\rm argmax}_{b} m(w^*,b)$$ Actually,\n",
    "$$m(w^*,b) = \\min\\{b+\\min_{x\\in A_1} w^*x, -b +\\min_{x\\in A_2} (-w^*x)\\}.$$\n",
    "It is easy to observe that\n",
    "$\\displaystyle \\mathop{\\rm argmax}_{b} m(w^*,b)$ is a singleton set and\n",
    "$$b^* = \\frac{\\min_{x\\in A_2} (-w^*x) - \\min_{x\\in A_1} w^*x}{2}.$$ ◻\n",
    ":::\n",
    "\n",
    "Denote $$\\label{maxSVM}\n",
    "    \\theta^*_{SVM} = (w_{SVM}^*,b_{SVM}^*) = \\mathop{\\rm argmax}_{\\|w\\|_2 = 1} \\min_{i} \\ell_i(wx_i+b).$$\n",
    "\n",
    "::: theorem\n",
    "Let $\\theta^*_{SVM} = (w_{SVM}^*,b_{SVM}^*)$ be the solution of\n",
    "[\\[maxSVM\\]](#maxSVM){reference-type=\"eqref\" reference=\"maxSVM\"}. Then,\n",
    "$w_{SVM}^*$ must be a linear combination of $x_i^T, i = 1,2,\\cdots,N$.\n",
    ":::\n",
    "\n",
    "::: proof\n",
    "*Proof.* Denote $$S = {\\rm span} \\{x_i^T\\}_{i=1}^N.$$ Then we have\n",
    "$$\\mathbb{R}^{1\\times d} = S \\oplus^{\\perp} S^{\\perp}.$$ So $w_{SVM}^*$\n",
    "can be uniquely decomposed as $w_{SVM}^* = w^*_S + w^*_{S^{\\perp}}$\n",
    "where $w_S\\in S$ and $w^*_{S^{\\perp}}\\in S^{\\perp}$. We will prove that\n",
    "$w^*_{S^{\\perp}} = 0$. Suppose not, we have\n",
    "$$\\|w^*_S\\|_2 < \\|w_{SVM}^*\\|_2 = 1.$$ Notice that\n",
    "$$w_{SVM}^* x_i = w_S^* x_i,\\ \\forall i = 1,2,\\cdots,N.$$ Thus we have\n",
    "$$\\min_{i} \\ell_i(w_{SVM}^*x_i+b_{SVM}^*) = \\min_{i} \\ell_i(w_S^*x_i+b_{SVM}^*).$$\n",
    "So\n",
    "$$\\min_{i} \\ell_i(w_{SVM}^*x_i+b_{SVM}^*) < \\frac{\\min_{i} \\ell_i(w_S^*x_i+b_{SVM}^*)}{\\|w_S^*\\|} = \\min_{i} \\ell_i(\\frac{w^*_S}{\\|w_S^*\\|_2}x_i+\\frac{b_{SVM}^*}{\\|w^*_S}\\|_2),$$\n",
    "which leads to a contradiction to the definition of $\\theta_{SVM}^*$. ◻\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bccadb",
   "metadata": {},
   "source": [
    "## 2.4.2 Soft margin maximization and kernel methods\n",
    "We may rewrite the SVM problem as $$\\begin{aligned}\n",
    "    \\max_{w,b}&\\ {2\\over \\|w\\|},\\\\\n",
    "    s.t.&\\ \\ell_i(wx_i+b) \\geq 1,\\ \\forall i. \\end{aligned}$$ or\n",
    "equivalently, $$\\begin{aligned}\n",
    "{\\label{SVM_Quad}}\n",
    "    \\min_{w,b}&\\ \\|w\\|^2,\\\\\n",
    "    s.t.&\\ \\ell_i(wx_i+b) \\geq 1,\\ \\forall i. \\end{aligned}$$ Notice\n",
    "that the feasible domain of margin maximization is nonempty if and only\n",
    "if dataset is linearly separable. So when the data is linearly\n",
    "nonseparable, this method can't even get a classifier even though it may\n",
    "not be good. One way to handle this problem is to relax the constraint\n",
    "by adding relaxation variables.\n",
    "\n",
    "Define soft margin maximization problem $$\\begin{aligned}\n",
    "{\\label{SVM_Quad_soft}}\n",
    "\\min_{w,b,\\xi}&\\ \\|w\\|^2 + \\lambda^{-1} \\sum_{i = 1}^N\\xi_i,\\\\\n",
    "s.t.&\\ \\ell_i(wx_i+b) + \\xi_i \\geq 1,\\ \\forall i. \\\\\n",
    "     &\\ \\xi_i \\geq 0.\\end{aligned}$$ where $\\lambda>0$. The above\n",
    "problem is equivalent to $$\\begin{aligned}\n",
    " \\min_{w,b}&\\ \\|w\\|^2 + \\lambda^{-1} \\sum_{i = 1}^N {\\rm ReLU}(1-\\ell_i(wx_i+b)).\n",
    " \\end{aligned}$$ Thus, soft margin maximization problem\n",
    "[\\[SVM_Quad_soft\\]](#SVM_Quad_soft){reference-type=\"eqref\"\n",
    "reference=\"SVM_Quad_soft\"} can be reformulated as $$\\begin{aligned}\n",
    "{\\label{SVM_soft}}\n",
    "\\min_{w,b}&\\  \\sum_{i = 1}^N {\\rm ReLU}(1-\\ell_i(wx_i+b)) + \\lambda \\|w\\|^2.\\end{aligned}$$\n",
    "We can still prove that the solution of\n",
    "([\\[SVM_soft\\]](#SVM_soft){reference-type=\"ref\" reference=\"SVM_soft\"})\n",
    "satisfies the representation theorem. Thus we can restrict $w$ to be in\n",
    "the set $S$. Assume that $$w = \\sum_{i = 1}^N \\alpha_i x_i^T,$$ Denote\n",
    "$\\alpha = (\\alpha_1,\\cdots,\\alpha_N)^T$. We can rewrite the problem\n",
    "([\\[SVM_soft\\]](#SVM_soft){reference-type=\"ref\" reference=\"SVM_soft\"})\n",
    "as\n",
    "$$\\min_{\\alpha}\\ \\sum_{i = 1}^N {\\rm ReLU}(1-\\ell_i(\\sum_{j = 1}^N \\langle x_i,x_j\\rangle \\alpha_j+b)) + \\lambda\\alpha^T \\big(\\langle x_i,x_j\\rangle\\big)_{N\\times N} \\alpha\\\\$$\n",
    "We can see that the whole problem is only determined by the inner\n",
    "product of data points but not the data itself directly.\\\n",
    "Use the above formulation, we can induce nonlinearity in SVM. Denote the\n",
    "input space as $X$ where $\\{x_i\\}_{i=1}^N \\subset X$. We use two steps\n",
    "to obtain a nonlinear classification model. First, we use a nonlinear\n",
    "feature mapping $\\phi: X\\rightarrow \\mathcal{H}$ to map input space $X$\n",
    "to a feature space $\\mathcal{H}$. Second, we use linear SVM to do\n",
    "classification on $\\{\\phi(x_i)\\}_{i=1}^N\\subset \\mathcal{H}$.\\\n",
    "We may just asssume dataset after feature mapping $\\phi$ is linearly\n",
    "separable. Then, the SVM problem after doing feature mapping can be\n",
    "formulated as problem ([\\[SVM_soft\\]](#SVM_soft){reference-type=\"ref\"\n",
    "reference=\"SVM_soft\"}) as\n",
    "$$\\min_{\\alpha}\\ \\sum_{i = 1}^N {\\rm ReLU}(1-\\ell_i(\\sum_{j = 1}^N \\langle \\phi(x_i),\\phi(x_j)\\rangle \\alpha_j+b)) + \\lambda\\alpha^T \\big(\\langle \\phi(x_i),\\phi(x_j)\\rangle\\big)_{N\\times N} \\alpha\\\\$$\n",
    "\n",
    "Notice that to obtain the above problem we don't really need to know\n",
    "what exactly is the nonlinear mapping $\\phi$, but only need to compute\n",
    "the value of $<\\phi(x_i),\\phi(x_j)>$. So we define a kernel function\n",
    "$k: X\\times X\\rightarrow \\mathbb{R}$ such that\n",
    "$$k(x,y) = \\langle\\phi(x),\\phi(y)\\rangle,\\ x,y\\in X.$$ Then the kernel\n",
    "SVM can be formulated as\n",
    "$$\\min_{\\alpha}\\ \\sum_{i = 1}^N {\\rm ReLU}(1-\\ell_i(\\sum_{j = 1}^N  k(x_i,x_j) \\alpha_j+b)) + \\lambda\\alpha^T \\big(k(x_i,x_j)\\big)_{N\\times N} \\alpha\\\\$$\n",
    "In practice, we just need to find a proper kernel function instead of a\n",
    "good nonlinear feature mapping. Here we list some common used kernel\n",
    "functions:\n",
    "\n",
    "-   Polynomial kernel:\n",
    "    $k(x,y) = (a\\langle x,y\\rangle+ b)^n, a > 0, b\\geq 0, n\\in \\mathbb{N}^+$.\n",
    "\n",
    "-   Gaussian kernel: $k(x,y) = e^{-\\gamma\\|x-y\\|^2}, \\gamma > 0$.\n",
    "\n",
    "-   Laplacian kernel: $k(x,y) = e^{-\\gamma\\|x-y\\|}, \\gamma > 0$\n",
    "\n",
    "-   Tanh kernel: $k(x,y) = \\tanh(a\\langle x,y\\rangle+b), a>0, b\\geq 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912101de",
   "metadata": {},
   "source": [
    "## 2.4.3 Binary logistic regression\n",
    "\n",
    "In multi-class Logistic regression, if we use $\\|W\\|$ to replace\n",
    "$\\|\\bm\\theta\\|$ in regularization term, we can get another version of\n",
    "logistic regression:\n",
    "$$\\mathcal L_\\lambda(\\bm \\theta) = - \\sum_{i=1}^k \\sum_{x\\in A_i} \\log p_{i}(x;\\bm \\theta) + \\lambda R(\\|W\\|),$$\n",
    "where $p_i(x;\\bm \\theta)$ and $R(\\cdot)$ share the same definitions as\n",
    "in previous sections of logistic regression. Let\n",
    "$$\\bm\\Theta_{\\lambda} = \\mathop{{\\arg\\min}}_{\\bm\\theta}  \\mathcal L_\\lambda(\\bm\\theta).$$\n",
    "The following lemma follows directly from the definition of\n",
    "$p_{i}(x;\\bm \\theta)$.\n",
    "\n",
    "::: lemma\n",
    "For any\n",
    "$W\\in \\mathbb{R}^{k\\times d}, b\\in \\mathbb{R}^k, \\alpha \\in \\mathbb{R}$,\n",
    "we have\n",
    "$$\\mathcal L_\\lambda(W,b) = \\mathcal L_\\lambda(W,b + \\alpha \\bm 1),$$\n",
    "where $\\bm 1 = (1,1,\\cdots,1)^T\\in\\mathbb{R}^k.$\n",
    ":::\n",
    "\n",
    "Binary logistic regression refers to the case when $k=2$.\n",
    "\n",
    "::: lemma\n",
    "If $k = 2$, given any $\\bm\\theta_\\lambda = \\begin{pmatrix}\n",
    "    w_1 &  b_1\\\\\n",
    "    w_2 & b_2 \n",
    "    \\end{pmatrix} \\in \\bm\\Theta_\\lambda$, we have $$w_1 = -w_2.$$\n",
    ":::\n",
    "\n",
    "According to the above two lemmas, we can restrict $\\bm\\theta$ to have\n",
    "the form $$\\bm\\theta = \\begin{pmatrix}\n",
    "    \\frac{w}{2}\\ &\\frac{b}{2}\\\\\n",
    "    -\\frac{w}{2}\\ &-\\frac{b}{2}\n",
    "    \\end{pmatrix},$$ so our score mapping can be written as\n",
    "$$\\label{key}\n",
    "\\bm p(x;\\theta) = \\begin{pmatrix} \n",
    "    \\frac{1}{1+ e^{-(wx+b)}}\\\\\n",
    "    \\frac{1}{1+e^{wx+b}}\n",
    "\\end{pmatrix}.$$ Denote $\\theta = (w,b)$ where\n",
    "$w\\in \\mathbb{R}^d, b\\in \\mathbb{R}$, correspondingly, we have\n",
    "$$P(\\theta) = \\prod_{i = 1}^N \\frac{1}{1+ e^{-y_i(wx+b)}}.$$ Here we\n",
    "have the new \"label\" $y_i$ defined by $$\\label{key}\n",
    "y_i = \\begin{cases}\n",
    "1, \\quad &\\text{if}  \\quad x_i \\in A_1 \\\\\n",
    "-1, \\quad &\\text{if} \\quad x_i \\in A_2\n",
    "\\end{cases}.$$ Thus we have\n",
    "$$L(\\theta) = -\\log P(\\theta) = \\sum_{i = 1}^N \\log(1+ e^{-y_i(wx+b)}),$$\n",
    "and take $R(t) = t^2$, we have\n",
    "$$\\mathcal L_{\\lambda}(\\theta)  = L(\\theta) + \\lambda \\|w\\|_2^2 = \\sum_{i = 1}^N \\log(1+ e^{-y_i(wx+b)}) + \\lambda \\|w\\|_2^2.$$\n",
    "Here $L(\\theta)$ is a strictly convex function without any global\n",
    "minima.\n",
    "\n",
    "::: lemma\n",
    "Assume that $A_1,A_2$ are linearly separable, and we follow the same\n",
    "definition of linearly classifiable weights:\n",
    "$$\\bm{\\Theta} = \\left\\{\\bm\\theta: p_i(x; \\bm\\theta)>p_j(x; \\bm\\theta),~\\forall x\\in A_i, j\\neq i, i= 1,2\\right\\},$$\n",
    "where $$\\label{key}\n",
    "\\bm p(x;\\theta) = \\begin{pmatrix} \n",
    "\\frac{1}{1+ e^{-(wx+b)}}\\\\\n",
    "\\frac{1}{1+e^{wx+b}}\n",
    "\\end{pmatrix}.$$ Then, we have the following statement:\\\n",
    "\n",
    "1.  $\\theta = (w,b) \\in \\Theta$ if and only if\n",
    "    $$\\frac{1}{1+ e^{-y_i(wx+b)}} > \\frac{1}{2},\\ \\forall i = 1,2\\cdots,N.$$\n",
    "\n",
    "2.  If $P(\\theta) > \\frac{1}{2}$, then $\\theta$ must be classifiable,\n",
    "    i.e. $$\\{\\theta: P(\\theta)>\\frac{1}{2}\\}\\subset \\bm \\Theta.$$\n",
    "\n",
    "3.  Prove that\n",
    "    $$\\bm\\Theta = \\{\\theta: \\lim_{\\alpha\\rightarrow +\\infty} P(\\alpha \\theta) = 1\\}.$$\n",
    ":::\n",
    "\n",
    "If $A_1,A_2$ are linearly separable, then $\\displaystyle \n",
    "\\mathop{\\rm argmin}_{w,b} \\mathcal L_{\\lambda}(\\theta)$ is nonempty for\n",
    "$\\lambda$ sufficiently small.\n",
    "\n",
    "::: lemma\n",
    "If $A_1,A_2$ are linearly separable, then $$\\label{binaryLR}\n",
    "    \\mathop{\\rm argmin}_{w,b} \\mathcal L_{\\lambda}(\\theta)$$ is a\n",
    "singleton set for $\\lambda$ sufficiently small.\n",
    ":::\n",
    "\n",
    "::: proof\n",
    "*Proof.* Because $L(\\theta)$ is strictly convex w.r.t. $\\theta$ and\n",
    "$\\|w\\|^2$ is convex w.r.t. $\\theta$, so\n",
    "$\\mathcal L(\\theta,\\lambda)  = L(\\theta) + \\lambda \\|w\\|_2^2$ is stricly\n",
    "convex w.r.t. $\\theta$, which implies our result directly. ◻\n",
    ":::\n",
    "\n",
    "For $\\lambda$ sufficiently small, denote\n",
    "$$\\theta_{LR}(\\lambda) = (w_{LR}(\\lambda),b_{LR}(\\lambda)) = \\mathop{\\rm argmin}_{w,b} \\mathcal L_{\\lambda}(\\theta).$$\n",
    "\n",
    "::: lemma\n",
    "If $A_1,A_2$ are linearly separable,\n",
    "\n",
    "1.  $\\mathcal L_{\\lambda}(\\theta) \\rightarrow 0$ as\n",
    "    $\\lambda \\rightarrow 0$.\n",
    "\n",
    "2.  $\\|w_{LR}(\\lambda)\\|\\rightarrow \\infty$ as $\\lambda \\rightarrow 0$.\n",
    "\n",
    "3.  $\\displaystyle \\min_{i} y_i(w_{LR}(\\lambda) x_i + b_{LR}(\\lambda))\\rightarrow \\infty$\n",
    "    as $\\lambda \\rightarrow 0$.\n",
    "\n",
    "4.  If $A_1,A_2$ are also nonempty,\n",
    "    $$\\|b_{LR}(\\lambda)\\|\\leq \\max_{i} \\|x_i\\| \\|w_{LR}(\\lambda)\\|$$ for\n",
    "    $\\lambda$ sufficiently small.\n",
    ":::\n",
    "\n",
    "The above lemma implies that $\\theta_{LR}(\\lambda)/\\|w_{LR}(\\lambda)\\|$\n",
    "is bounded for $\\lambda$ sufficiently small.\n",
    "\n",
    "::: theorem\n",
    "If $A_1,A_2$ are linearly separable, then\n",
    "$\\frac{\\theta_{LR}(\\lambda)}{\\|w_{LR}(\\lambda)\\|}$ converge to\n",
    "$\\theta^*_{SVM}$ as $\\lambda \\rightarrow 0$, i.e.\n",
    "$$\\theta^*_{SVM} = \\lim_{\\lambda\\rightarrow 0} \\frac{\\theta_{LR}(\\lambda)}{\\|w_{LR}(\\lambda)\\|}.$$\n",
    ":::\n",
    "\n",
    "::: proof\n",
    "*Proof.* Because $\\frac{\\theta_{LR}(\\lambda)}{\\|w_{LR}(\\lambda)\\|}$ is\n",
    "bounded for $\\lambda$ sufficiently small, we only need to prove that it\n",
    "has no convergence points other than $\\theta^*_{SVM}$ as\n",
    "$\\lambda \\rightarrow 0$.\\\n",
    "We first introduce a soft margin function\n",
    "$m: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ such that\n",
    "$$m(\\theta) = \\min_{i} y_i(wx_i + b).$$ Suppose that there exist a\n",
    "sequence $\\{\\lambda_n\\}_{n = 1}^\\infty$ and\n",
    "$\\bar{\\theta} = (\\bar{w},\\bar{b}) \\neq \\theta^*_{SVM}$ so that\n",
    "$\\lambda_n \\downarrow 0$ and\n",
    "$\\frac{\\theta_{LR}(\\lambda_n)}{\\|w_{LR}(\\lambda_n)\\|} \\rightarrow \\bar{\\theta}$\n",
    "as $n\\rightarrow \\infty$. Obviously $\\|\\bar{w}\\| = 1$, and\n",
    "$$0 \\leq m(\\bar{\\theta}) < m(\\theta^*_{SVM}).$$ Take a positive number\n",
    "$\\epsilon < m(\\theta^*_{SVM}) - m(\\bar{\\theta})$. Then there exists\n",
    "$K_0\\in \\mathbb{N}^+$ such that\n",
    "$$0 < m(\\frac{\\theta_{LR}(\\lambda_n)}{\\|w_{LR}(\\lambda_n)\\|}) <  m(\\theta^*_{SVM})- \\epsilon,$$\n",
    "for all $n \\geq K_0$.\\\n",
    "Consider $\\theta_n = (w_n,b_n) = \\|w_{LR}(\\lambda_n)\\|\\theta^*_{SVM}$.\n",
    "Then\n",
    "$$\\lim_{n\\rightarrow \\infty} \\frac{\\log(1 + e^{-m(\\theta_n)})}{\\log(1 + e^{-m(\\theta_{LR}(\\lambda_n))}} = \\lim_{n \\rightarrow \\infty} e^{\\|w_{LR}(\\lambda_n)\\| (m(\\frac{\\theta_{LR}(\\lambda_n)}{\\|w_{LR}(\\lambda_n)\\|}) -  m(\\theta^*_{SVM}))} \\leq \\lim_{n \\rightarrow \\infty} e^{-\\|w_{LR}(\\lambda_n)\\| \\epsilon} = 0.$$\n",
    "So there exists a $K_1 \\in \\mathbb{N}^+$ such that\n",
    "$$N \\log(1 + e^{-m(\\theta_n)}) < \\log(1 + e^{-m(\\theta_{LR}(\\lambda_n))})$$\n",
    "for all $n \\geq K_1$. Take $K  = \\max\\{K_0, K _1\\}$. Then for all\n",
    "$n \\geq K$, we have\n",
    "$$\\mathcal{L}(\\theta_n) \\leq N \\log(1 + e^{-m(\\theta_n)}) + \\lambda \\|w_{\\lambda_n}\\|^2 < \\log(1 + e^{-m(\\theta_{LR}(\\lambda_n))})+ \\lambda \\|w_{\\lambda_n}\\|^2 \\leq \\mathcal{L}(\\theta_{LR}(\\lambda_n)),$$\n",
    "which contradicts the definition of $\\theta_{LR}(\\lambda)$. ◻\n",
    ":::\n",
    "\n",
    "For the proof of the above theorem, you can also refer to the paper by\n",
    "Rosset Saharon, Zhu Ji and Trevor J. Hastie  [@rosset2004margin].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59453b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
