{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Stochastic gradient descent method and convergence theory\n",
    "\n",
    "The next optimization problem is the most common case in machine\n",
    "learning.\n",
    "\n",
    "```{admonition} Problem\n",
    ":label: SGDproblem\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in \\mathbb{R}^n} f(x)\\quad \\mbox{and}\\quad f(x) = \\frac{1}{N} \\sum_{i=1}^N f_i(x).\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "One version of stochastic gradient descent (SGD) algorithm is:\n",
    "\n",
    "```{prf:algorithm} SGD\n",
    "**Input**: initialization $x_0$, learning rate $\\eta_t$.\n",
    "\n",
    "**For**: t = 0,1,2,$\\dots$\n",
    "\n",
    "Randomly pick $i_t \\in \\{1, 2, \\cdots, N\\}$ independently with\n",
    "probability $\\frac{1}{N}$ \n",
    "\n",
    "$$\n",
    "    x_{t+1} = x_{t} - \\eta_t \\nabla f_{i_t}(x_t).\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "## 4.2.1 Convergence of SGD\n",
    "\n",
    "```{prf:theorem} Theorem\n",
    "Assume that each $f_i(x)$ is $\\lambda$-strongly convex and\n",
    "$\\|\\nabla f_i(x)\\| \\le M$ for some $M >0$. If we take\n",
    "$\\eta_t = \\frac{a}{\\lambda (t+1)}$ with sufficiently large $a$ such that\n",
    "\n",
    "$$\n",
    "    \\|x_0 - x^*\\|^2 \\le \\frac{a^2M^2}{(a-1)\\lambda^2}\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}e_{t}^2 \\le \\frac{a^2M^2}{(a-1)\\lambda^2 (t+1)}, \\quad  t\\ge 1,\n",
    "$$\n",
    "\n",
    "where $e_t = \\|x_t - x^*\\|$.\n",
    "```\n",
    "\n",
    "```{prf:proof} Proof\n",
    "*Proof.* The $L^2$ error of SGD can be written as \n",
    "\n",
    "$$\n",
    "      \\begin{split}\n",
    "            \\mathbb{E} \\|x_{t+1} - x^*\\|^2 &\\le \\mathbb{E}\\| x_{t} - \\eta_t \\nabla f_{i_t}(x_t) - x^* \\|^2 \\\\\n",
    "            &\\le \\mathbb{E} \\|x_t - x^*\\|^2 \n",
    "            - 2 \\eta_t \\mathbb{E} (\\nabla f_{i_t}(x_t) \\cdot (x_t - x^*)) \n",
    "            + \\eta_t^2 \\mathbb{E} \\|\\nabla f_{i_t}(x_t)\\|^2 \\\\\n",
    "            & \\le \\mathbb{E} \\|x_t - x^*\\|^2 - 2 \\eta_t \\mathbb{E} (\\nabla f (x_t) \\cdot (x_t - x^*))\n",
    "            + \\eta_t^2 M^2 \\\\\n",
    "            & \\le \\mathbb{E} \\|x_t - x^*\\|^2 -  \\eta_t \\lambda \\mathbb{E} \\|x_t - x^*\\|^2 + \\eta_t^2 M^2 \\\\\n",
    "            & = (1 - \\eta_t\\lambda) \\mathbb{E} \\|x_t - x^*\\|^2 + \\eta_t^2 M^2\n",
    "      \\end{split}\n",
    "$$\n",
    "\n",
    "The third line comes from the fact that\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    \\mathbb{E} (\\nabla f_{i_t}(x_t) \\cdot (x_t - x^*))  &= \\mathbb{E}_{i_1i_2\\cdots i_t} (\\nabla f_{i_t}(x_t) \\cdot (x_t - x^*)) \\\\\n",
    "&= \\mathbb{E}_{i_1i_2\\cdots i_{t-1}} \\frac{1}{N} \\sum_{i=1}^N \\nabla f_i(x_t)\\cdot (x_t - x^*) \\\\\n",
    "&= \\mathbb{E}_{i_1i_2\\cdots i_{t-1}}  \\nabla f(x_t)\\cdot (x_t - x^*) \\\\\n",
    "&= \\mathbb{E}\\nabla f(x_t)\\cdot (x_t - x^*),\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    \\mathbb{E} \\|\\nabla f_{i_t}(x_t)\\|^2 \\le \\mathbb{E} M^2 = M^2.\n",
    "$$\n",
    "\n",
    "Note when $t=0$, we have \n",
    "\n",
    "$$\n",
    "    \\mathbb{E} e_0^2 = \\|x_0 - x^*\\|^2 \\le \\frac{a^2M^2}{(a-1)\\lambda},\n",
    "$$\n",
    "\n",
    "based on the assumption.\n",
    "\n",
    "In the case of SDG, by the inductive hypothesis, \n",
    "\n",
    "$$\n",
    "    \\begin{split}\n",
    "            \\mathbb{E}e_{t+1}^2 & \\le (1 - \\eta_t\\lambda)\\mathbb{E}e_{t}^2  + \\eta_t^2 M^2\\\\\n",
    "            &\\le  (1 - \\frac{a}{t+1}) \\frac{a^2M^2}{(a-1)\\lambda^2 (t+1)} + \\frac{a^2M^2}{\\lambda^2 (t+1)^2} \\\\\n",
    "            & \\le \\frac{a^2M^2}{(a-1)\\lambda^2} \\frac{1}{(t+1)^2}(t+1 -a + a-1) \\\\\n",
    "            & = \\frac{a^2M^2}{(a-1)\\lambda^2} \\frac{t}{(t+1)^2} \\\\\n",
    "            & \\le \\frac{a^2M^2}{(a-1)\\lambda^2(t+2)}. \\quad \\left(\\frac{t}{(t+1)^2} \\le \\frac{1}{t+2}\\right),\n",
    "      \\end{split}\n",
    "$$\n",
    "\n",
    "which completes the proof. ◻\n",
    "```\n",
    "\n",
    "## 4.2.2 SGD with mini-batch\n",
    "\n",
    "Firstly, we will introduce a natural extended version of the SGD\n",
    "discussed above with introducing mini-batch.\n",
    "\n",
    "```{prf:algorithm} SGD with mini-batch\n",
    "**Input**: initialization $x_0$, learning rate $\\eta_t$.\n",
    "\n",
    "**For**: t = 0,1,2,$\\dots$\n",
    "\n",
    "\n",
    "Randomly pick $B_t \\subset \\{1, 2, \\cdots, N\\}$ independently with\n",
    "probability $\\frac{m!(N-m)!}{N!}$\\\n",
    "and $\\# B_t = m$.\n",
    "\n",
    "$$\n",
    "    x_{t+1} = x_{t} - \\eta_t g_t(x_t).\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    g_{t}(x_t) = \\frac{1}{m} \\sum_{i \\in B_{t}}  \\nabla f_i(x_t)\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "Now we introduce the SGD algorithm with mini-batch without replacement\n",
    "which is the most commonly used version of SGD in machine learning.\n",
    "\n",
    "```{prf:algorithm} Shuffle SGD with mini-batch\n",
    "**Input**: learning rate $\\eta_k$, mini-batch size $m$, parameter\n",
    "initialization $x_{0}$ and denote $M = \\lceil \\frac{N}{m} \\rceil$.\n",
    "\n",
    "**For** Epoch $k = 1,2,\\dots$\n",
    "\n",
    "\n",
    "Randomly pick $B_t \\subset \\{1, 2, \\cdots, N \\}$ without replacement\\\n",
    "with $\\# B_t = m$ for $t = 1,2,\\cdots,M$.\n",
    "\n",
    "\n",
    "mini-batch $t = 1:M$\n",
    "\n",
    "Compute the gradient on $B_{t}$:\n",
    "    \n",
    "$$\n",
    "    g_{t}(x) = \\frac{1}{m} \\sum_{i \\in B_{t}}  \\nabla f_i(x)\n",
    "$$\n",
    "\n",
    "Update $x$:\n",
    "\n",
    "$$\n",
    "    x  \\leftarrow  x - \\eta_k g_t(x),\n",
    "$$\n",
    "\n",
    "**EndFor**\n",
    "```\n",
    "\n",
    "To \\\"randomly pick $B_i \\subset \\{1, 2, \\cdots, N \\}$ without\n",
    "replacement with $\\# B_i = m$ for $i = 1,2,\\cdots,t$\", we usually just\n",
    "randomly shuffle the index set first and then consecutively pick every\n",
    "$m$ elements in the shuffled index set. That is the reason why we would\n",
    "like to call the algorithm as shuffled SGD while this is the mostly used\n",
    "version of SGD in machine learning.\n",
    "\n",
    "```{admonition} Remark\n",
    "Let us recall a general machine learning loss function \n",
    "\n",
    "$$\n",
    "    \\label{key}\n",
    "    L(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\ell(h(X_i; \\theta), Y_i),\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\{(X_i, Y_i)\\}_{i=1}^N$ correspond to these data pairs. For example,\n",
    "$\\ell(\\cdot, \\cdot)$ takes cross-entropy and\n",
    "$h(x; \\theta) = p(x;\\theta)$ as we discussed in Section 2.2.1. Thus, we\n",
    "have the following corresponding relation\n",
    "\n",
    "$$\n",
    "    f(x) \\leftrightsquigarrow L(\\theta), \\quad\n",
    "    f_i(x) \\leftrightsquigarrow \\ell(h(X_i; \\theta), Y_i).\n",
    "$$\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
