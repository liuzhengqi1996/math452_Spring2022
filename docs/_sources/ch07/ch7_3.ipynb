{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Definition of deep neural networks (DNN)\n",
    "\n",
    "In this section, we will give a brief introduction to a special function\n",
    "class related to deep neural networks (DNN) used in machine learning. We\n",
    "then explore the relationship between DNN (with ReLU as activation\n",
    "function) and linear finite element methods.\n",
    "\n",
    "Given $n, m\\ge 1$, the first ingredient in defining a deep neural\n",
    "network (DNN) is (vector) linear functions of the form\n",
    "$$\\label{thetamap1}\n",
    "\\theta:\\mathbb{R}^{n}\\to\\mathbb{R}^{m} ,$$as $\\theta(x)=Wx+b$ where\n",
    "$W=(w_{ij})\\in\\mathbb{R}^{m\\times n}$, $b\\in\\mathbb{R}^{m}$. The second\n",
    "main ingredient is a nonlinear activation function, usually denoted as\n",
    "$$\\label{sigma}\n",
    "\\sigma: \\mathbb{R} \\to \\mathbb{R}.$$ By applying the function to each\n",
    "component, we can extend this naturally to\n",
    "$$\\sigma:\\mathbb R^{n}\\mapsto \\mathbb R^{n}.$$\n",
    "\n",
    "## 7.3.1 Definition of neurons\n",
    "\n",
    "1.  Primary variables $n_0=d$ $$x^0=x=\n",
    "        \\begin{pmatrix}\n",
    "        x_1\\\\\n",
    "        x_2\\\\\n",
    "        \\vdots \\\\  \n",
    "        x_{d}\n",
    "        \\end{pmatrix}$$\n",
    "\n",
    "2.  $n_1$ hyperplanes $\\theta^{0}(x^0) = W^0 x + b^0$ where\n",
    "    $W^0: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{n_1}$: $$W^0x+b^0=\n",
    "        \\begin{pmatrix}\n",
    "        w^0_1x+b^0_1\\\\\n",
    "        w^0_2x+b^0_2\\\\\n",
    "        \\vdots \\\\  \n",
    "        w^0_{n_1}x+b^0_{n_1}\n",
    "        \\end{pmatrix}\\quad \\mbox{with }\\quad W^0=\n",
    "        \\begin{pmatrix}\n",
    "        w^0_1\\\\\n",
    "        w^0_2\\\\\n",
    "        \\vdots \\\\  \n",
    "        w^0_{n_1}\n",
    "        \\end{pmatrix},\\quad b^0=\n",
    "        \\begin{pmatrix}\n",
    "        b^0_1\\\\\n",
    "        b^0_2\\\\\n",
    "        \\vdots \\\\  \n",
    "        b^0_{n_1}\n",
    "        \\end{pmatrix}$$\n",
    "\n",
    "3.  $n_1$-neurons: $$x^1=\\sigma(W^0x+b^0)\n",
    "        =\\begin{pmatrix}\n",
    "        \\sigma(w^0_1x+b^0_1)\\\\\n",
    "        \\sigma(w^0_2x+b^0_2)\\\\\n",
    "        \\vdots \\\\  \n",
    "        \\sigma(w^0_{n_1}x+b^0_{n_1})\n",
    "        \\end{pmatrix}$$\n",
    "\n",
    "4.  $n_2$-hyperplanes $\\theta^{1}(x^1) = W^1 x + b^1$ where\n",
    "    $W^1: \\mathbb{R}^{n_1} \\mapsto \\mathbb{R}^{n_2}$: $$W^1x^1+b^1=\n",
    "        \\begin{pmatrix}\n",
    "        w^1_1x^1+b^1_1\\\\\n",
    "        w^1_2x^1+b^1_2\\\\\n",
    "        \\vdots \\\\  \n",
    "        w^1_{n_2}x^1+b^1_{n_2}\n",
    "        \\end{pmatrix}\\quad \\mbox{with }\\quad \n",
    "        W^1=\n",
    "        \\begin{pmatrix}\n",
    "        w^1_1 \\\\\n",
    "        w^1_2 \\\\\n",
    "        \\vdots \\\\  \n",
    "        w^1_{n_2} \n",
    "        \\end{pmatrix},\\ \n",
    "        b^1=\n",
    "        \\begin{pmatrix}\n",
    "        b^1_1\\\\\n",
    "        b^1_2\\\\\n",
    "        \\vdots \\\\  \n",
    "        b^1_{n_2}\n",
    "        \\end{pmatrix}$$\n",
    "\n",
    "5.  $n_2$-neurons: $$x^2=\\sigma(W^1x+b^1)\n",
    "        =\\begin{pmatrix}\n",
    "        \\sigma(w^1_1x+b^1_1)\\\\\n",
    "        \\sigma(w^1_2x+b^1_2)\\\\\n",
    "        \\vdots \\\\  \n",
    "        \\sigma(w^1_{n_2}x+b^1_{n_2})\n",
    "        \\end{pmatrix}$$\n",
    "\n",
    "6.  $\\cdots$\n",
    "\n",
    "## 7.3.2 Definition of deep neural network functions {#sec:DNN}\n",
    "\n",
    "Given $d, k\\in\\mathbb{N}^+$ and\n",
    "$$n_1,\\dots,n_{k}\\in\\mathbb{N} \\mbox{ with }n_0=d, n_{k+1}=1,$$ a\n",
    "general DNN function from $\\mathbb{R}^d$ to $\\mathbb{R}$ is given by\n",
    "$$\\begin{aligned}\n",
    "f^0(x)   &=\\theta^0(x) \\\\ \n",
    "f^{\\ell}(x) &= [  \\theta^{\\ell} \\circ \\sigma ](f^{\\ell-1}(x)) \\quad \\ell = 1:k \\\\\n",
    "f(x) &= f^k(x). \\end{aligned}$$ The following more concise notation is\n",
    "often used in computer science literature: $$\\label{compress-dnn}\n",
    "f(x) = \\theta^{k}\\circ \\sigma \\circ \\theta^{k-1} \\circ \\sigma \\cdots \\circ \\theta^1 \\circ \\sigma \\circ \\theta^0(x),$$\n",
    "here $\\theta^i: \\mathbb{R}^{n_{i}}\\to\\mathbb{R}^{n_{i+1}}$ are linear\n",
    "functions as defined in\n",
    "[\\[thetamap1\\]](#thetamap1){reference-type=\"eqref\"\n",
    "reference=\"thetamap1\"}. Such a DNN is called a $(k+1)$-layer DNN, and is\n",
    "said to have $k$-hidden layers. The size of this DNN is\n",
    "$n_1+\\cdots+n_k$.\n",
    "\n",
    "Thus, we have the following connection of neurons and DNN functions\n",
    "$$f^k(x) = \\theta^{k}(x^k) = \\theta^{k} \\circ \\sigma \\circ \\theta^{k-1}(x^{k-1}) = [\\theta^{k} \\circ \\sigma ] (f^{k-1}),$$\n",
    "or we can see that\n",
    "$$x^k = \\sigma(f^{k-1}) = \\sigma \\circ \\theta^{k-1} \\circ \\sigma (f^{k-2}) = [\\sigma \\circ \\theta^{k-1}] (x^{k-1}).$$\n",
    "Based on these notation and connections, we have the following\n",
    "definition of general artificial neural network functions.\n",
    "\n",
    "Shallow (one hidden layer) neural network functions: $$\\label{NN1}\n",
    "\\dnn(\\sigma; n_1) \n",
    "=\\bigg\\{ f^1(x) = \\theta^1 (x^1), \\mbox{ with } W^\\ell\\in \\mathbb R^{n_{\\ell+1}\\times\n",
    "    n_{\\ell}}, b^\\ell\\in\\mathbb R^{n_\\ell}, \\ell=0, 1, n_0=d, n_2 = 1\\bigg\\}$$\n",
    "Deep neural network functions: $$\\label{NNL}\n",
    "\\dnn(\\sigma; n_1,n_2,\\ldots, n_L)=\\bigg\\{ f^{L}(x) = \\theta^L (x^{L}), \n",
    " \\mbox{ with } W^\\ell\\in \\mathbb R^{n_{\\ell+1}\\times\n",
    "    n_{\\ell}}, b^\\ell\\in\\mathbb R^{n_\\ell}, \\ell=0:L, n_0=d, n_{L+1}=1\\bigg\\}$$\n",
    "If we ignore the width (number of neurons) of network functions, we may\n",
    "denote the general deep neural network functions with certain layers.\n",
    "\n",
    "The 1-hidden layer (shallow) neural network is defined as:\n",
    "$$\\dnn=\\dnn(\\sigma) = \\dnn^1(\\sigma)\n",
    "=\\bigcup_{n_1\\ge 1} \\dnn(\\sigma;n_1,1)$$ Generally, we can define the\n",
    "L-hidden layer neural network as:\n",
    "$$\\dnn^L(\\sigma) := \\bigcup_{n_1, n_2, \\cdots, n_{L}\\ge 1} \\dnn(\\sigma;n_1,n_2,\\cdots,n_L, 1).$$\n",
    "\n",
    "## 7.3.3 ReLU DNN\n",
    "\n",
    "In this section, we mainly consider a special activation function, known\n",
    "as the *rectified linear unit* (ReLU), and defined as $\\rm\n",
    "ReLU: \\mathbb R\\mapsto \\mathbb R$, $$\\label{relu}\n",
    " {\\rm ReLU}(x):=\\max(0,x), \\quad x\\in\\mathbb{R}.$$ A ReLU DNN with $k$\n",
    "hidden layers might be written as: $$\\label{relu-dnn}\n",
    "f(x) = \\theta^{k}\\circ {\\rm ReLU} \\circ \\theta^{k-1} \\circ {\\rm ReLU} \\cdots \\circ \\theta^1 \\circ {\\rm ReLU} \\circ \\theta^0(x).$$\n",
    "\n",
    "We note that $\\rm ReLU$ is a continuous piecewise linear (CPWL)\n",
    "function. Since the composition of two CPWL functions is still a CPWL\n",
    "function, we have the following observation [@arora2016understanding].\n",
    "\n",
    "::: {.lemma}\n",
    "[\\[dnn-cpwl\\]]{#dnn-cpwl label=\"dnn-cpwl\"} Every ReLU DNN:\n",
    "$\\mathbb{R}^d\\to\\mathbb{R}^c$ is a continuous piecewise linear function.\n",
    "More specifically, given any ReLU DNN, there is a polyhedral\n",
    "decomposition of $\\mathbb R^d$ such that this ReLU DNN is linear on each\n",
    "polyhedron in such a decomposition.\n",
    ":::\n",
    "\n",
    "Here is a simple example for the \"grid\\\" created by some 2-layer ReLU\n",
    "DNNs in $\\mathbb{R}^2$.\n",
    "\n",
    "![Projections of the domain partitions formed by 2-layer ReLU DNNs with\n",
    "sizes\n",
    "$(n_0, n_1, n_2, n_3)= (2, 5, 5, 1), (2, 10, 10, 1) \\text{and}\\ (2, 20, 20, 1)$\n",
    "with random\n",
    "parameters.](figures/2to5to5to1-eps-converted-to.pdf \"fig:\"){#fig:dnn-region\n",
    "width=\".3\\\\textwidth\"} ![Projections of the domain partitions formed by\n",
    "2-layer ReLU DNNs with sizes\n",
    "$(n_0, n_1, n_2, n_3)= (2, 5, 5, 1), (2, 10, 10, 1) \\text{and}\\ (2, 20, 20, 1)$\n",
    "with random\n",
    "parameters.](figures/2to10to10to1-eps-converted-to.pdf \"fig:\"){#fig:dnn-region\n",
    "width=\".3\\\\textwidth\"} ![Projections of the domain partitions formed by\n",
    "2-layer ReLU DNNs with sizes\n",
    "$(n_0, n_1, n_2, n_3)= (2, 5, 5, 1), (2, 10, 10, 1) \\text{and}\\ (2, 20, 20, 1)$\n",
    "with random\n",
    "parameters.](figures/2to20to20to1-eps-converted-to.pdf \"fig:\"){#fig:dnn-region\n",
    "width=\".3\\\\textwidth\"}\n",
    "\n",
    "For convenience of exposition, we introduce the following notation:\n",
    "Namely $\\dnn^L({\\sigma})$ represents the DNN model with $L$ hidden\n",
    "layers and ReLU activation function with arbitrary size, if\n",
    "$\\sigma = {\\rm ReLU}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14f614",
   "metadata": {},
   "source": [
    "## 7.3.4 Fourier transform of polynomials\n",
    "\n",
    "We begin by noting that an activation function $\\sigma$, which satisfies\n",
    "a polynomial growth condition $|\\sigma(x)| \\leq C(1 + |x|)^n$ for some\n",
    "constants $C$ and $n$, is a tempered distribution. As a result, we make\n",
    "this assumption on our activation functions in the following theorems.\n",
    "We briefly note that this condition is sufficient, but not necessary\n",
    "(for instance an integrable function need not satisfy a pointwise\n",
    "polynomial growth bound) for $\\sigma$ to be represent a tempered\n",
    "distribution.\n",
    "\n",
    "We begin by studying the convolution of $\\sigma$ with a Gaussian\n",
    "mollifier. Let $\\eta$ be a Gaussian mollifier\n",
    "$$\\eta(x) = \\frac{1}{\\sqrt{\\pi}}e^{-x^2}.$$ Set\n",
    "$\\eta_\\epsilon=\\frac{1}{\\epsilon}\\eta(\\frac{x}{\\epsilon})$. Then\n",
    "consider $$\\label{sigma-epsilon}\n",
    "\\sigma_{\\epsilon}(x):=\\sigma\\ast{\\eta_\\epsilon}(x)=\\int_{\\mathbb{R}}\\sigma(x-y){\\eta_\\epsilon}(y)dy$$\n",
    "for a given activation function $\\sigma$. It is clear that\n",
    "$\\sigma_{\\epsilon}\\in C^\\infty(\\mathbb{R})$. Moreover, by considering\n",
    "the Fourier transform (as a tempered distribution) we see that\n",
    "$$\\label{eq_278}\n",
    " \\hat{\\sigma}_{\\epsilon} = \\hat{\\sigma}\\hat{\\eta}_{\\epsilon} = \\hat{\\sigma}\\eta_{\\epsilon^{-1}}.$$\n",
    "\n",
    "We begin by stating a lemma which characterizes the set of polynomials\n",
    "in terms of their Fourier transform.\n",
    "\n",
    "::: {.lemma}\n",
    "[\\[polynomial_lemma\\]]{#polynomial_lemma label=\"polynomial_lemma\"} Given\n",
    "a tempered distribution $\\sigma$, the following statements are\n",
    "equivalent:\n",
    "\n",
    "1.  $\\sigma$ is a polynomial\n",
    "\n",
    "2.  $\\sigma_\\epsilon$ given by\n",
    "    [\\[sigma-epsilon\\]](#sigma-epsilon){reference-type=\"eqref\"\n",
    "    reference=\"sigma-epsilon\"} is a polynomial for any $\\epsilon>0$.\n",
    "\n",
    "3.  $\\text{\\normalfont supp}(\\hat{\\sigma})\\subset \\{0\\}$.\n",
    ":::\n",
    "\n",
    "::: {.proof}\n",
    "*Proof.* We begin by proving that (3) and (1) are equivalent. This\n",
    "follows from a characterization of distributions supported at a single\n",
    "point (see [@strichartz2003guide], section 6.3). In particular, a\n",
    "distribution supported at $0$ must be a finite linear combination of\n",
    "Dirac masses and their derivatives. In particular, if $\\hat{\\sigma}$ is\n",
    "supported at $0$, then\n",
    "$$\\hat{\\sigma} = \\displaystyle\\sum_{i=1}^n a_i\\delta^{(i)}.$$ Taking the\n",
    "inverse Fourier transform and noting that the inverse Fourier transform\n",
    "of $\\delta^{(i)}$ is $c_ix^i$, we see that $\\sigma$ is a polynomial.\n",
    "This shows that (3) implies (1), for the converse we simply take the\n",
    "Fourier transform of a polynomial and note that it is a finite linear\n",
    "combination of Dirac masses and their derivatives.\n",
    "\n",
    "Finally, we prove the equivalence of (2) and (3). For this it suffices\n",
    "to show that $\\hat{\\sigma}$ is supported at $0$ iff\n",
    "$\\hat{\\sigma}_\\epsilon$ is supported at $0$. This follows from equation\n",
    "[\\[eq_278\\]](#eq_278){reference-type=\"ref\" reference=\"eq_278\"} and the\n",
    "fact that $\\eta_{\\epsilon^{-1}}$ is nowhere vanishing. ◻\n",
    ":::\n",
    "\n",
    "As an application of Lemma\n",
    "[\\[polynomial_lemma\\]](#polynomial_lemma){reference-type=\"ref\"\n",
    "reference=\"polynomial_lemma\"}, we give a simple proof of the result in\n",
    "the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f5d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
