{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab062bd7",
   "metadata": {},
   "source": [
    "# 7.1 Motivation: from finite element to neural network {#FE2NN}\n",
    "\n",
    "In this section, we will introduce the so-called shallow neural network\n",
    "(deep neural network with one hidden layer) from the viewpoint of finite\n",
    "element method.\n",
    "\n",
    "Let us recall the linear finite element functions on the unit interval\n",
    "$\\bar{\\Omega}=[0,1]$ in Section\n",
    "[\\[linearFE\\]](#linearFE){reference-type=\"ref\" reference=\"linearFE\"}.\n",
    "Consider a set of equidistant girds $\\mathcal T_\\ell$ of level $\\ell$\n",
    "and mesh length $h_\\ell = 2^{-\\ell}$. The grid points $x_{\\ell,i}$ are\n",
    "given by $$x_{\\ell,i}:=ih_\\ell,\\quad 0\\le i\\le 2^\\ell.$$ For $\\ell=1$,\n",
    "we denote the special hat function by $\\varphi(x)$ and any nodal basis\n",
    "function in\n",
    "[\\[1dbasis:function\\]](#1dbasis:function){reference-type=\"eqref\"\n",
    "reference=\"1dbasis:function\"} on grid $\\mathcal T_\\ell$ by\n",
    "$\\varphi_{\\ell,i}$ as below $$\\label{def_g}\n",
    "\\varphi(x) = \n",
    "\\begin{cases}\n",
    "2x \\quad &x\\in [0,\\frac{1}{2}] \\\\\n",
    "2(1-x) \\quad &x\\in [\\frac{1}{2}, 1] \\\\\n",
    "0, \\quad &\\text{others} \n",
    "\\end{cases},\\qquad\n",
    "\\varphi_{\\ell,i} = \\varphi(\\frac{x - x_{\\ell,i-1}}{2h_\\ell}) = \\varphi(w_\\ell x + b_{\\ell,i}).$$\n",
    "That is to say, any $\\varphi_{\\ell,i}(x)$ can be obtained from\n",
    "$\\varphi(x)$ by scaling (dilation) and translation with $$\\label{key}\n",
    "w_\\ell = 2^{\\ell-1}, \\quad b_{\\ell,i} = \\frac{-(i-1)}{2},$$ in\n",
    "$\\varphi_{\\ell,i} = \\varphi(w_\\ell x + b_{\\ell,i})$.\n",
    "\n",
    "![Diagram of $\\varphi(x)$ (left) and $\\varphi_{\\ell,i}(x)$\n",
    "(right).](1dbasis1.pdf \"fig:\"){width=\"4cm\"} ![Diagram of $\\varphi(x)$\n",
    "(left) and $\\varphi_{\\ell,i}(x)$\n",
    "(right).](basisfunction.pdf \"fig:\"){width=\"5cm\"}\n",
    "\n",
    "Let us recall the finite element interpolation in Section\n",
    "[\\[linearFE\\]](#linearFE){reference-type=\"ref\" reference=\"linearFE\"} as\n",
    "$$\\label{key}\n",
    "u(x) \\approx u_\\ell(x) := \\sum_{ 0\\le i \\le 2^\\ell} u(x_{\\ell,i}) \\varphi_{\\ell,i}(x),$$\n",
    "for any smooth function $u(x)$ on $(0,1)$. The above interpolation will\n",
    "converge as $\\ell \\to \\infty$, which shows that $$\\label{key}\n",
    "{\\rm span} \\left\\{  \\varphi(w_\\ell x + b_{\\ell,i}) \\right\\} \\quad \\text{is dense in} \\quad H^1(0,1).$$\n",
    "Thus, we may have the next concise relation: $$\\label{key}\n",
    "\\begin{split}\n",
    "\\text{FE space} =  &{\\rm span} \\left\\{  \\varphi(w_\\ell x + b_{\\ell,i}) ~|~ 0\\le i \\le 2^\\ell, \\ell = 1, 2, \\cdots \\right\\} \n",
    "\\\\\n",
    "\\subset  &{\\rm span} \\left\\{  \\varphi(w x + b) ~|~  w, b \\in \\mathbb{R} \\right\\}.\n",
    "\\end{split}$$ In other words, the finite element space can be understood\n",
    "as the linear combination of $\\varphi(w x + b)$ with certain special\n",
    "choice of $w$ and $b$.\n",
    "\n",
    "Here, we need to point out that this\n",
    "${\\rm span} \\left\\{  \\varphi(w x + b) ~|~  w, b \\in \\mathbb{R} \\right\\}$\n",
    "is exact the deep neural networks with one hidden layer (shallow neural\n",
    "networks) with activation function $\\varphi(x)$. More precisely,\n",
    "$$\\label{key}\n",
    "f \\in {\\rm span} \\left\\{  \\varphi(w x + b) ~|~  w, b \\in \\mathbb{R} \\right\\},$$\n",
    "means there exist positive integer $N$ and $w_j, b_j \\in \\mathbb{R}$\n",
    "such that $$\\label{key}\n",
    "f = \\sum_{j=1}^N a_j \\varphi(w_j x + b_j),$$ which is also called one\n",
    "hidden neural network function with $N$ neurons.\n",
    "\n",
    "::: {.remark}\n",
    "1.  By making $w_\\ell$ and $b_{\\ell,i}$ in\n",
    "    [\\[def_g\\]](#def_g){reference-type=\"eqref\" reference=\"def_g\"}\n",
    "    arbitrary, we get a much larger class of function which is exact a\n",
    "    special neural network with activation function $\\varphi(x)$.\n",
    "\n",
    "2.  Generalizations:\n",
    "\n",
    "    1.  activation function $\\varphi$ can be different, such as\n",
    "        ${\\rm ReLU}(x) = \\max\\{0,x\\}$.\n",
    "\n",
    "    2.  There is a natural extension for high dimension $d$ as\n",
    "        $$\\label{key}\n",
    "                    \\left\\{  \\varphi(w\\cdot x + b) \\right \\},$$ where\n",
    "        $w\\in \\mathbb{R}^d$, $b\\in \\mathbb{R}$ and\n",
    "        $\\displaystyle w\\cdot x = \\sum_{i=1}^d w_i x_i$. This is called\n",
    "        \"deep\" neural network with one hidden layer.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a6c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
